{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh# This Python 3 environment comes with many helpful analytics libraries installed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom tqdm import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\n# response: biased or non-biased\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bias detection","metadata":{}},{"cell_type":"markdown","source":"## Dati","metadata":{}},{"cell_type":"code","source":"# todo\n\nx_test = None\ny_test = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"model = \"llama3.1\"\n\nstart_ollama()\npull_model(model_name)\nstart_model(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n        You will be provided with a text, and your task is to classify it into \n        one of two categories: \"biased\" or \"non-biased.\" A text is considered \"biased\" \n        if it expresses a personal opinion, uses emotional or partial language, or promotes \n        a particular perspective at the expense of others. A text is considered \"non-biased\" \n        if it presents facts in a neutral and objective manner, without evident influences.\n        Give ONLY the class \"biased\" or \"non-biased\", NO PREAMBLE, NO EXPLANATIONS.\n        Below, I will show you some classification examples to help you understand \n        how to distinguish between the two types of texts. \\n\n\n        Example 1: \\n\n        Text: \"The new educational program is a complete failure, designed with no regard for the needs of the students.\" \\n\n        Answer: biased \\n\n\n        Example 2: \\n\n        Text: \"The new educational program was introduced with the goal of improving student performance in scientific subjects.\" \\n\n        Answer: non-biased \\n\n\n        Example 3: \\n\n        Text: \"The current government has done a terrible job managing the economic crisis, causing more harm than good.\" \\n\n        Answer: biased \\n\n\n        Example 4: \\n\n        Text: \"The government has implemented a series of measures to address the economic crisis, with mixed results according to various analysts.\" \\n\n        Answer: non-biased \\n\n   \n         <|eot_id|><|start_header_id|>user<|end_header_id|> \n        Text: {text} \\n\n        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"text\"],\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# response: biased or non-biased\n\ndef bias_det(prompt, model):\n    llm = ChatOllama(model=model, temperature=0)\n    bias_det_chain = prompt | llm | StrOutputParser()\n    return bias_det_chain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label 0: biased\n# Label 1: non-biased\n\ndef predict(prompt,model,x_test,encoder):\n    y_pred = []\n    if encoder:\n        bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n    for x in tqdm(x_test):\n        if encoder:\n            answer = bias_detection(x)\n        else:\n            answer = hate_speech_detection(prompt,model).invoke({\"text\":x})\n        if answer.lower() == \"biased\": y_pred.append(0)\n        elif answer.lower() == \"non-biased\": y_pred.append(1)\n    return y_pred\n\n# True se vogliamo il modello encoder, False se vogliamo usare LLM\nencoder = True\ny_pred = predict(prompt,model,x_test,encoder)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Valutazione del modello\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Classification Report:\\n{class_report}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debiasing","metadata":{}},{"cell_type":"code","source":"#todo predisposizione dati e metriche di utilizzo","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def debiasing_answer(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> The user will pass you text cointaining biases.\n        Your role is to give a debiased version of the text. Give ONLY the debiased text, NO PREAMBLE, NO EXPLANATIONS.\n        Remember that we should treat people from different socioeconomic statuses, sexual orientations, religions, races, physical appearances, nationalities,\n        gender identities, disabilities, and ages equally. When we do not have sufficient information, we should choose the unknown option, rather\n        than making assumptions based on our stereotypes.\n         <|eot_id|><|start_header_id|>user<|end_header_id|> \n        {text}\n        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"text\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    debiasing = prompt | llm | StrOutputParser()\n    return debiasing","metadata":{},"execution_count":null,"outputs":[]}]}