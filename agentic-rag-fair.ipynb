{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"Starting ollama server and pulling llama3","metadata":{"id":"HX2pKChL7-J3"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\nt.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ollama pull llama3","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\nt2.start()","metadata":{"id":"o2WTtPsM8zxb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U langchain-ai21 langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choosing llm\n\nlocal_llm = \"llama3\"","metadata":{"id":"QqQOyQr49oeW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","execution":{"iopub.status.busy":"2024-07-28T06:53:20.080890Z","iopub.execute_input":"2024-07-28T06:53:20.081923Z","iopub.status.idle":"2024-07-28T06:53:20.095621Z","shell.execute_reply.started":"2024-07-28T06:53:20.081883Z","shell.execute_reply":"2024-07-28T06:53:20.094742Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nweb_search_tool = TavilySearchResults(k=3)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"code","source":"from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls] #text + meta-data on docs\ndocs_list = [item for sublist in docs for item in sublist] #ci serve l'attributo page_content\n\n# Chunking\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=250, chunk_overlap=0\n)\n\ndoc_splits = text_splitter.split_documents(docs_list)\nindex_name = \"langchain-test-index\"\n\n# Add to vectorDB\nvectorstore = PineconeVectorStore.from_documents(\n    documents=doc_splits,\n    #embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"cuda\"),\n    embedding=AI21Embeddings(device=\"cuda\"),\n    index_name=index_name\n)\nretriever = vectorstore.as_retriever()","metadata":{"id":"Xfgup11aAFly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing KBT","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nurls = [#TODO\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls] #text + meta-data on docs\ndocs_list = [item for sublist in docs for item in sublist] #ci serve l'attributo page_content\n\n# Chunking\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=250, chunk_overlap=0\n)\n\ndoc_splits = text_splitter.split_documents(docs_list)\nindex_name = \"langchain-test-index\"\n\n# Add to vectorDB\nvectorstore_KBT = PineconeVectorStore.from_documents(\n    documents=doc_splits,\n    #embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"cuda\"),\n    embedding=AI21Embeddings(device=\"cuda\"),\n    index_name=index_name\n)\nretriever_KBT = vectorstore_KBT.as_retriever()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Query generation (multi-aspects)","metadata":{"id":"ThEK6sQSP4nC"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant that generates multiple\n    search queries based on some specified aspects from a single input query. You have to generate ONLY ONE query\n    for EACH aspect. <|eot_id|><|start_header_id|>user<|end_header_id|>\n    Generate search aspect-queries related to the original query: {original_query}.\n    Where the aspects are: {aspects}\n    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"aspects\",\"original_query\"],\n)\n\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\nquery_generator = prompt | llm | JsonOutputParser()\n\n# Prova\n\n# original_query = \"What about Covid19?\"\n# generation = query_generator.invoke({\"original_query\": original_query})\n# print(generation)\n\n\n# # Reciprocal Rank Fusion algorithm\n# def reciprocal_rank_fusion(search_results_dict, k=60):\n#     fused_scores = {}\n#     print(\"Initial individual search result ranks:\")\n#     for query, doc_scores in search_results_dict.items():\n#         print(f\"For query '{query}': {doc_scores}\")\n\n#     for query, doc_scores in search_results_dict.items():\n#         for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n#             if doc not in fused_scores:\n#                 fused_scores[doc] = 0\n#             previous_score = fused_scores[doc]\n#             fused_scores[doc] += 1 / (rank + k)\n#             print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n\n#     reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n#     print(\"Final reranked results:\", reranked_results)\n#     return reranked_results","metadata":{"id":"ovAC1Rg8QACa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organizing outputs","metadata":{"id":"AeLcJqM3KK_Y"}},{"cell_type":"code","source":"prompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant that organizes and puts together the answers from multiple\n    queries based on different aspects in a single answer. Make the concatenations or a summary. <|eot_id|><|start_header_id|>user<|end_header_id|>\n    Here are the answers: {answers}\n    Result answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"answers\"],\n)\n\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\nfinal_answer = prompt | llm | JsonOutputParser()\n#final_output = final_answer.invoke({\"answers\": answers})\n#print(final_output)","metadata":{"id":"viQg-NxkKNv4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"answers= [\"\"\"You didn't ask a specific question, but I can summarize the text for you.\n \\n\\nThe article discusses the economic impact of COVID-19 around the world. \n It highlights that the pandemic has had significant and persistent effects on output and prices,\n especially in emerging and developing countries. The article also mentions that the partnership\n with BioNTech, high cost of production and management of the vaccine, and growing number of \n same-size competitors have reduced investors' trust in the company to have bigger revenue \n in 2021.\\n\\nThe article further notes that global coordination and cooperation are crucial for \n slowing the spread of the pandemic and alleviating economic damage. It also emphasizes the \n humanitarian and economic toll the global recession will take on economies with extensive \n informal sectors, which make up an estimated one-third of GDP and about 70% of total employment \n in emerging market and developing economies.\\n\\nThe article also discusses the long-term damage \n to potential output and productivity growth, citing the June 2020 Global Economic Prospects \n report. The report assumes that the pandemic recedes in such a way that domestic mitigation \n measures can be lifted by mid-year in advanced economies and later in developing countries, \n and that adverse global spillovers ease during the second half of 2020.\\n\\nFinally, the article \n mentions the UN's latest World Economic Situation and Prospects report, which projects the world \n economy to grow by 2.3% in 2023 and 2.5% in 2024, with the least developed countries forecast \n to grow by 4.1% in 2023 and 5.2% in 2024.\\n\\nIf you have a specific question or would like me to \n answer something related to this text, feel free to ask!\", \"It seems like you didn't provide a \n specific question for me to answer. The text appears to be a scientific article and a brief \n report on the impact of COVID-19 pandemic on social interactions and economic well-being.\n \\n\\nIf you have a specific question related to this topic, I'd be happy to help.\"\"\"]","metadata":{}},{"cell_type":"markdown","source":"# Retrieval","metadata":{"id":"kNT1n4npE6YP"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0) #higher temperature more likely hallucinations\n\nprompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n    of a retrieved document to a user question. If the document contains keywords related to the user question,\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n     <|eot_id|><|start_header_id|>user<|end_header_id|>\n    Here is the retrieved document: \\n\\n {document} \\n\\n\n    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n    \"\"\",\n    input_variables=[\"question\", \"document\"],\n)\n\nretrieval_grader = prompt | llm | JsonOutputParser()\n# question = \"agent memory\"\n# docs = retriever.invoke(question)\n# doc_txt = docs[1].page_content\n# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))","metadata":{"id":"WzXfhJQ-E-mg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating answer","metadata":{"id":"MFgY6-seHsdQ"}},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n    Question: {question}\n    Context: {context}\n    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"question\", \"document\"],\n)\n\nllm = ChatOllama(model=local_llm, temperature=0)\n\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n# Chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run\n# question = \"agent memory\"\n# docs = retriever.invoke(question)\n# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n# print(generation)","metadata":{"id":"DYmsobX4IIQD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hallucinations check","metadata":{"id":"0Ot0mK4hISuW"}},{"cell_type":"markdown","source":"Per ora non uso hallucinations check","metadata":{}},{"cell_type":"code","source":"llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' (both in lower case) to indicate\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n    SINGLE KEY 'score' and NO preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n    Here are the facts:\n    \\n ------- \\n\n    {documents}\n    \\n ------- \\n\n    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"generation\", \"documents\"],\n)\n\nhallucination_grader = prompt | llm | JsonOutputParser()\n#hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})","metadata":{"id":"zLqFdBSSIatd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Answer check","metadata":{"id":"Vky-iraRI7V4"}},{"cell_type":"code","source":"llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' (both in lower case) to indicate whether the answer is\n    useful to resolve a question. Provide the binary score as a JSON with a SINGLE KEY 'score' and NO preamble or explanation.\n     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n    \\n ------- \\n\n    {generation}\n    \\n ------- \\n\n    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"generation\", \"question\"],\n)\n\nanswer_grader = prompt | llm | JsonOutputParser()\n#answer_grader.invoke({\"question\": question, \"generation\": generation})","metadata":{"id":"nETsNNGuI9g_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Routing","metadata":{"id":"b4jwj6nZJBsA"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\n# LLM\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\nprompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents,\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"question\"],\n)\n\nquestion_router = prompt | llm | JsonOutputParser()\n# question = \"llm agent memory\"\n# docs = retriever.get_relevant_documents(question)\n# doc_txt = docs[1].page_content\n# print(question_router.invoke({\"question\": question}))","metadata":{"id":"kDKQ-NA7JG9S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Document check (Entailment)","metadata":{}},{"cell_type":"code","source":"llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You have to perform a task of textual entailments\n    between two documents. Give a binary score 'yes' or 'no' (both in lower case) to indicate whether the second document is\n    entailed by the first document. Provide the binary score as a JSON with a SINGLE KEY 'score' and NO preamble or explanation.\n     <|eot_id|><|start_header_id|>user<|end_header_id|> The first document:\n    {first_doc}\n    \\n ------- \\n\n    Entails with:\n    \\n ------- \\n\n    {second_doc}\n    \\n ------- \\n\n    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n    input_variables=[\"first_doc\", \"second_doc\"],\n)\n\nentailment_checker = prompt | llm | JsonOutputParser()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# Control flow (Aspect agents)","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        web_search: whether to add search\n        documents: list of documents\n    \"\"\"\n    \n    query: str\n    answers_agent: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n\n\ndef retrieve(state):\n    \"\"\"\n    Retrieve documents from vectorstore and from KBT\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    print(f\"State: {state}\")\n    queries = state[\"query\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retriever_KBT.invoke(query)\n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state):\n    \"\"\"\n    Generate answer using RAG on retrieved documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    print(\"---GENERATE---\")\n    print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # RAG generation\n    generation = rag_chain.invoke({\"context\": documents, \"question\": query})\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation]}\n\n\ndef grade_documents(state):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question\n    If any document is not relevant, we will set a flag to run web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Filtered out irrelevant documents and updated web_search state\n    \"\"\"\n\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = retrieval_grader.invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        grade = score[\"score\"]\n        # Document relevant\n        if grade.lower() == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            web_search = \"Yes\"\n            continue\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state):\n    \"\"\"\n    Web search based based on the question\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Appended web results to documents\n    \"\"\"\n\n    print(\"---WEB SEARCH---\")\n    print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    if documents is not None:\n        documents.append(web_results)\n    else:\n        documents = [web_results]\n    return {\"documents\": documents, \"query\": query}\n\ndef entailment_filter(state):\n    \"\"\"\n    Filter documents that doesn't entail with KBT\n\n    Args:\n        state (dict): The current graph state\n    \"\"\"\n\n    print(\"---ENTAILMENT FILTER---\")\n    print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_KBT\"]\n\n    # Score each doc\n    filtered_docs = []\n    for d in documents:\n        for d_kbt in documents:\n            score = entailment_checker.invoke(\n                {\"first_doc\": d_kbt.page_content, \"second_doc\": d.page_content}\n            )\n            grade = score[\"score\"]\n            # Document entailed\n            if grade.lower() == \"yes\":\n                print(\"---DOCUMENT ENTAILED---\")\n                filtered_docs.append(d)\n                break\n    return {\"documents\": filtered_docs}\n\n\n### Conditional edge\n\ndef route_question(state):\n    \"\"\"\n    Route question to web search or RAG.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n\n    print(\"---ROUTE QUESTION---\")\n    print(f\"State: {state}\")\n    query = state[\"query\"]\n    #print(queries)\n    source = question_router.invoke({\"question\": query})\n    #print(source)\n    #print(source[\"datasource\"])\n    if source[\"datasource\"] == \"web_search\":\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return \"websearch\"\n    elif source[\"datasource\"] == \"vectorstore\":\n        print(\"---ROUTE QUESTION TO RAG---\")\n        return \"vectorstore\"\n\n\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or add web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n        )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n\n\n\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    print(\"---CHECK HALLUCINATIONS---\")\n    print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    my_answer = state[\"my_answer\"]\n\n    score = hallucination_grader.invoke(\n        {\"documents\": documents, \"generation\": my_answer} #answers_agent[0]\n    )\n    print(f\"score: {score}\")\n    grade = score[\"score\"]\n\n    # Check hallucination\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": query, \"generation\": my_answer})\n        grade = score[\"score\"]\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"","metadata":{"id":"6iV1oJJtLcFM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Build graph\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"websearch\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # generatae\nworkflow.add_node(\"entailment_filter\", entailment_filter)  # generatae\n\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"websearch\": \"websearch\", #se la risposta è websearch, allora vai al nodo websearch\n        \"vectorstore\": \"retrieve\", #se la risposta è vectorstore, allora vai al nodo retrieve\n    },\n)\n\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"websearch\": \"websearch\",\n        \"generate\": \"entailment_filter\",\n    },\n)\nworkflow.add_edge(\"websearch\", \"entailment_filter\")\n\nworkflow.add_edge(\"entailment_filter\", \"generate\")\n\n# Per ora non faccio il controllo sulle allucinazioni\nworkflow.add_edge(\"generate\", END)\n\"\"\"\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"websearch\",\n    },\n)\n\"\"\"\n    \nworkflow_compiled = workflow.compile()","metadata":{"id":"PL0NHOMy3oQs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for i in range(num_aspects):\n#    print(workflows[i].config_specs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_compiled.get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\ninputs = {\"query\": \"What about COVID19?\"}\n#print(workflows[0].stream(inputs))\nfor output in workflow_compiled.stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"my_answer\"])\n#workflows[0].invoke({\"queries\": [\"What about COVID19?\"]},{\"id_agent\":1})","metadata":{"id":"tnAKcBTfRwc4","outputId":"251e97c3-58c3-4d28-bcc7-1c1ac92b587f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Application","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        agents_answer: generation of agents\n        answer: the final answer\n    \"\"\"\n    \n    question: str\n    queries: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    final_answer: str\n\n\ndef generate_queries(state):\n    \"\"\"\n    Generate multi-aspect queries from the starting question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains the multi aspect queries\n    \"\"\"\n    print(\"---GENERATE MULTI-ASPECTS QUERIES---\")\n    print(f\"State: {state}\")\n    question = state[\"question\"]\n    aspects = [\"Health\",\"Economy\",\"Society\"]\n\n    generation = query_generator.invoke({\"aspects\": aspects, \"original_query\": question})\n    #print(list(generation.values()))\n    return {\"queries\": list(generation.values())}\n\ndef send_queries(state):\n    return [Send(\"aspect_agent_node\", {\"query\": q}) for q in state[\"queries\"]]\n\ndef organize_outputs(state):\n    \"\"\"\n    Organize the outputs of the agents.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, answer, that contains the final answer to give to user\n    \"\"\"\n    print(\"---ORGANIZE OUTPUTS---\")\n    print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n\n    final_output =final_answer.invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}\n\nmaster_flow = StateGraph(SuperGraphState)\n\n# Define the nodes\nmaster_flow.add_node(\"generate_queries\", generate_queries)\nmaster_flow.add_node(\"organize_queries\", organize_outputs)\nmaster_flow.add_node(\"aspect_agent_node\",workflow_compiled)\n\n# Build graph\nmaster_flow.add_edge(START, \"generate_queries\")\nmaster_flow.add_conditional_edges(\"generate_queries\", send_queries, [\"aspect_agent_node\"])\nmaster_flow.add_edge(\"aspect_agent_node\", \"organize_queries\")\nmaster_flow.add_edge(\"organize_queries\", END)\n\napp = master_flow.compile()","metadata":{"id":"TxQR9nC5FEUw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(app.get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\ninputs = {\"question\": \"What about COVID19?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"final_answer\"])","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]}]}