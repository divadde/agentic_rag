{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 ragas langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bias detection model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_entailment/Textual%20Entailment%20Explanation%20Demo.html\n- https://huggingface.co/facebook/bart-large-mnli","metadata":{}},{"cell_type":"markdown","source":"Entailment model (BART):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbart_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\",device=device)\nbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BART_prediction(premise,hypothesis):\n    #print(f\"Premise: {premise}\")\n    #print(f\"Hypo: {hypothesis}\")\n    input_ids = bart_tokenizer.encode(premise, hypothesis, return_tensors=\"pt\")\n    logits = bart_model(input_ids)[0]\n    probs = logits.softmax(dim=1)\n\n    max_index = torch.argmax(probs).item()\n\n    bart_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n    return bart_label_map[max_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nweb_search_tool = TavilySearchResults(k=2)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"markdown","source":"osservazione sul chuncking: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338","metadata":{}},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\ndef create_retriever(index_name,top_k):\n    vectorstore = PineconeVectorStore(\n        index_name=index_name,\n        embedding=AI21Embeddings(device=\"cuda\")\n    )\n    return vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\ndef create_KBT_retrievers(aspects,top_k):\n    retrievers = []\n    for aspect in aspects:\n        retriever = create_retriever(f\"{aspect.lower()}-kbt\",top_k)\n        retrievers.append(retriever)\n    return retrievers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Query generation (multi-aspects)","metadata":{"id":"ThEK6sQSP4nC"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\ndef query_generator(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You have to generate multiple\n        search queries based on some specified aspects. You have to generate an answer as a Python list,\n        and in each position of the list there is the generated query of the aspect. NO PREAMBLE: return only the list.\n        In the generation, USE the escape character \"\\\" if it is needed: for example: \"People\\'s'\"\n        Here some examples of the task: \\n\n        Original query: \"What about COVID19?\"\n        Aspects: [\"Health\",\"Economy\"]\n        Answer: [\"Symptoms of COVID19\",\"Economic consequences of COVID19\"]\n        \\n ----- \\n\n        Original query: \"COVID19 was fake?\"\n        Aspects: [\"Health\",\"Society\"]\n        Answer: [\"Is COVID19 just a cold?\",\"What do people think about COVID19?\"]\n        \\n ----- \\n\n        Original query: \"COVID19 was a hoax?\"\n        Aspects: [\"Health\",\"Technology\",\"Society\"]\n        Answer: [\"Consequences of COVID19 on health\",\"Were there micro-chip into vaccines?\", \"Consequences of people\\'s thought on Covid19 as a hoax\"]\n        \\n ----- \\n\n        <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Original query: {original_query}\n        Aspects: {aspects}\n        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"aspects\",\"original_query\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0) \n    query_generator = prompt | llm | StrOutputParser() \n    return query_generator\n\n\n#original_query = \"Covid19 was a hoax?\"\n#aspects = [\"Health\",\"Economy\"]\n#generation = query_generator(local_llm).invoke({\"original_query\": original_query, \"aspects\":aspects})\n#print(eval(generation))\n\n\n# # Reciprocal Rank Fusion algorithm\n# def reciprocal_rank_fusion(search_results_dict, k=60):\n#     fused_scores = {}\n#     print(\"Initial individual search result ranks:\")\n#     for query, doc_scores in search_results_dict.items():\n#         print(f\"For query '{query}': {doc_scores}\")\n\n#     for query, doc_scores in search_results_dict.items():\n#         for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n#             if doc not in fused_scores:\n#                 fused_scores[doc] = 0\n#             previous_score = fused_scores[doc]\n#             fused_scores[doc] += 1 / (rank + k)\n#             print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n\n#     reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n#     print(\"Final reranked results:\", reranked_results)\n#     return reranked_results","metadata":{"id":"ovAC1Rg8QACa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organizing outputs","metadata":{"id":"AeLcJqM3KK_Y"}},{"cell_type":"code","source":"def final_answer(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n        You are an helpful assistant who must organize a coherent speech that comes from multiple sources. \n        Your job is to put these pieces of text together, maintaining correct punctuation and coherence in your speech. \n        Provide only the organized segments, without the preamble.\n        <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Here are pieces of text: {answers}\n        Result answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"answers\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    final_answer = prompt | llm | StrOutputParser()\n    return final_answer\n\n#final_output = final_answer.invoke({\"answers\": answers})\n#print(final_output)","metadata":{"id":"viQg-NxkKNv4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval","metadata":{"id":"kNT1n4npE6YP"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\n#higher temperature more likely hallucinations\n\ndef retrieval_grader(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n        of a retrieved document to a user question. If the document contains keywords related to the user question,\n        grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n        Provide only the answer 'yes' or 'no', NOT ANYMORE. NO PREAMBLE. NO EXPLANATION.\n        <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Here is the retrieved document: \\n\\n {document} \\n\\n\n        Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n        \"\"\",\n        input_variables=[\"question\", \"document\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0) #higher temperature more likely hallucinations\n    retrieval_grader = prompt | llm | StrOutputParser()\n    return retrieval_grader\n\n# question = \"agent memory\"\n# docs = retriever.invoke(question)\n# doc_txt = docs[1].page_content\n# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))","metadata":{"id":"WzXfhJQ-E-mg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating answer","metadata":{"id":"MFgY6-seHsdQ"}},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\ndef rag_chain(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n        Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Question: {question}\n        Context: {context}\n        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"question\", \"document\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    # Chain\n    rag_chain = prompt | llm | StrOutputParser()\n    return rag_chain\n\n# Run\n# question = \"agent memory\"\n# docs = retriever.invoke(question)\n# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n# print(generation)","metadata":{"id":"DYmsobX4IIQD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hallucinations check (not used)","metadata":{"id":"0Ot0mK4hISuW"}},{"cell_type":"markdown","source":"Per ora non uso hallucinations check","metadata":{}},{"cell_type":"code","source":"def hallucination_grader(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n        an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' (both in lower case) to indicate\n        whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n        SINGLE KEY 'score' and NO preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Here are the facts:\n        \\n ------- \\n\n        {documents}\n        \\n ------- \\n\n        Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"generation\", \"documents\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    hallucination_grader = prompt | llm | JsonOutputParser()\n    return hallucination_grader\n\n#hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})","metadata":{"id":"zLqFdBSSIatd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Answer check","metadata":{"id":"Vky-iraRI7V4"}},{"cell_type":"code","source":"def answer_grader(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n        answer is useful to resolve a question. Give a binary score 'yes' or 'no' (both in lower case) to indicate whether the answer is\n        useful to resolve a question. Provide the binary score as a JSON with a SINGLE KEY 'score' and NO preamble or explanation.\n         <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n        \\n ------- \\n\n        {generation}\n        \\n ------- \\n\n        Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"generation\", \"question\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    answer_grader = prompt | llm | JsonOutputParser()\n    return answer_grader\n\n#answer_grader.invoke({\"question\": question, \"generation\": generation})","metadata":{"id":"nETsNNGuI9g_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Routing (not used)","metadata":{"id":"b4jwj6nZJBsA"}},{"cell_type":"markdown","source":"Per ora non applichiamo il routing!","metadata":{}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\ndef question_router(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n        user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents,\n        prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords\n        in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n        or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n        no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"question\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    question_router = prompt | llm | JsonOutputParser()\n    return question_router\n    \n# question = \"llm agent memory\"\n# docs = retriever.get_relevant_documents(question)\n# doc_txt = docs[1].page_content\n# print(question_router.invoke({\"question\": question}))","metadata":{"id":"kDKQ-NA7JG9S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Entailment","metadata":{}},{"cell_type":"code","source":"def entailment_checker(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You have to perform a task of text-entailment.\n        In particular, you must determine the relationship between a premise and a hypothesis. \n        \"The possible relationships are:\\n\"\n        \"1. contradiction: The premise contradicts the hypothesis.\\n\"\n        \"2. entailment: The premise implies the hypothesis.\\n\"\n        \"3. neutral: The premise and hypothesis are neutral.\\n\\n\"\n        Premise and hypothesis are in 'contradiction', if in the hypothesis there are contradicting statements with the premise statements.\n        There is an 'entailment' between premise and hypothesis if it is likely that from the premise you could deduce what is stated in the hypothesis. \n        Premise and hypothesis are 'neutral' if they are not in contradiction or in entailment.\n        Provide only the answer with the category, NOT ANYMORE. NO PREAMBLE. NO EXPLANATION. \\n\n        Here some examples: \\n\n        \n        \"Example 1:\\n\"\n        \"Premise: COVID-19 is caused by the SARS-CoV-2 virus.\\n\"\n        \"Hypothesis: The SARS-CoV-2 virus causes COVID-19.\\n\"\n        \"Answer: Entailment\\n\\n\"\n    \n        \"Example 2:\\\\n\"\n        \"Premise: Health agencies have stated that COVID-19 vaccines do not contain any microchips or tracking devices.\\\\n\"\n        \"Hypothesis: COVID-19 vaccines contain microchips to monitor people..\\\\n\"\n        \"Answer: Contradiction\\\\n\\\\n\"\n\n        \"Example 3:\\\\n\"\n        \"Premise: Social distancing can reduce the spread of COVID-19.\\\\n\"\n        \"Hypothesis: Washing hands frequently is essential to prevent the spread of COVID-19.\\\\n\"\n        \"Answer: Neutral\\\\n\\\\n\"\n\n        \"Example 4:\\\\n\"\n        \"Premise: COVID-19 has led to global economic disruptions.\\\\n\"\n        \"Hypothesis: Many businesses have suffered financial losses due to the pandemic.\\\\n\"\n        \"Answer: Entailment\\\\n\\\\n\"\n\n        \"Example 5:\\\\n\"\n        \"Premise: Wearing masks can help reduce the transmission of COVID-19.\\\\n\"\n        \"Hypothesis: Masks are the only measure needed to prevent COVID-19.\\\\n\"\n        \"Answer: Contradiction\\\\n\\\\n\"\n\n        \"Example 6:\\\\n\"\n        \"Premise: The COVID-19 pandemic has accelerated the development of remote work technologies.\\\\n\"\n        \"Hypothesis: More people are working from home due to advancements in technology.\\\\n\"\n        \"Answer: Neutral\\\\n\\\\n\"\n        \n        \"Example 7:\\n\"\n        \"Premise: Research has shown that COVID-19 primarily affects the respiratory system, but can also lead to complications in other organs.\\n\"\n        \"Hypothesis: COVID-19 affects only the respiratory system.\\n\"\n        \"Answer: Contradiction\\n\\n\"\n        \n        \"Example 8:\\n\"\n        \"Premise: Despite initial vaccine hesitancy, widespread vaccination campaigns have significantly reduced the rate of severe COVID-19 cases globally.\\n\"\n        \"Hypothesis: Vaccine hesitancy has prevented the reduction of severe COVID-19 cases.\\n\"\n        \"Answer: Contradiction\\n\\n\"\n        \n        \"Example 9:\\n\"\n        \"Premise: The COVID-19 pandemic has led to an unprecedented increase in remote work, with companies around the world adopting digital tools to maintain operations.\\n\"\n        \"Hypothesis: The pandemic has forced many companies to adapt to remote work.\\n\"\n        \"Answer: Entailment\\n\\n\"\n        \n        \"Example 10:\\n\"\n        \"Premise: Although vaccines have been effective in preventing severe disease, the emergence of new variants has led to concerns about vaccine efficacy.\\n\"\n        \"Hypothesis: Vaccines are completely ineffective against new variants.\\n\"\n        \"Answer: Contradiction\\n\\n\"\n        \n        \"Example 11:\\n\"\n        \"Premise: Governments worldwide have implemented various public health measures, including lockdowns, mask mandates, and social distancing, to control the spread of COVID-19.\\n\"\n        \"Hypothesis: Lockdowns were the only measure used to control the spread of COVID-19.\\n\"\n        \"Answer: Contradiction\\n\\n\"\n        \n        \"Example 12:\\n\"\n        \"Premise: The economic impact of COVID-19 has been severe, with a sharp increase in unemployment rates, especially in sectors such as hospitality and tourism.\\n\"\n        \"Hypothesis: The pandemic has disproportionately affected certain industries, leading to higher unemployment rates in those sectors.\\n\"\n        \"Answer: Entailment\\n\\n\"\n        \n         <|eot_id|><|start_header_id|>user<|end_header_id|> Premise:\n        {first_doc}\n        \\n ------- \\n\n        Hypothesis:\n        \\n ------- \\n\n        {second_doc}\n        \\n ------- \\n\n        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"first_doc\", \"second_doc\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    entailment_checker = prompt | llm | StrOutputParser()\n    return entailment_checker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debiasing","metadata":{}},{"cell_type":"markdown","source":"https://learnprompting.org/docs/reliability/debiasing","metadata":{}},{"cell_type":"code","source":"def debiasing_answer(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> The user will pass you text cointaining biases.\n        Your role is to give a debiased version of the text. Give ONLY the debiased text, NO PREAMBLE, NO EXPLANATIONS.\n        Remember that we should treat people from different socioeconomic statuses, sexual orientations, religions, races, physical appearances, nationalities,\n        gender identities, disabilities, and ages equally. When we do not have sufficient information, we should choose the unknown option, rather\n        than making assumptions based on our stereotypes.\n         <|eot_id|><|start_header_id|>user<|end_header_id|> \n        {text}\n        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"text\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    debiasing = prompt | llm | StrOutputParser()\n    return debiasing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hate speech detection","metadata":{}},{"cell_type":"markdown","source":"https://arxiv.org/html/2401.03346v1/#S4","metadata":{}},{"cell_type":"code","source":"def hate_speech_detection(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an helpful assistant who has to detect the presence of hate speech.\n            Hate speech is speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. \n            You have to answer \"yes\" if it contains hate speech, or \"no\" if it doesn't contain hate speech. NO PREAMBLE, NO EXPLANATIONS.\n            <|eot_id|><|start_header_id|>user<|end_header_id|> \n            Do you think this document contain hate speech? document: {document}.\n            <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"document\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    hate_speech_detection = prompt | llm | StrOutputParser()\n    return hate_speech_detection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\nimport sklearn.metrics\nimport numpy as np\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    query: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    \"\"\"\n    Retrieve documents from vectorstore and from KBT\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fairness):\n    \"\"\"\n    Generate answer using RAG on retrieved documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # RAG generation\n    generation = rag_chain(llm).invoke({\"context\": documents, \"question\": query})\n    aspect_id = state[\"aspect_id\"]\n    #print(f\"Aspect agent {aspect_id} generates: {generation}\") #Debug\n    if fairness:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation]}\n\ndef confirm_answer(state,verbose):\n    \"\"\"\n    Confirm answer if there is no bias in the text.\n\n    Args:\n        state (dict): The current graph state\n\n    \"\"\"\n    if verbose:\n        print(\"---CONFIRM ANSWER---\")\n        print(f\"State: {state}\")\n    my_answer = state[\"my_answer\"]\n\n    return {\"answers_agent\": [my_answer]}\n\n\ndef grade_documents(state,verbose,llm):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question\n    If any document is not relevant, we will set a flag to run web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Filtered out irrelevant documents and updated web_search state\n    \"\"\"\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = retrieval_grader(llm).invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if score.lower() == \"yes\":\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            web_search = \"Yes\"\n            continue\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose):\n    \"\"\"\n    Web search based based on the question\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Appended web results to documents\n    \"\"\"\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents([web_results])\n    for doc in doc_splits:\n        if documents is None:\n            documents = [doc]\n        else:\n            documents.append(doc)\n    return {\"documents\": documents, \"query\": query}\n\n\ndef hate_speech_filter(state,verbose,llm):\n    if verbose:\n        print(\"---HATE SPEECH FILTER---\")\n        print(f\"State: {state}\")\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    for d in documents:\n        score = hate_speech_detection(llm).invoke(\n            {\"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        if score.lower() == \"no\":\n            if verbose: print(\"---DOCUMENT ACCEPTED---\")\n            filtered_docs.append(d)\n    \n    return {\"documents\": filtered_docs}\n\n\ndef entailment_filter(state,BART_model,strategy_entailment,neutral_acceptance,verbose,test_mode,observer,llm):\n    \"\"\"\n    Filter documents that doesn't entail with KBT\n\n    Args:\n        state (dict): The current graph state\n    \"\"\"\n    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Score each doc\n    filtered_docs = []\n    #counter_docs = 0 # Debug\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = entailment_checker(llm).invoke(\n                        {\"first_doc\": d_kbt.page_content, \"second_doc\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                if score.lower() != \"neutral\":\n                    neutral = False\n                if score.lower() == \"contradiction\":\n                    # contradiction found\n                    break\n            if (not neutral or neutral_acceptance) and score.lower() != \"contradiction\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug            \n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = entailment_checker(llm).invoke(\n                        {\"first_doc\": d_kbt.page_content, \"second_doc\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                # Document entailed\n                if score.lower() != \"neutral\":\n                    neutral = False\n                if score.lower() == \"entailment\":\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                    filtered_docs.append(d)\n                    break\n            if (neutral and neutral_acceptance) and score.lower() != \"entailment\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    \n    if test_mode:\n        #todo misura metriche con sklearn classifier\n        #0 tweet veri, #1 tweet falsi\n        y_true = [int(document.metadata.get(\"label\")) for document in documents]\n        y_pred = [0 if document in filtered_docs else 1 for document in documents]\n        #print(f\"Real docs with aspect {aspect_id} ,y_true: {y_true}\") #Debug\n        #print(f\"Filtered docs with aspect {aspect_id} ,y_pred: {y_pred}\") #Debug\n        report = sklearn.metrics.classification_report(y_true,y_pred,labels=[0,1],\n                                                       output_dict=True,zero_division=0)\n        my_dict = {f\"aspect_{aspect_id}\": {\"query\": query, \"report\": report}}\n        observer.generated_queries.update(my_dict)\n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose,llm):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    \n    unbiased_answer = debiasing_answer(llm).invoke({\"text\": answer})\n    \n    return {\"answers_agent\": [unbiased_answer]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    \n    bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n    answer = bias_detection(answer)\n    if verbose: print(answer[0]['label']) #Biased, Non-biased\n    \n    return answer[0]['label']\n\n    \n#Not used\ndef route_question(state,verbose,llm):\n    \"\"\"\n    Route question to web search or RAG.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    if verbose:\n        print(\"---ROUTE QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    #print(queries)\n    source = question_router(llm).invoke({\"question\": query})\n    #print(source)\n    #print(source[\"datasource\"])\n    if source[\"datasource\"] == \"web_search\":\n        if verbose: print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return \"websearch\"\n    elif source[\"datasource\"] == \"vectorstore\":\n        if verbose: print(\"---ROUTE QUESTION TO RAG---\")\n        return \"vectorstore\"\n\n\ndef decide_to_generate(state,verbose):\n    \"\"\"\n    Determines whether to generate an answer, or add web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: RELEVANT---\")\n        return \"relevant\"\n\n\n# Not used\ndef grade_generation_v_documents_and_question(state,verbose,llm):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n    \n    if verbose:\n        print(\"---CHECK HALLUCINATIONS---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    my_answer = state[\"my_answer\"]\n\n    score = hallucination_grader(llm).invoke(\n        {\"documents\": documents, \"generation\": my_answer} #answers_agent[0]\n    )\n    if verbose: print(f\"score: {score}\")\n    grade = score[\"score\"]\n\n    # Check hallucination\n    if grade == \"yes\":\n        if verbose: print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        if verbose: print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": query, \"generation\": my_answer})\n        grade = score[\"score\"]\n        if grade == \"yes\":\n            if verbose: print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            if verbose: print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        if verbose: pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"","metadata":{"id":"6iV1oJJtLcFM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Workflow condizionale\ndef workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, \n                                                    retriever=configs.retriever, retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, \n                                                    llm=configs.local_llm, fairness=configs.fairness))  # generatae\n    \n    if configs.web_search:\n        workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose))  # web search\n        workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, \n                                                               llm=configs.local_llm))  # grade documents\n    if configs.safeness:\n        workflow.add_node(\"hate_speech_filter\", functools.partial(hate_speech_filter, verbose=configs.verbose, \n                                                                  llm=configs.local_llm))\n    if configs.trustworthiness:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, BART_model=configs.BART_model, strategy_entailment=configs.strategy_entailment, \n                                                                 neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.local_llm, \n                                                                 test_mode=configs.test_mode, observer=configs.observer))  # entailment\n    if configs.fairness:  \n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose, llm=configs.local_llm))\n        workflow.add_node(\"confirm_answer\", functools.partial(confirm_answer, verbose=configs.verbose))\n\n    # Non applichiamo il routing\n    \"\"\"\n    workflow.add_conditional_edges(\n        START,\n        route_question,\n        {\n            \"websearch\": \"websearch\", #se la risposta è websearch, allora vai al nodo websearch\n            \"vectorstore\": \"retrieve\", #se la risposta è vectorstore, allora vai al nodo retrieve\n        },\n    )\n    \"\"\"\n    \n    workflow.add_edge(START, \"retrieve\")\n    \n    if configs.web_search:\n        workflow.add_edge(\"retrieve\", \"grade_documents\")  \n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            functools.partial(decide_to_generate, verbose=configs.verbose),\n            {\n                \"websearch\": \"websearch\",\n                \"relevant\": \"hate_speech_filter\" if configs.safeness else \"entailment_filter\" if configs.trustworthiness else \"generate\"\n            },\n        )\n    \n    if configs.safeness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"hate_speech_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"hate_speech_filter\")\n        if configs.trustworthiness:\n            workflow.add_edge(\"hate_speech_filter\", \"entailment_filter\")\n            workflow.add_edge(\"entailment_filter\", \"generate\")\n        else:\n            workflow.add_edge(\"hate_speech_filter\",\"generate\")\n    elif configs.trustworthiness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"entailment_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n    else:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"generate\")\n        else: \n            workflow.add_edge(\"retrieve\", \"generate\")\n    \n    if configs.fairness:\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose),\n            {\n                \"Biased\": \"debiasing_filter\",\n                \"Non-biased\": \"confirm_answer\",\n            },\n        )\n        workflow.add_edge(\"confirm_answer\", END)\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"generate\", END)\n\n    # Non faccio il controllo sulle allucinazioni\n    \"\"\"\n    workflow.add_conditional_edges(\n        \"generate\",\n        grade_generation_v_documents_and_question,\n        {\n            \"not supported\": \"generate\",\n            \"useful\": END,\n            \"not useful\": \"websearch\",\n        },\n    )\n    \"\"\"    \n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    queries: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    final_answer: str\n\n\ndef generate_queries(state,verbose,llm):\n    \"\"\"\n    Generate multi-aspect queries from the starting question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains the multi aspect queries\n    \"\"\"\n    if verbose: \n        print(\"---GENERATE MULTI-ASPECTS QUERIES---\")\n        print(f\"State: {state}\")\n    question = state[\"question\"]\n    aspects = state[\"aspects\"]\n\n    generation = query_generator(llm).invoke({\"original_query\": question, \"aspects\": aspects})\n    #print(list(generation.values())) #Debug\n    return {\"queries\": eval(generation)}\n\ndef send_queries(state,verbose):\n    if verbose: \n        print(\"---SEND MULTI-ASPECTS QUERIES---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"query\": q, \"aspect_id\": state[\"queries\"].index(q)}) for q in state[\"queries\"]]\n\ndef organize_answers(state,verbose,llm):\n    \"\"\"\n    Organize the outputs of the agents.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, answer, that contains the final answer to give to user\n    \"\"\"\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n    \n    #print(f\"I'm the master agent and I received: {answers_agent}\") #Debug\n\n    final_output =final_answer(llm).invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"generate_queries\", functools.partial(generate_queries, verbose=configs.verbose, llm=configs.local_llm))\n    master_flow.add_node(\"organize_answers\", functools.partial(organize_answers, verbose=configs.verbose, llm=configs.local_llm))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_edge(START, \"generate_queries\")\n    master_flow.add_conditional_edges(\"generate_queries\", functools.partial(send_queries, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_answers\")\n    master_flow.add_edge(\"organize_answers\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"markdown","source":"Vectorstore configuration","metadata":{}},{"cell_type":"code","source":"index_name = \"entailment-test\"\naspects = [\"Health\",\"Technology\",\"Society\"] #Technology #Society\n\ntop_retriever = 10 #documents retrieved by retriever\ntop_KBT = 5 #documents retrievede by KBT retriever\n\nretriever = create_retriever(index_name, top_retriever)\nretrievers_KBT = create_KBT_retrievers(aspects, top_KBT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,retriever,retrievers_KBT,aspects):\n        self.local_llm = \"llama3.1\" #\"llama3.1:70b\"\n        \n        self.retriever = retriever\n        self.retrievers_KBT = retrievers_KBT\n        \n        self.aspects = aspects\n        \n        #if we want print all the process: True\n        self.verbose = False \n        \n        #if we want to include websearch in the workflow\n        self.web_search = False\n\n        # Controlling properties\n        self.safeness = False # if we want to add hate speech detection module\n        self.trustworthiness = True # if we want to add entailment module with KBT\n        self.fairness = False  # if we want to add debiasing module.\n\n        #Controlling entailment\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = True\n\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True\n        \n        # True: uses BART model for the entailment, False: uses LLM\n        self.BART_model = False\n        \n        # For testing\n        self.test_mode = True\n        self.observer = None\n    \nconfigs = Config(retriever,retrievers_KBT,aspects)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting language model","metadata":{}},{"cell_type":"code","source":"start_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:28:06.414161Z","iopub.execute_input":"2024-08-08T17:28:06.415049Z","iopub.status.idle":"2024-08-08T17:28:14.921185Z","shell.execute_reply.started":"2024-08-08T17:28:06.415008Z","shell.execute_reply":"2024-08-08T17:28:14.919917Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n2024/08/08 17:28:06 routes.go:1108: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\ntime=2024-08-08T17:28:06.438Z level=INFO source=images.go:781 msg=\"total blobs: 7\"\ntime=2024-08-08T17:28:06.438Z level=INFO source=images.go:788 msg=\"total unused blobs removed: 0\"\ntime=2024-08-08T17:28:06.439Z level=INFO source=routes.go:1155 msg=\"Listening on 127.0.0.1:11434 (version 0.3.4)\"\ntime=2024-08-08T17:28:06.440Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama867434731/runners\ntime=2024-08-08T17:28:14.028Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]\"\ntime=2024-08-08T17:28:14.028Z level=INFO source=gpu.go:204 msg=\"looking for compatible GPUs\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/08 - 17:28:14 | 200 |      61.171µs |       127.0.0.1 | HEAD     \"/\"\n\u001b[?25lpulling manifest ⠋ \u001b[?25h","output_type":"stream"},{"name":"stderr","text":"time=2024-08-08T17:28:14.295Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-abbc0562-4230-3ff1-7dd6-cf4f9e15f779 library=cuda compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"936.1 MiB\"\ntime=2024-08-08T17:28:14.295Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-00a7bd91-7c7e-b9cd-b294-8ee529faabb4 library=cuda compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.5 GiB\"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h[GIN] 2024/08/08 - 17:28:14 | 200 |  516.437061ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u001b[?25h\nError: pull model manifest: file does not exist\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = {\"question\": \"Covid19 was a hoax?\", \"aspects\": configs.aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\nanswer = value[\"final_answer\"]\nprint(answer)","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Funzioni di supporto:\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n    \nqueries_gpt = [\"Does the COVID-19 vaccine contain a microchip for controlling people?\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Does inhaling hot steam kill the COVID-19 virus?\",\n\"Was COVID-19 created in a lab as a biological weapon?\",\n\"Does drinking alcohol help prevent COVID-19 infection?\",\n\"Is 5G responsible for spreading COVID-19?\",\n\"Do masks cause a reduction in blood oxygen levels?\",\n\"Do COVID-19 vaccines cause infertility in women?\",\n\"Can high doses of vitamin C cure COVID-19?\",\n\"Can pets transmit COVID-19 to humans?\"]\n\n#write_file(\"/kaggle/working/gpt_queries.json\",queries_gpt)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:28:14.924869Z","iopub.execute_input":"2024-08-08T17:28:14.925317Z","iopub.status.idle":"2024-08-08T17:28:14.935375Z","shell.execute_reply.started":"2024-08-08T17:28:14.925279Z","shell.execute_reply":"2024-08-08T17:28:14.934082Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"Predisposizione dell'observer per salvare i risultati","metadata":{}},{"cell_type":"code","source":"class Observer(object):\n    def __init__(self):\n        self.query=\"\"\n        self.type_of_acceptance=\"\"\n        self.neutral_acceptance=False\n        self.generated_queries={}\n        self.final_answer=\"\"\n    \n    def generate_dict(self):\n        return {\"query\": self.query,\n               \"type_of_acceptance\": self.type_of_acceptance,\n               \"neutral_acceptance\": self.neutral_acceptance,\n               \"generated_queries\": self.generated_queries,\n               \"final_answer\": self.final_answer}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combinazioni di configurazione con skeptical/credulous e neutral","metadata":{}},{"cell_type":"markdown","source":"Multiple combination:","metadata":{}},{"cell_type":"code","source":"combination = [(True,True), (True,False), (False,True),(False,False)] \n\nfor comb in combination:\n    print(\"Start combination\")\n    configs.strategy_entailment =  comb[0]\n    configs.neutral_acceptance = comb[1]\n    \n    queries_list = queries_gpt\n\n    attempt = 1\n    ret_dict = {}\n    for query in queries_list:\n        inputs = {\"question\": query, \"aspects\": configs.aspects}\n        print(f\"Attempt {attempt} start\")\n    \n        configs.observer = Observer()\n        configs.observer.query=query\n        if configs.strategy_entailment:\n            configs.observer.type_of_acceptance=\"Skeptical\"\n        else:\n            configs.observer.type_of_acceptance=\"Credulous\"\n        configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n        for output in master_flow(configs).stream(inputs):\n            for key, value in output.items():\n                pass\n                #pprint(f\"Finished running: {key}:\")\n        answer = value[\"final_answer\"]\n    \n        configs.observer.final_answer= answer\n        ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n        attempt = attempt + 1\n\n    stringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\n    write_file(f\"/kaggle/working/test_llama31_third_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:28:14.937127Z","iopub.execute_input":"2024-08-08T17:28:14.937602Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[GIN] 2024/08/08 - 17:28:14 | 200 |       33.97µs |       127.0.0.1 | HEAD     \"/\"\nStart combination\nAttempt 1 start\n[GIN] 2024/08/08 - 17:28:14 | 200 |   44.478171ms |       127.0.0.1 | POST     \"/api/show\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25htime=2024-08-08T17:28:15.263Z level=INFO source=sched.go:710 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-4f6dc812262ac5e1a74791c2a86310ebba1aa1804fa3cd1c216f5547a620d2f2 gpu=GPU-00a7bd91-7c7e-b9cd-b294-8ee529faabb4 parallel=4 available=15615524864 required=\"6.2 GiB\"\ntime=2024-08-08T17:28:15.263Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.5 GiB]\" memory.required.full=\"6.2 GiB\" memory.required.partial=\"6.2 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.2 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-08-08T17:28:15.265Z level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"/tmp/ollama867434731/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-4f6dc812262ac5e1a74791c2a86310ebba1aa1804fa3cd1c216f5547a620d2f2 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 40809\"\ntime=2024-08-08T17:28:15.265Z level=INFO source=sched.go:445 msg=\"loaded runners\" count=1\ntime=2024-08-08T17:28:15.265Z level=INFO source=server.go:592 msg=\"waiting for llama runner to start responding\"\ntime=2024-08-08T17:28:15.265Z level=INFO source=server.go:626 msg=\"waiting for server to become available\" status=\"llm server error\"\n\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"140569441091584\" timestamp=1723138095\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"140569441091584\" timestamp=1723138095 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"40809\" tid=\"140569441091584\" timestamp=1723138095\n","output_type":"stream"},{"name":"stderr","text":"llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-4f6dc812262ac5e1a74791c2a86310ebba1aa1804fa3cd1c216f5547a620d2f2 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 2\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25hllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25htime=2024-08-08T17:28:15.516Z level=INFO source=server.go:626 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\n\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25hllm_load_vocab: special tokens cache size = 256\n\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25hllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25hllm_load_tensors: ggml ctx size =    0.27 MiB\n\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25hllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4156.00 MiB\n\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25hllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25htime=2024-08-08T17:28:18.974Z level=INFO source=server.go:631 msg=\"llama runner started in 3.71 seconds\"\n\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"140569441091584\" timestamp=1723138098\n[GIN] 2024/08/08 - 17:28:18 | 200 |  3.991167992s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/08 - 17:28:20 | 200 |  5.323011795s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:25 | 200 |  2.793204773s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:25 | 200 |  2.843618364s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:26 | 200 |  2.056602843s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:26 | 200 |  1.179514314s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:27 | 200 |  1.803284231s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:28 | 200 |  2.480916586s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:28 | 200 |  1.968207324s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:29 | 200 |  2.066734676s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:29 | 200 |  1.059182859s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:29 | 200 |  1.140350973s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:30 | 200 |   785.58237ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:30 | 200 |  860.705719ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:30 | 200 |  896.791712ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:31 | 200 |  863.831909ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:31 | 200 |  880.986464ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:31 | 200 |  1.182527944s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:32 | 200 |  945.386528ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:32 | 200 |  660.842666ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:33 | 200 |  1.333718787s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:33 | 200 |  1.316125083s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:33 | 200 |  1.350823168s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:33 | 200 |  430.081601ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:34 | 200 |  912.562691ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:34 | 200 |  817.847534ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:34 | 200 |  701.644678ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:35 | 200 |    866.3049ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:35 | 200 |  861.011419ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:35 | 200 |  854.097732ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:35 | 200 |  590.810351ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:36 | 200 |  930.138935ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:36 | 200 |  974.381892ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:36 | 200 |  750.281017ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:36 | 200 |  733.870343ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:36 | 200 |  694.048523ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:37 | 200 |  791.091539ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:37 | 200 |  784.266021ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:38 | 200 |  988.502556ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:38 | 200 |  1.036010229s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:39 | 200 |  891.713127ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:39 | 200 |  3.189043236s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:40 | 200 |  1.171887609s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:40 | 200 |  1.175509761s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:41 | 200 |  857.700776ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:41 | 200 |  895.585898ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:42 | 200 |  691.462504ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:42 | 200 |  687.510831ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:43 | 200 |  791.526343ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:43 | 200 |  690.439804ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:44 | 200 |  821.932266ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:45 | 200 |  678.520328ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:45 | 200 |  6.779005371s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:46 | 200 |  622.981853ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:46 | 200 |  547.772576ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:47 | 200 |  609.684771ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:48 | 200 |   5.53524588s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:48 | 200 |  838.849468ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:48 | 200 |  285.797886ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:49 | 200 |  482.520446ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:51 | 200 |  2.861434379s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:28:59 | 200 |  7.677119174s |       127.0.0.1 | POST     \"/api/chat\"\nAttempt 2 start\n[GIN] 2024/08/08 - 17:29:01 | 200 |  1.629374607s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:06 | 200 |  3.427395483s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:07 | 200 |  4.163090717s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:08 | 200 |  5.354477975s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:08 | 200 |  2.253588844s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:08 | 200 |  1.657153723s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:09 | 200 |  849.045026ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:09 | 200 |  1.315654208s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:10 | 200 |  1.050793213s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:10 | 200 |  1.208141649s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:10 | 200 |  1.051245419s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:11 | 200 |  1.016212941s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:12 | 200 |  1.106102851s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:12 | 200 |  1.105282552s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:12 | 200 |  1.104904641s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:12 | 200 |  784.319638ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:13 | 200 |  984.292247ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:13 | 200 |  1.099637296s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:13 | 200 |  806.834469ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:13 | 200 |  527.871087ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:14 | 200 |  648.734851ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:14 | 200 |  734.470217ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:14 | 200 |  486.070551ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:15 | 200 |  918.336646ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:15 | 200 |  503.828039ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:15 | 200 |  2.714481601s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:15 | 200 |  566.401255ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:16 | 200 |  629.603036ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:16 | 200 |  828.026888ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:16 | 200 |  948.736628ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:17 | 200 |  815.352351ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:17 | 200 |   990.49734ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:18 | 200 |  714.309455ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:18 | 200 |  708.916126ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:19 | 200 |  671.071527ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:19 | 200 |  3.122259183s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:19 | 200 |  667.766661ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:19 | 200 |  726.575732ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:20 | 200 |  834.132353ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:20 | 200 |  962.073853ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:21 | 200 |  918.747364ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:21 | 200 |  519.563116ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:22 | 200 |  1.276241926s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:22 | 200 |  3.100950325s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:22 | 200 |  929.224875ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:23 | 200 |  1.007195861s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:23 | 200 |  983.903993ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:24 | 200 |  949.067402ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:24 | 200 |  950.680662ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:25 | 200 |  1.079777723s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:25 | 200 |  1.036737901s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:25 | 200 |  3.105071313s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:26 | 200 |  1.089182834s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:26 | 200 |  1.132586021s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:27 | 200 |   873.61873ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:27 | 200 |  919.985876ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:28 | 200 |  813.140156ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:28 | 200 |  908.042954ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:28 | 200 |  503.513942ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:28 | 200 |  3.365631593s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:29 | 200 |  1.146046872s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:29 | 200 |  738.174038ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:30 | 200 |  1.121042647s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:30 | 200 |  1.207341232s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:31 | 200 |  833.448808ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:31 | 200 |  926.613845ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:32 | 200 |  501.881462ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:32 | 200 |  3.634608735s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:32 | 200 |   1.09742617s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:32 | 200 |  718.569085ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:33 | 200 |  1.016004561s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:33 | 200 |  994.669134ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:34 | 200 |  765.999056ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:34 | 200 |   1.14787716s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:35 | 200 |  733.906569ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:35 | 200 |  810.593511ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:36 | 200 |  840.368178ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:36 | 200 |  805.825261ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:37 | 200 |  1.224955962s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:38 | 200 |  627.555043ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:38 | 200 |    469.5155ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:39 | 200 |  6.519709378s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:40 | 200 |  1.524166319s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:40 | 200 |   4.63805261s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:41 | 200 |   1.35789904s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:43 | 200 |  1.387039471s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:44 | 200 |   1.22359973s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:45 | 200 |  1.197684173s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:45 | 200 |  432.804564ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:46 | 200 |  278.064777ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:47 | 200 |  1.673486428s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:29:54 | 200 |  5.979286428s |       127.0.0.1 | POST     \"/api/chat\"\nAttempt 3 start\n[GIN] 2024/08/08 - 17:29:55 | 200 |  1.896953218s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:01 | 200 |  3.848232788s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:02 | 200 |  5.194967604s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:03 | 200 |  5.253775295s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:03 | 200 |  1.691068869s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:03 | 200 |  967.103943ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:03 | 200 |  788.527217ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:04 | 200 |  754.523079ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:04 | 200 |  693.399199ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:04 | 200 |  786.965977ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:04 | 200 |  712.170474ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:05 | 200 |  424.357914ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:05 | 200 |  651.435357ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:05 | 200 |  958.680547ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:05 | 200 |  824.315281ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:06 | 200 |  730.915236ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:06 | 200 |  702.028188ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:06 | 200 |  975.657119ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:06 | 200 |  768.775386ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:07 | 200 |  731.058728ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:08 | 200 |   1.26795177s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:08 | 200 |  1.237588028s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:08 | 200 |  995.840706ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:09 | 200 |  1.079116935s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:09 | 200 |  1.277276495s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:09 | 200 |  1.317515798s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:10 | 200 |  882.198248ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:10 | 200 |  671.683737ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:11 | 200 |  1.522444091s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:11 | 200 |  1.020237939s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:11 | 200 |   1.01955694s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:12 | 200 |  1.228160191s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:12 | 200 |  1.178239951s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:12 | 200 |  1.215186066s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:13 | 200 |  801.018981ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:13 | 200 |  803.838916ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:13 | 200 |  809.807831ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:14 | 200 |  727.722443ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:14 | 200 |   824.94287ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:14 | 200 |  776.054631ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:14 | 200 |  829.631323ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:14 | 200 |  830.403911ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:14 | 200 |  832.062746ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:15 | 200 |  722.917723ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:15 | 200 |  723.675691ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:15 | 200 |  718.505649ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:16 | 200 |  660.817472ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:16 | 200 |  818.405152ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:16 | 200 |  778.171857ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:16 | 200 |  542.654016ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:17 | 200 |  1.011812488s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:17 | 200 |  1.269070817s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:17 | 200 |  949.557651ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:18 | 200 |  1.363236724s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:19 | 200 |  1.320275931s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:19 | 200 |  1.668872461s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:19 | 200 |  550.634477ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:20 | 200 |  991.863122ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:20 | 200 |  1.026067006s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:20 | 200 |  1.027787985s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:21 | 200 |  1.074965044s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:21 | 200 |  1.252507645s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:21 | 200 |  693.006989ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:22 | 200 |  627.493905ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:22 | 200 |  620.248229ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:23 | 200 |  835.010742ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:23 | 200 |  799.227473ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:24 | 200 |  825.724636ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:24 | 200 |  826.161683ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:24 | 200 |  759.799727ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:25 | 200 |  766.647987ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:25 | 200 |  881.063143ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:25 | 200 |   841.46926ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:26 | 200 |  692.999309ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:26 | 200 |  684.925267ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:27 | 200 |  759.779197ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:27 | 200 |  763.690243ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:28 | 200 |  710.249282ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:28 | 200 |  710.665674ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:28 | 200 |  734.318526ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:28 | 200 |  732.333464ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:28 | 200 |  8.419106551s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:29 | 200 |  563.979606ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:29 | 200 |   557.52714ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:30 | 200 |  727.002776ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:30 | 200 |  723.184837ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:30 | 200 |  564.306023ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:30 | 200 |  565.761161ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:31 | 200 |  961.246112ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:32 | 200 |  567.280906ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:32 | 200 |  517.890659ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:33 | 200 |  345.818805ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:33 | 200 |   602.30952ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:34 | 200 |   469.40495ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:34 | 200 |  508.582901ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:35 | 200 |  457.903253ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:36 | 200 |  6.207235239s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:38 | 200 |  2.690341046s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:46 | 200 |  8.414039068s |       127.0.0.1 | POST     \"/api/chat\"\nAttempt 4 start\n[GIN] 2024/08/08 - 17:30:48 | 200 |  1.756177006s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:52 | 200 |  2.330329609s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:54 | 200 |  3.858803177s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:55 | 200 |  5.077570543s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:55 | 200 |    2.9074122s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:55 | 200 |  1.339160403s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:55 | 200 |  458.450144ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:55 | 200 |  536.919649ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:56 | 200 |  662.298148ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:56 | 200 |   707.07392ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:56 | 200 |  748.933141ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:57 | 200 |  533.145985ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:57 | 200 |  711.800682ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:57 | 200 |  996.519577ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:57 | 200 |  635.597138ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:58 | 200 |  1.060857741s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:58 | 200 |  788.591874ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:58 | 200 |  758.013468ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:58 | 200 |  399.719081ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:59 | 200 |  767.532691ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:59 | 200 |  832.414026ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:30:59 | 200 |  650.982911ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:00 | 200 |  777.257792ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:00 | 200 |  676.570835ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:00 | 200 |  633.855828ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:01 | 200 |  784.631716ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:01 | 200 |  790.315525ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:01 | 200 |  789.709585ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:01 | 200 |  662.547402ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:01 | 200 |  762.945441ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:01 | 200 |  721.265546ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:02 | 200 |  859.995313ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:02 | 200 |  875.417075ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:02 | 200 |  960.667009ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:03 | 200 |  517.091392ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:03 | 200 |  669.235333ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:03 | 200 |  664.954466ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:04 | 200 |  712.137926ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:04 | 200 |   414.45629ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:04 | 200 |   286.29467ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:04 | 200 |  422.818203ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:05 | 200 |  632.348148ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:05 | 200 |  477.966693ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:06 | 200 |  1.856917521s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:06 | 200 |  567.021944ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:06 | 200 |  469.717207ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:07 | 200 |  480.628563ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:07 | 200 |  304.812552ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:08 | 200 |  278.009183ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:08 | 200 |  404.505303ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:08 | 200 |   390.43017ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:12 | 200 |   3.47087201s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:16 | 200 |  4.373332356s |       127.0.0.1 | POST     \"/api/chat\"\nAttempt 5 start\n[GIN] 2024/08/08 - 17:31:18 | 200 |  1.722023256s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:20 | 200 |  282.217924ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:21 | 200 |  623.132139ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:22 | 200 |  1.433652181s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:23 | 200 |  2.834546404s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:24 | 200 |  2.931161984s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:25 | 200 |  2.917671827s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:26 | 200 |  2.661210374s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:26 | 200 |  1.257158785s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:26 | 200 |  2.251169464s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:27 | 200 |  980.655934ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:27 | 200 |   960.54962ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:27 | 200 |  990.287318ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:27 | 200 |  866.002908ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:28 | 200 |  888.261846ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:28 | 200 |  968.885207ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:28 | 200 |  673.338216ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:29 | 200 |   1.06639001s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:29 | 200 |  1.278593305s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:29 | 200 |  1.049561193s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:30 | 200 |  829.446924ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:30 | 200 |  931.606619ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:31 | 200 |  1.025360663s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:31 | 200 |    1.5936547s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:31 | 200 |   1.31496196s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:31 | 200 |  569.986301ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:32 | 200 |  951.099894ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:32 | 200 |  715.633808ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:32 | 200 |  1.015860994s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:32 | 200 |  666.004934ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:33 | 200 |  660.937872ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:34 | 200 |  1.094032901s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:34 | 200 |  1.059731915s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:34 | 200 |  977.825141ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:34 | 200 |  505.523529ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:35 | 200 |  1.003678861s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:35 | 200 |  834.399849ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:35 | 200 |  1.297595766s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:36 | 200 |  1.252182587s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:36 | 200 |  1.587728103s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:36 | 200 |  801.615743ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:37 | 200 |  808.492831ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:37 | 200 |  1.110152581s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:37 | 200 |   889.68889ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:38 | 200 |   561.79043ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:38 | 200 |  939.568236ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:38 | 200 |  959.274007ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:39 | 200 |  1.247084076s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:39 | 200 |  911.671134ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:39 | 200 |   863.12461ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:39 | 200 |   418.73384ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:40 | 200 |  1.166199779s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:40 | 200 |  1.135708692s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:40 | 200 |  910.903443ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:41 | 200 |  849.881179ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:41 | 200 |    845.6009ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:41 | 200 |  837.269453ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:42 | 200 |  1.106210997s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:42 | 200 |    1.2811393s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:42 | 200 |  1.292531309s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:43 | 200 |  695.484373ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:43 | 200 |  739.682453ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:43 | 200 |   999.97283ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:43 | 200 |  620.181989ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:44 | 200 |  1.098815208s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:44 | 200 |  779.228263ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:44 | 200 |  852.881315ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:45 | 200 |  677.406996ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:45 | 200 |  657.823736ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:45 | 200 |  811.300458ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:45 | 200 |  534.211504ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:46 | 200 |  832.590013ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:46 | 200 |  683.450559ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:46 | 200 |  1.084397119s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:47 | 200 |  1.070140489s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:47 | 200 |  837.529494ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:48 | 200 |  1.878021472s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:48 | 200 |  871.205781ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:48 | 200 |  652.264425ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:48 | 200 |  795.504992ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:49 | 200 |  1.251306323s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:49 | 200 |  1.054243368s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:50 | 200 |  677.943384ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:50 | 200 |   1.48536651s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:50 | 200 |  1.220656321s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:50 | 200 |   684.96763ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:51 | 200 |  1.389174994s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:52 | 200 |  1.319617808s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:52 | 200 |  1.325081155s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:52 | 200 |  893.639558ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:53 | 200 |  888.856847ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:53 | 200 |  971.642353ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:53 | 200 |  796.151388ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:53 | 200 |  796.643878ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:54 | 200 |  1.036107073s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:54 | 200 |  832.674728ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:54 | 200 |  562.149011ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:55 | 200 |  1.258730524s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:55 | 200 |  1.015909156s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:55 | 200 |  1.092048571s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:56 | 200 |  1.193584708s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:56 | 200 |  1.246302767s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:57 | 200 |  787.915084ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/08/08 - 17:31:57 | 200 |    793.7469ms |       127.0.0.1 | POST     \"/api/chat\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Single combination:","metadata":{}},{"cell_type":"code","source":"#queries_list = load_queries(\"/kaggle/input/preference/gpt_queries.json\")\nqueries_list = queries_gpt\n\nattempt = 1\nret_dict = {}\nfor query in queries_list:\n    inputs = {\"question\": query, \"aspects\": configs.aspects}\n    print(f\"Attempt {attempt} start\")\n    \n    configs.observer = Observer()\n    configs.observer.query=query\n    if configs.strategy_entailment:\n        configs.observer.type_of_acceptance=\"Skeptical\"\n    else:\n        configs.observer.type_of_acceptance=\"Credulous\"\n    configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n    for output in master_flow(configs).stream(inputs):\n        for key, value in output.items():\n            pass\n            #pprint(f\"Finished running: {key}:\")\n    answer = value[\"final_answer\"]\n    \n    configs.observer.final_answer= answer\n    ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n    attempt = attempt + 1\n\nstringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\nwrite_file(f\"/kaggle/working/test_llama31_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Entailment evaluation","metadata":{}},{"cell_type":"code","source":"dict_input = load_json(\"/kaggle/working/test_llama31_second_Skeptical_Neutral.json\")\nnum_attempts = 10\nnum_aspects = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione su notizie vere (0) e notizie false (1)","metadata":{}},{"cell_type":"code","source":"# Label 0, notizie vere\n\nrecall = 0\nprecision = 0\nf1 = 0\nsupport = 0\n\nfor i in range(1,num_attempts+1):\n    for j in range(num_aspects):\n        recall = recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"0\"][\"recall\"]\n        precision = precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"0\"][\"precision\"]\n        f1 = f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"0\"][\"f1-score\"]\n        support = support + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"0\"][\"support\"]\n\nprint(f\"Recall: {recall/(num_attempts*num_aspects)}\")\nprint(f\"Precision: {precision/(num_attempts*num_aspects)}\")\nprint(f\"F1: {f1/(num_attempts*num_aspects)}\")\nprint(f\"Support: {support/(num_attempts*num_aspects)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label 1, notizie false\n\nrecall = 0\nprecision = 0\nf1 = 0\nsupport = 0\n\nfor i in range(1,num_attempts+1):\n    for j in range(num_aspects):\n        recall = recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"1\"][\"recall\"]\n        precision = precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"1\"][\"precision\"]\n        f1 = f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"1\"][\"f1-score\"]\n        support = support + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][\"1\"][\"support\"]\n\nprint(f\"Recall: {recall/(num_attempts*num_aspects)}\")\nprint(f\"Precision: {precision/(num_attempts*num_aspects)}\")\nprint(f\"F1: {f1/(num_attempts*num_aspects)}\")\nprint(f\"Support: {support/(num_attempts*num_aspects)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione per aspetto","metadata":{}},{"cell_type":"code","source":"aspect_id = 2\n\nrecall = 0\nprecision = 0\nf1 = 0\n\nweight_recall = 0\nweight_precision = 0\nweight_f1 = 0\naccuracy = 0\n\nfor i in range(1,num_attempts+1):\n    recall = recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"recall\"]\n    precision = precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"precision\"]\n    f1 = f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"f1-score\"]\n    weight_recall = weight_recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"weighted avg\"][\"recall\"]\n    weight_precision = weight_precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"weighted avg\"][\"precision\"]\n    weight_f1 = weight_f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"weighted avg\"][\"f1-score\"]\n    try:\n        accuracy = accuracy + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"accuracy\"]\n    except:\n        accuracy = accuracy + 1 #se l'accuracy non è presente nel dizionario, è perché vale 1 (vedi esempio 4 di llama_second_skeptical_neutral)\n    \nprint(f\"Recall for aspect {aspect_id}: {recall/(num_attempts)}\")\nprint(f\"Precision for aspect {aspect_id}: {precision/(num_attempts)}\")\nprint(f\"F1 for aspect {aspect_id}: {f1/(num_attempts)}\")\n\nprint(f\"Weighted Recall for aspect {aspect_id}: {weight_recall/(num_attempts)}\")\nprint(f\"Weighted Precision for aspect {aspect_id}: {weight_precision/(num_attempts)}\")\nprint(f\"Weighted F1 for aspect {aspect_id}: {weight_f1/(num_attempts)}\")\n\nprint(f\"Accuracy for aspect {aspect_id}: {accuracy/(num_attempts)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ragas (application evaluation)","metadata":{}},{"cell_type":"markdown","source":"https://docs.ragas.io/en/stable/","metadata":{}},{"cell_type":"code","source":"#todo","metadata":{},"execution_count":null,"outputs":[]}]}