{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U langchain-ai21 langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bias detection model\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\ntokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nweb_search_tool = TavilySearchResults(k=3)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ndef create_vectorstore(urls: list[str]):\n    docs = [WebBaseLoader(url).load() for url in urls] #text + meta-data on docs\n    docs_list = [item for sublist in docs for item in sublist] #ci serve l'attributo page_content\n\n    # Chunking\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents(docs_list)\n    index_name = \"vectorstore\"\n\n    # Add to vectorDB\n    vectorstore = PineconeVectorStore.from_documents(\n        documents=doc_splits,\n        #embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"cuda\"),\n        embedding=AI21Embeddings(device=\"cuda\"),\n        index_name=index_name\n    )\n    return vectorstore.as_retriever()","metadata":{"id":"Xfgup11aAFly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing KBTs","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n\n# metodo che permette di creare una lista di vectorstore\ndef create_KBTs(aspects, urls_list: list[list[str]]):\n    retriever_list = []\n    index=0\n    for urls in urls_list:\n        docs = [WebBaseLoader(url).load() for url in urls] #text + meta-data on docs\n        docs_list = [item for sublist in docs for item in sublist] #ci serve l'attributo page_content\n\n        # Chunking\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=250, chunk_overlap=0\n        )\n\n        doc_splits = text_splitter.split_documents(docs_list)\n        index_name = f\"{aspects[index].lower()}-kbt\"\n        \n        #debug:\n        #print(doc_splits)\n\n        # Add to vectorDB\n        vectorstore_KBT = PineconeVectorStore.from_documents(\n            documents=doc_splits,\n            #embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"cuda\"),\n            embedding=AI21Embeddings(device=\"cuda\"),\n            index_name=index_name\n        )\n        retriever_KBT = vectorstore_KBT.as_retriever()\n        retriever_list.append(retriever_KBT)\n        index=index+1\n    return retriever_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Query generation (multi-aspects)","metadata":{"id":"ThEK6sQSP4nC"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\ndef query_generator(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You have to generate multiple\n        search queries based on some specified aspects. You have to generate an answer as a Python list,\n        and in each position of the list there is the generated query of the aspect. NO PREAMBLE: return only the list.\n        Here some examples:\n        Original query: \"What about COVID19?\"\n        Aspects: [\"Health\",\"Economy\"]\n        Answer: [\"Symptoms of COVID19\",\"Economic consequences of COVID19\"]\n        \\n ----- \\n\n        Original query: \"COVID19 was fake?\"\n        Aspects: [\"Health\",\"Society\"]\n        Answer: [\"Is COVID19 just a cold?\",\"What people think about COVID19?\"]\n        \\n ----- \\n\n        <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Original query: {original_query}\n        Aspects: {aspects}\n        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"aspects\",\"original_query\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0) \n    query_generator = prompt | llm | StrOutputParser() \n    return query_generator\n\n\n#original_query = \"Covid19 was a hoax?\"\n#aspects = [\"Health\",\"Economy\"]\n#generation = query_generator(local_llm).invoke({\"original_query\": original_query, \"aspects\":aspects})\n#print(eval(generation))\n\n\n# # Reciprocal Rank Fusion algorithm\n# def reciprocal_rank_fusion(search_results_dict, k=60):\n#     fused_scores = {}\n#     print(\"Initial individual search result ranks:\")\n#     for query, doc_scores in search_results_dict.items():\n#         print(f\"For query '{query}': {doc_scores}\")\n\n#     for query, doc_scores in search_results_dict.items():\n#         for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n#             if doc not in fused_scores:\n#                 fused_scores[doc] = 0\n#             previous_score = fused_scores[doc]\n#             fused_scores[doc] += 1 / (rank + k)\n#             print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n\n#     reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n#     print(\"Final reranked results:\", reranked_results)\n#     return reranked_results","metadata":{"id":"ovAC1Rg8QACa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organizing outputs","metadata":{"id":"AeLcJqM3KK_Y"}},{"cell_type":"code","source":"def final_answer(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant that organizes and puts together many answers.\n        Make the summary of each answer, if it is necessary, and then put them together maintaining coherence in the discussion. No preamble, just give the final output. Expected JSON format.\n        <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Here are the answers: {answers}\n        Result answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"answers\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    final_answer = prompt | llm | JsonOutputParser()\n    return final_answer\n\n#final_output = final_answer.invoke({\"answers\": answers})\n#print(final_output)","metadata":{"id":"viQg-NxkKNv4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval","metadata":{"id":"kNT1n4npE6YP"}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\n#higher temperature more likely hallucinations\n\ndef retrieval_grader(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n        of a retrieved document to a user question. If the document contains keywords related to the user question,\n        grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n        Provide only the answer 'yes' or 'no', NOT ANYMORE. NO PREAMBLE. NO EXPLANATION.\n        <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Here is the retrieved document: \\n\\n {document} \\n\\n\n        Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n        \"\"\",\n        input_variables=[\"question\", \"document\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0) #higher temperature more likely hallucinations\n    retrieval_grader = prompt | llm | StrOutputParser()\n    return retrieval_grader\n\n# question = \"agent memory\"\n# docs = retriever.invoke(question)\n# doc_txt = docs[1].page_content\n# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))","metadata":{"id":"WzXfhJQ-E-mg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating answer","metadata":{"id":"MFgY6-seHsdQ"}},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\ndef rag_chain(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n        Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Question: {question}\n        Context: {context}\n        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"question\", \"document\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    # Chain\n    rag_chain = prompt | llm | StrOutputParser()\n    return rag_chain\n\n# Run\n# question = \"agent memory\"\n# docs = retriever.invoke(question)\n# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n# print(generation)","metadata":{"id":"DYmsobX4IIQD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hallucinations check (not used)","metadata":{"id":"0Ot0mK4hISuW"}},{"cell_type":"markdown","source":"Per ora non uso hallucinations check","metadata":{}},{"cell_type":"code","source":"def hallucination_grader(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n        an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' (both in lower case) to indicate\n        whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n        SINGLE KEY 'score' and NO preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n        Here are the facts:\n        \\n ------- \\n\n        {documents}\n        \\n ------- \\n\n        Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"generation\", \"documents\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    hallucination_grader = prompt | llm | JsonOutputParser()\n    return hallucination_grader\n\n#hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})","metadata":{"id":"zLqFdBSSIatd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Answer check","metadata":{"id":"Vky-iraRI7V4"}},{"cell_type":"code","source":"def answer_grader(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n        answer is useful to resolve a question. Give a binary score 'yes' or 'no' (both in lower case) to indicate whether the answer is\n        useful to resolve a question. Provide the binary score as a JSON with a SINGLE KEY 'score' and NO preamble or explanation.\n         <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n        \\n ------- \\n\n        {generation}\n        \\n ------- \\n\n        Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"generation\", \"question\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    answer_grader = prompt | llm | JsonOutputParser()\n    return answer_grader\n\n#answer_grader.invoke({\"question\": question, \"generation\": generation})","metadata":{"id":"nETsNNGuI9g_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Routing (not used)","metadata":{"id":"b4jwj6nZJBsA"}},{"cell_type":"markdown","source":"Per ora non applichiamo il routing!","metadata":{}},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\ndef question_router(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n        user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents,\n        prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords\n        in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n        or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n        no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"question\"],\n    )\n    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n    question_router = prompt | llm | JsonOutputParser()\n    return question_router\n    \n# question = \"llm agent memory\"\n# docs = retriever.get_relevant_documents(question)\n# doc_txt = docs[1].page_content\n# print(question_router.invoke({\"question\": question}))","metadata":{"id":"kDKQ-NA7JG9S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Document check (Entailment)","metadata":{}},{"cell_type":"code","source":"def entailment_checker(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You have to perform a task of textual entailments\n        between two documents. Give a binary score 'yes' or 'no' (both in lower case) to indicate whether the second document is\n        entailed by the first document. \n        Provide only the answer 'yes' or 'no', NOT ANYMORE. NO PREAMBLE. NO EXPLANATION.\n         <|eot_id|><|start_header_id|>user<|end_header_id|> The first document:\n        {first_doc}\n        \\n ------- \\n\n        Entails with:\n        \\n ------- \\n\n        {second_doc}\n        \\n ------- \\n\n        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"first_doc\", \"second_doc\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    entailment_checker = prompt | llm | StrOutputParser()\n    return entailment_checker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debiasing","metadata":{}},{"cell_type":"code","source":"# todo\n\ndef debiasing(local_llm):\n    prompt = PromptTemplate(\n        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> The user will pass you text cointaining biases.\n        Your role is to give a debiased version of the text. Give ONLY the debiased text, NO PREAMBLE, NO EXPLANATIONS.\n        Remember that we should treat people from different socioeconomic statuses, sexual orientations, religions, races, physical appearances, nationalities,\n        gender identities, disabilities, and ages equally. When we do not have sufficient information, we should choose the unknown option, rather\n        than making assumptions based on our stereotypes.\n         <|eot_id|><|start_header_id|>user<|end_header_id|> \n        {text}\n        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n        input_variables=[\"text\"],\n    )\n    llm = ChatOllama(model=local_llm, temperature=0)\n    debiasing = prompt | llm | StrOutputParser()\n    return debiasing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    query: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    \"\"\"\n    Retrieve documents from vectorstore and from KBT\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fair_trust):\n    \"\"\"\n    Generate answer using RAG on retrieved documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # RAG generation\n    generation = rag_chain(llm).invoke({\"context\": documents, \"question\": query})\n    if fair_trust:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation]}\n\n\ndef grade_documents(state,verbose,llm):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question\n    If any document is not relevant, we will set a flag to run web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Filtered out irrelevant documents and updated web_search state\n    \"\"\"\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = retrieval_grader(llm).invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if score.lower() == \"yes\":\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            web_search = \"Yes\"\n            continue\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose):\n    \"\"\"\n    Web search based based on the question\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Appended web results to documents\n    \"\"\"\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    if documents is not None:\n        documents.append(web_results)\n    else:\n        documents = [web_results]\n    return {\"documents\": documents, \"query\": query}\n\ndef hate_speech_detection(state):\n    #todo\n    return None\n\ndef entailment_filter(state,strategy_entailment,neutral_acceptance,verbose,llm):\n    \"\"\"\n    Filter documents that doesn't entail with KBT\n\n    Args:\n        state (dict): The current graph state\n    \"\"\"\n    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n\n    # Score each doc\n    filtered_docs = []\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            neutral = True\n            for d_kbt in documents:\n                score = entailment_checker(llm).invoke(\n                    {\"first_doc\": d_kbt.page_content, \"second_doc\": d.page_content}\n                )\n                #grade = score[\"score\"]\n                # Document entailed\n                if score.lower() != \"neutral\":\n                    neutral = False\n                if score.lower() == \"no\":\n                    # contraddiction found\n                    break\n            if not neutral or neutral_acceptance:\n                filtered_docs.append(d)\n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            neutral = True\n            for d_kbt in documents:\n                score = entailment_checker(llm).invoke(\n                    {\"first_doc\": d_kbt.page_content, \"second_doc\": d.page_content}\n                )\n                #grade = score[\"score\"]\n                # Document entailed\n                 if score.lower() != \"neutral\":\n                    neutral = False\n                if score.lower() == \"yes\":\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    filtered_docs.append(d)\n                    break\n            if neutral and neutral_acceptance:\n                filtered_docs.append(d)\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    \n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    \n    unbiased_answer = debiasing(llm).invoke({\"answer\": answer})\n    \n    return {\"answers_agent\": [unbiased_answer]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    \n    bias_detection = pipeline('text-classification', model=model, tokenizer=tokenizer, device=device) # cuda = 0,1 based on gpu availability\n    answer = bias_detection(answer)\n    if verbose: print(answer[0]['label']) #Biased, Non-biased\n    \n    return answer[0]['label']\n\n    \n#Not used\ndef route_question(state,verbose,llm):\n    \"\"\"\n    Route question to web search or RAG.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    if verbose:\n        print(\"---ROUTE QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    #print(queries)\n    source = question_router(llm).invoke({\"question\": query})\n    #print(source)\n    #print(source[\"datasource\"])\n    if source[\"datasource\"] == \"web_search\":\n        if verbose: print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return \"websearch\"\n    elif source[\"datasource\"] == \"vectorstore\":\n        if verbose: print(\"---ROUTE QUESTION TO RAG---\")\n        return \"vectorstore\"\n\n\ndef decide_to_generate(state,verbose):\n    \"\"\"\n    Determines whether to generate an answer, or add web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n\n\n# Not used\ndef grade_generation_v_documents_and_question(state,verbose,llm):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n    \n    if verbose:\n        print(\"---CHECK HALLUCINATIONS---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    my_answer = state[\"my_answer\"]\n\n    score = hallucination_grader(llm).invoke(\n        {\"documents\": documents, \"generation\": my_answer} #answers_agent[0]\n    )\n    if verbose: print(f\"score: {score}\")\n    grade = score[\"score\"]\n\n    # Check hallucination\n    if grade == \"yes\":\n        if verbose: print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        if verbose: print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": query, \"generation\": my_answer})\n        grade = score[\"score\"]\n        if grade == \"yes\":\n            if verbose: print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            if verbose: print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        if verbose: pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"","metadata":{"id":"6iV1oJJtLcFM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"def workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    # Define the nodes\n    workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose))  # web search\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, retriever=configs.retriever, retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, llm=configs.llm))  # grade documents\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, llm=configs.llm, fair_trust=configs.fair_trust))  # generatae\n    if fair_trust:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, strategy_entailment=configs.strategy_entailment, neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.llm))  # entailment\n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose)) \n\n    # Non applichiamo il routing\n    \"\"\"\n    workflow.add_conditional_edges(\n        START,\n        route_question,\n        {\n            \"websearch\": \"websearch\", #se la risposta è websearch, allora vai al nodo websearch\n            \"vectorstore\": \"retrieve\", #se la risposta è vectorstore, allora vai al nodo retrieve\n        },\n    )\n    \"\"\"\n    workflow.add_edge(START, \"retrieve\")\n\n    workflow.add_edge(\"retrieve\", \"grade_documents\")\n    workflow.add_conditional_edges(\n        \"grade_documents\",\n        functools.partial(decide_to_generate, verbose=configs.verbose),\n        {\n            \"websearch\": \"websearch\",\n            \"generate\": \"entailment_filter\" if fair_trust else \"generate\"\n        },\n    )\n    \n    if fair_trust:\n        workflow.add_edge(\"websearch\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose),\n            {\n                \"Biased\": \"debiasing_filter\",\n                \"Non-biased\": END,\n            },\n        )\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"websearch\", \"generate\")\n        workflow.add_edge(\"generate\", END)\n\n    # Non faccio il controllo sulle allucinazioni\n    \"\"\"\n    workflow.add_conditional_edges(\n        \"generate\",\n        grade_generation_v_documents_and_question,\n        {\n            \"not supported\": \"generate\",\n            \"useful\": END,\n            \"not useful\": \"websearch\",\n        },\n    )\n    \"\"\"    \n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(object).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    queries: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    final_answer: str\n\n\ndef generate_queries(state,verbose,llm):\n    \"\"\"\n    Generate multi-aspect queries from the starting question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains the multi aspect queries\n    \"\"\"\n    if verbose: \n        print(\"---GENERATE MULTI-ASPECTS QUERIES---\")\n        print(f\"State: {state}\")\n    question = state[\"question\"]\n    aspects = state[\"aspects\"]\n\n    generation = query_generator(llm).invoke({\"original_query\": question, \"aspects\": aspects})\n    #print(list(generation.values()))\n    return {\"queries\": eval(generation)}\n\ndef send_queries(state,verbose):\n    if verbose: \n        print(\"---SEND MULTI-ASPECTS QUERIES---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"query\": q, \"aspect_id\": state[\"queries\"].index(q)}) for q in state[\"queries\"]]\n\ndef organize_outputs(state,verbose,llm):\n    \"\"\"\n    Organize the outputs of the agents.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, answer, that contains the final answer to give to user\n    \"\"\"\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n\n    final_output =final_answer(llm).invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"generate_queries\", functools.partial(generate_queries, verbose=configs.verbose, llm=configs.llm))\n    master_flow.add_node(\"organize_queries\", functools.partial(organize_outputs, verbose=configs.verbose, llm=configs.llm))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_edge(START, \"generate_queries\")\n    master_flow.add_conditional_edges(\"generate_queries\", functools.partial(send_queries, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_queries\")\n    master_flow.add_edge(\"organize_queries\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(object).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"code","source":"class Config(object):\n    def __init__(self):\n        self.local_llm = \"llama3.1\" #\"llama3.1:70b\"\n        \n        #VectorStore configuration\n        urls = [\n            \"https://bmjgroup.com/celebrity-tweets-likely-shaped-us-negative-public-opinion-of-covid-19-pandemic/\",\n            \"https://eu.usatoday.com/story/news/health/2024/07/26/covid-vaccine-us-china-propaganda/74555829007/\",\n            \"https://www.theguardian.com/society/2023/jun/13/quarter-in-uk-believe-covid-was-a-hoax-poll-on-conspiracy-theories-finds\",\n        ]\n        #richiede lettura tramite pdf:\n        # \"https://www.ourcommons.ca/Content/Committee/441/HESA/Brief/BR11822476/br-external/LutzMitchell-e.pdf\"\n        self.retriever = create_vectorstore(urls)\n        \n        #Aspects and KBT configuration\n        #aspects and urls_list must be of the same size and ordered for each aspect.\n        aspects = [\"Health\",\"Economy\"] #Possible aspects: Health, Economy\n        urls_list = [[\"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/covid-19-vaccines\",\n                     \"https://www.who.int/news-room/questions-and-answers/item/vaccines-and-immunization-vaccine-safety\",\n                     \"https://www.bbc.com/news/stories-52731624\",\n                     \"https://www.bbc.com/news/technology-52903680\"],\n                    [\"https://www.worldbank.org/en/publication/wdr2022/brief/chapter-1-introduction-the-economic-impacts-of-the-covid-19-crisis\"]]\n        self.retrievers_KBT = create_KBTs(aspects, urls_list)\n        \n        #if we want print all the process: True\n        self.verbose = True \n\n        # if we want to add entailment with KBT, hate speech detection (todo) and debiasing module.\n        self.fair_trust = True\n\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = True\n\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configs = Config()\n\nstart_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = {\"question\": \"What about covid19?\", \"aspects\": aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"final_answer\"])\nanswer = value[\"final_answer\"]","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]}]}