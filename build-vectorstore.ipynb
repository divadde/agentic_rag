{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U trafilatura pinecone-client[grpc] langchain_experimental langchain-ai21 langchain-pinecone langchain-nomic langchain_community langchainhub chromadb langchain nomic[local] langchain-text-splitters","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:21:32.323472Z","iopub.execute_input":"2024-08-06T16:21:32.323880Z","iopub.status.idle":"2024-08-06T16:21:48.735265Z","shell.execute_reply.started":"2024-08-06T16:21:32.323841Z","shell.execute_reply":"2024-08-06T16:21:48.733917Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:56:25.391116Z","iopub.execute_input":"2024-08-06T15:56:25.391417Z","iopub.status.idle":"2024-08-06T15:56:25.396745Z","shell.execute_reply.started":"2024-08-06T15:56:25.391390Z","shell.execute_reply":"2024-08-06T15:56:25.395835Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Scraping","metadata":{}},{"cell_type":"markdown","source":"- https://trafilatura.readthedocs.io/en/latest/\n- https://www.diariodiunanalista.it/posts/chatbot-python-langchain-rag/\n- https://www.diariodiunanalista.it/posts/come-scraperare-un-blog-e-raccogliere-i-suoi-articoli/","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport time\nfrom trafilatura.sitemaps import sitemap_search\nfrom trafilatura import fetch_url, extract, extract_metadata\n\n\ndef get_urls_from_sitemap(resource_url: str) -> list:\n    \"\"\"\n    Funzione che recupera la sitemap attraverso Trafilatura\n    \"\"\"\n    urls = sitemap_search(resource_url)\n    return urls\n\n\ndef create_dataset(list_of_websites: list) -> pd.DataFrame:\n    \"\"\"\n    Funzione che crea un DataFrame Pandas di URL e articoli.\n    \"\"\"\n    data = []\n    for url in tqdm(list_of_websites, desc=\"Websites\"): #Per ogni sitoweb estraiamo il testo\n        html = fetch_url(url)\n        body = extract(html)\n        try:\n            metadata = extract_metadata(html)\n            title = metadata.title\n            description = metadata.description\n        except:\n            metadata = \"\"\n            title = \"\"\n            description = \"\"\n        d = {\n            'url': url,\n            \"body\": body,\n            \"title\": title,\n            \"description\": description\n        }\n        data.append(d)\n        time.sleep(0.5)\n    df = pd.DataFrame(data)\n    df = df.drop_duplicates()\n    df = df.dropna()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:56:25.397815Z","iopub.execute_input":"2024-08-06T15:56:25.398131Z","iopub.status.idle":"2024-08-06T15:56:26.501665Z","shell.execute_reply.started":"2024-08-06T15:56:25.398107Z","shell.execute_reply":"2024-08-06T15:56:26.500939Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"urls = [\n    \"https://bmjgroup.com/celebrity-tweets-likely-shaped-us-negative-public-opinion-of-covid-19-pandemic/\",\n    \"https://eu.usatoday.com/story/news/health/2024/07/26/covid-vaccine-us-china-propaganda/74555829007/\",\n    \"https://www.theguardian.com/society/2023/jun/13/quarter-in-uk-believe-covid-was-a-hoax-poll-on-conspiracy-theories-finds\",\n]\n\ndf = create_dataset(urls)\ndf.to_csv(\"/kaggle/working/dataset.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:56:26.503940Z","iopub.execute_input":"2024-08-06T15:56:26.504642Z","iopub.status.idle":"2024-08-06T15:56:29.676204Z","shell.execute_reply.started":"2024-08-06T15:56:26.504610Z","shell.execute_reply":"2024-08-06T15:56:29.675287Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Websites: 100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"aspects = [\"Health\",\"Technology\",\"Society\"] \nurls_list = [[\"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/covid-19-vaccines\",\n                \"https://www.who.int/news-room/questions-and-answers/item/vaccines-and-immunization-vaccine-safety\",\n                \"https://www.bbc.com/news/stories-52731624\",\n                \"https://www.bbc.com/news/technology-52903680\"],\n            [\"https://www.worldbank.org/en/publication/wdr2022/brief/chapter-1-introduction-the-economic-impacts-of-the-covid-19-crisis\"],\n            []]\n\nindex = 0\nfor aspect in aspects:\n    df = create_dataset(urls_list[index])\n    index=index+1\n    df.to_csv(f\"/kaggle/working/dataset_{aspect.lower()}_kbt.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:56:29.677297Z","iopub.execute_input":"2024-08-06T15:56:29.677585Z","iopub.status.idle":"2024-08-06T15:56:34.235683Z","shell.execute_reply.started":"2024-08-06T15:56:29.677559Z","shell.execute_reply":"2024-08-06T15:56:34.234762Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Websites: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]\nWebsites: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test splitters","metadata":{}},{"cell_type":"markdown","source":"text splitters: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/","metadata":{}},{"cell_type":"markdown","source":"Recursive character splitter fa la suddivisione senza staccare le parole, ma mettendole insieme.","metadata":{}},{"cell_type":"markdown","source":"https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/semantic-chunker/","metadata":{}},{"cell_type":"markdown","source":"https://python.langchain.com/v0.1/docs/integrations/document_transformers/ai21_semantic_text_splitter/","metadata":{}},{"cell_type":"code","source":"# Prova recursivecharactertextsplitter\nfrom langchain.document_loaders import DataFrameLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_ai21 import AI21SemanticTextSplitter\nfrom langchain_ai21 import AI21Embeddings\n\ntext_chunks_recursive = DataFrameLoader(\n        pd.read_csv(\"/kaggle/working/dataset.csv\"), page_content_column=\"body\"\n    ).load_and_split(\n        text_splitter=RecursiveCharacterTextSplitter(\n            chunk_size=500, chunk_overlap=0, length_function=len)\n    )\n\n#Sembra suddividere in pezzi più lunghi\ntext_chunks_semantic = DataFrameLoader(\n        pd.read_csv(\"/kaggle/working/dataset.csv\"), page_content_column=\"body\"\n    ).load_and_split(\n        text_splitter=SemanticChunker(\n    AI21Embeddings(device=\"cuda\"), breakpoint_threshold_type=\"percentile\"\n    )\n)\n    \n#Sembra suddividere in pezzi più piccoli\ntext_chunks_ai21 = DataFrameLoader(\n        pd.read_csv(\"/kaggle/working/dataset.csv\"), page_content_column=\"body\"\n    ).load_and_split(\n        text_splitter=AI21SemanticTextSplitter(chunk_size=500)\n    )\n\nprint(len(text_chunks_recursive))\nprint(len(text_chunks_semantic))\nprint(len(text_chunks_ai21))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:30:08.415495Z","iopub.execute_input":"2024-08-06T16:30:08.416261Z","iopub.status.idle":"2024-08-06T16:30:17.999160Z","shell.execute_reply.started":"2024-08-06T16:30:08.416226Z","shell.execute_reply":"2024-08-06T16:30:17.998222Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"9\n32\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creazione vectorstore","metadata":{}},{"cell_type":"markdown","source":"https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/","metadata":{}},{"cell_type":"markdown","source":"Creazione chunks","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import DataFrameLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\ndef create_chunks(dataset:pd.DataFrame, text_splitter):\n    \"\"\"\n    Crea chunk informazionali dal dataset \n\n    Args:\n        dataset (pd.DataFrame): Dataset Pandas\n        chunk_size (int): Quanti chunk informazionali?\n        chunk_overlap (int): Quanti chunk condivisi?\n\n    Returns:\n        list: lista di chunk\n    \"\"\"\n    text_chunks = DataFrameLoader(\n        dataset, page_content_column=\"body\"\n    ).load_and_split(text_splitter)\n    \n    # aggiungiamo i metadati ai chunk stessi per facilitare il lavoro di recupero\n    for doc in tqdm(text_chunks):\n        title = doc.metadata[\"title\"]\n        description = doc.metadata[\"description\"]\n        content = doc.page_content\n        url = doc.metadata[\"url\"]\n        final_content = f\"TITLE: {title}\\DESCRIPTION: {description}\\BODY: {content}\\nURL: {url}\"\n        doc.page_content = final_content\n\n    return text_chunks","metadata":{"execution":{"iopub.status.busy":"2024-08-05T15:14:08.310902Z","iopub.execute_input":"2024-08-05T15:14:08.311257Z","iopub.status.idle":"2024-08-05T15:14:08.739630Z","shell.execute_reply.started":"2024-08-05T15:14:08.311227Z","shell.execute_reply":"2024-08-05T15:14:08.738863Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Vector store","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\nindex_name = \"vectorstore\"\n\nchunks = create_chunks(dataset=pd.read_csv(\"/kaggle/working/dataset.csv\"),SemanticChunker(\n    AI21Embeddings(device=\"cuda\"), breakpoint_threshold_type=\"percentile\"))\n\n# Add to vectorDB\nvectorstore = PineconeVectorStore.from_documents(\n    documents=chunks,\n    #embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"cuda\"),\n    embedding=AI21Embeddings(),\n    index_name=index_name\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KBT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\naspects = [\"Health\",\"Technology\",\"Society\"]\n\nfor aspect in aspects:\n    index_name = f\"{aspect.lower()}-kbt\"\n    \n    chunks = create_chunks(dataset=pd.read_csv(f\"/kaggle/working/dataset_{aspect.lower()}_kbt.csv\"),chunk_size=100,chunk_overlap=0)\n\n    vectorstore_KBT = PineconeVectorStore.from_documents(\n        documents=chunks,\n        #embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"cuda\"),\n        embedding=AI21Embeddings(),\n        index_name=index_name\n    )","metadata":{"execution":{"iopub.status.busy":"2024-08-05T15:14:56.804595Z","iopub.execute_input":"2024-08-05T15:14:56.805522Z","iopub.status.idle":"2024-08-05T15:15:04.629207Z","shell.execute_reply.started":"2024-08-05T15:14:56.805470Z","shell.execute_reply":"2024-08-05T15:15:04.628434Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 30/30 [00:00<00:00, 159479.24it/s]\n100%|██████████| 6/6 [00:00<00:00, 41187.93it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Verifica","metadata":{}},{"cell_type":"code","source":"from pinecone.grpc import PineconeGRPC as Pinecone\n\n# Inizializza il client Pinecone con il tuo API key e ambiente\npc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n\nindex_name = \"prova\"\n\n# Ottenere la lista degli indici\nindex = pc.Index(index_name)\n\n# Stampare gli indici\nresponse = index.query(top_k=10, namespace=\"prova\", include_values=True, vector=[0.93,0.23,0.87,0.87,0.72,0.08,0.31,0.21,0.7,0.78,0.58,0.57,0.16,0.26,0.25,0.08,0.64,0.53,0.6,0.24,0.54,0.08,0.51,0.7,0.78,0.97,0.78,0.1,0.77,0.84,0.67,0.55,0.66,0.96,0.45,0.26,0.11,0.46,0.45,0.68,0.19,0.92,0.18,0.14,0.05,0.44,0.46,0.77,0.93,0.05,0.75,0.05,0.35,0.17,0.98,0.32,0.35,0.88,0.56,0.69,0.96,0.7,0.27,0.68,0.87,0.15,0.25,0.01,0.61,0.99,0.95,0.06,0.86,0.03,0.98,0.56,0.19,0.67,0.5,0.7,0.25,0.45,0.13,0.7,0.78,0.47,0.37,0.48,0.16,0.34,0.24,0.8,0.78,0.06,1,0.33,0.29,0.89,0.64,0.4,0.37,0.8,0.01,0.47,0.39,0.86,0.7,0.4,0.93,0.4,0.5,0.81,0.83,0.2,0.74,0.48,0.31,0.6,0.99,0.7,1,0.84,0.16,0.25,0.74,0.54,0.92,0.6,0.88,0.21,0.99,0.43,0.55,0.4,0.46,0.21,0.64,0.53,0.55,0.8,0.39,0.43,0.07,0.95,0.56,0.95,0.46,0.95,0.74,0.73,0.01,0.96,0.72,0.43,0.9,0.47,0.83,0.78,0.51,0.79,0.35,0.98,0.05,0.25,0.77,0.97,0.55,0.82,0.2,0.15,0.66,0.69,0.07,0.26,0.47,0.61,0.9,0.57,0.93,0.92,0.94,0.42,0.23,0.28,0.56,0.5,0.17,0.64,0.61,0.92,0.75,0.63,0.57,0.06,0.5,0.9,0.02,0.66,0.22,0.73,0.97,0.87,0.87,0.81,0.02,0.94,0.22,0.62,0.5,0.33,0.64,0.19,0.83,0.87,0.9,0.05,0.74,0.03,0.93,0.67,0.05,0.3,0.3,0.99,0.01,0.34,0.88,0.08,0.06,0.27,0.01,0.83,0.53,0.21,0.02,0.12,0.68,0.9,0.31,0.26,0.76,0.51,0.09,0.78,0.64,0.14,0.65,0.89,0.66,0.47,0.46,0.68,0.61,0.46,0.19,0.4,0.53,0.58,0.09,0.78,0.31,0.41,0.84,0.93,0.42,0.39,0.91,0.9,0.31,0.85,0.89,0.99,0.99,0.8,0.34,0.07,0.54,0.98,0.6,0.67,0.83,0.74,0.37,0.28,0.43,0.09,0.22,0.73,0.39,0.47,0.07,0.8,0.42,0.49,0.26,0.71,0.21,0.82,0.97,0.94,0.75,0.51,0.62,0.03,0.99,0.82,0.81,0.77,0.65,0.78,0.2,0.36,0.1,0.95,0.4,0.59,0.72,0.76,0.12,0.75,0.97,0.06,0.46,0.99,0.12,0.43,0.69,0.54,0.69,0.54,0.16,0.85,0.57,0.74,0.78,0.08,0.2,0.83,0.78,0.64,0.54,0.33,0.24,0.63,0.09,0.31,0.02,0.63,0.3,0.63,0.73,0.33,0.68,0.83,0.15,0.69,0.69,0.61,0.59,0.65,0.25,0.1,0.1,0.09,0.29,0.8,0.91,0.24,0.53,0.2,0.23,0.38,0.82,0.37,0.2,0.99,0.92,0.12,0.57,0.94,0.33,0.72,0.47,0.38,0.35,0.38,0.84,0.67,0.53,0.1,0.64,0.06,0.36,0.13,0.47,0.65,0.55,0,0.11,0.82,0.34,0.63,0.75,0.73,0.39,0.49,0.92,0.42,0.18,0.24,0.84,0.23,0.33,0.61,0.35,0.02,0.97,0.67,0.97,0.15,0.45,0.77,0.44,0.76,0.72,0.35,0.53,0.84,0.6,0.11,0.03,0.38,0.26,0.64,0.43,0.15,0.39,0.72,0.67,0.16,0.68,0.67,0.04,0.61,0.69,0.25,0.62,0.04,0.58,0.98,0.08,0.05,0.4,0.05,0.94,0.75,0.86,0.98,0.43,0.1,0.49,0.93,0.32,0.77,0.51,0.34,0.33,0.98,0.76,0.93,0.13,0.14,0.84,0.62,0.2,0.5,0.45,0.26,0.54,0.04,0.23,0.74,0.06,0.41,0.38,0.84,0.39,0.77,0.38,0.17,0.09,0.43,0.4,0.48,0.74,0.96,0.09,0.04,0.93,0.85,0.53,0.38,0.94,0.65,0.38,0.12,0.33,0.97,0.67,0.85,0.85,0.32,0.42,0.21,0.76,0.93,0.6,0.93,0.85,0.76,0.39,0.51,0.66,0.17,0.68,0.41,0.85,0.52,0.22,0.88,0.67,0.5,0.06,0.38,0.28,0.53,0.53,0.74,0.36,0.01,0.21,0.36,0.67,0.47,0.16,0.66,0.53,0.71,0.46,0.9,0.54,0.94,0.31,0.11,0.01,0.1,0.99,0.05,0.24,0.2,0.37,0.07,0.5,0.09,0.78,0.09,0.73,0.88,0.42,0.35,0.06,0.8,0.15,0.57,0.36,0.43,0.11,0.8,0.77,0.59,0.16,0.07,0.36,0.43,0.45,0.96,0.36,0.28,0.35,0.65,0.14,0.26,0.26,0.18,0.07,0.29,0.98,0.28,0.81,0.83,0.61,0.94,0.9,0.28,0.7,0.91,0.09,0.64,0.91,0.82,0.95,0.93,0.88,0.49,0.59,0.82,0.03,0.05,0.93,0.89,0.96,0.38,0.27,0.48,0.75,0.43,0.16,0.53,0.84,0.99,0.23,0.45,0.71,0.6,0.43,0.2,0.08,0.13,0.68,0.81,0.52,0.6,0.25,0.37,0.76,0.82,0.28,0.82,0.54,0.8,0.93,0.57,0.1,0.76,0.23,0.29,0.32,0.72,0.42,0.6,0.15,0.7,0.67,0.58,0.53,0.1,0.73,0.39,0.62,0.06,0.55,0.15,0.58,0.09,0.68,0.54,0.91,0.96,0.21,0.87,0.24,0.83,0.48,0.99,0.3,0.14,0.9,0.5,0.61,0.01,0.91,0.97,0.3,0.01,0.25,0.24,0.36,0.78,0.02,0.95,0.97,0.66,0.46,0.32,0.24,0.59,0.36,0.36,0.19,0.97,0.74,0.47,0.59,0.43,0.48,0.51,0.77,0,0.64,0.76,0.82,0.11,0.8,0.34,0.98,0.02,0.59,0.37,0.06,0.39,0.31,0.6,0.19,0.23,0.55,0.85,0.13,0.77,0.38,0.02,0.71,0.95,0.19,0.92,0.93,0.56,0.19,0.83,0.79,0.52,0.23,0.31,0.61,0.1,0.74,0.42,0.58,0.96,0.1,0.06,0.47,0.13,0.55,0.37,0.3,0.64,0.11,0.3,0.29,0.43,0.55,0.44,0.9,0.9,0.43,0.2,0.75,0.77,0.96,0.1,0.65,0.97,0.38,0.23,0.23,0.94,0.29,0.73,0.64,0.64,0.7,0.55,0.33,0.42,0.5,0.73,0.71,0.01,0.01,1,0.54,0.11,0.46,0.24,0.94,0.25,0.14,0.96,0.28,0.95,0.59,0.9,0.81,0.9,0.62,0.62,0.44,0.07,0.52,0.15,0.33,0.95,0.96,0.53,0.6,0.02,0.89,0.25,0.53,0.19,0.03,0.6,0.08,0.45,0.55,0.91,0.11,0.66,0.9,0.69,0.36,0.06,0.2,0.21,0.19,0.64,0.93,0.35,0.84,0.78,0.99,0.02,0.81,0.02,0.66,0.02,0.2,0.24,0.65,0.16,0.96,0.23,0.69,0.5,0.7,0.01,0.88,0.26,0.59,0.8,0.62,0.51,0.17,0.91,0.41,0.33,0.42,0.45,0,0.14,0.45,0.13,0.93,0.66,0.62,0.41,0.23,0.81,0.08,0.36,0.75,0.41,1,0.02,0.67,0.66,0.45,0.16,0.76,0.78,0.7,0.28,0.38,0.57,0.59,0.51,0.77,0.84,0.46,0.25,0.78,0.49,0.12,0.7,0.03,0.75,0.53,0.7,0.22,0.96,0.75,0.31,0.48,0.13,0.61,0.41,0.97,0.3,0.25,0.18,0.62,0.79,0.4,0.77,0.46,0.12,0.26,0.77,0.21,0.28,0.57,0.95,0.82,0.58,0.14,0.15,0.84,0.13,0.92,0.23,0.5,0.1,0.14,0.43,0.65,0.47,0.99,0.86,0.77,0.9,0.87,0.58,0.61,0.63,0.88,0.68,0.73,0.46,0.07,0.8,0.62,0.75,0.29,0.12,0.64,0.11,0.56,0.38,0.14,0.26,0.29,0.13,0.68,0.43,0.24,0.5,0.71,0.6,0.58,0.09,0.26,0.59,0.51,0.79,0.84,0.4,0.3,0.5,0.44,0.52,0.44,0.06,0.12,0.15,0.92,0.93,0.68,0.11,0.42,0.89,0.02,0.06,0.55,0.58,0.89,0.79,0.48,0.83,0.41,0.56,0.09,0.86,0.25,0.94])  # Recupera i primi 10000 elementi\nprint(response)\nitems = response['matches']\n\n# Stampa tutti gli elementi recuperati\nprint(\"Elementi presenti nell'indice:\")\nfor item in items:\n    print(f\"ID: {item['id']}, Vettore: {item['values']}\")","metadata":{},"execution_count":null,"outputs":[]}]}