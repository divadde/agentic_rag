{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9197853,"sourceType":"datasetVersion","datasetId":5560790}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 ragas langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bias detection model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_entailment/Textual%20Entailment%20Explanation%20Demo.html\n- https://huggingface.co/facebook/bart-large-mnli","metadata":{}},{"cell_type":"markdown","source":"Entailment model (BART):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbart_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\",device=device)\nbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BART_prediction(premise,hypothesis):\n    #print(f\"Premise: {premise}\")\n    #print(f\"Hypo: {hypothesis}\")\n    input_ids = bart_tokenizer.encode(premise, hypothesis, return_tensors=\"pt\")\n    logits = bart_model(input_ids)[0]\n    probs = logits.softmax(dim=1)\n\n    max_index = torch.argmax(probs).item()\n\n    bart_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n    return bart_label_map[max_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nweb_search_tool = TavilySearchResults(k=2)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"markdown","source":"osservazione sul chuncking: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338","metadata":{}},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\ndef create_retriever(index_name,top_k):\n    vectorstore = PineconeVectorStore(\n        index_name=index_name,\n        embedding=AI21Embeddings(device=\"cuda\")\n    )\n    return vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\ndef create_KBT_retrievers(aspects,top_k):\n    retrievers = []\n    for aspect in aspects:\n        retriever = create_retriever(f\"{aspect.lower()}-kbt\",top_k)\n        retrievers.append(retriever)\n    return retrievers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt support","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_community.llms import Ollama\n\ndef create_prompt(system_prompt, input_variables, examples=None,):\n    user_message = [(\"user\", f\"{{{input_var}}}\") for input_var in input_variables]\n    final_messages = [(\"system\",system_prompt)]\n    \n    if examples!=None:\n        example_message = [(\"user\", f\"{{{input_var}}}\") for input_var in input_variables]\n        example_message.append((\"assistant\", \"{output}\"))\n        example_prompt  = ChatPromptTemplate.from_messages(example_message)\n        few_shot_prompt = FewShotChatMessagePromptTemplate(\n            example_prompt=example_prompt,\n            examples=examples, \n        )\n        final_messages.append(few_shot_prompt)\n    \n    for mess in user_message:\n        final_messages.append(mess)\n    #print(final_messages)\n    return ChatPromptTemplate.from_messages(final_messages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prompt = create_prompt(\"Ciao stronzo\",[\"input\",\"zio\"],examples=[{\"input\":\"ciao\", \"zio\":\"pera\", \"output\":\"yes\"}])\n#prompt.invoke({\"input\":\"ciao\", \"zio\":\"peppe\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def call_model(llm: str, prompt: str, input_variables:list[str], examples=None):\n    model = Ollama(model=llm, temperature=0)\n    prompt_final = create_prompt(prompt, input_variables, examples)\n    return prompt_final | model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prompt(path):\n    with open(path, 'r') as file:\n        prompt = file.read()\n    return prompt\n\ndef get_examples(path):\n    with open(path, 'r') as file:\n        examples = file.read()\n    return eval(examples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://learnprompting.org/docs/reliability/debiasing","metadata":{}},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\nimport sklearn.metrics\nimport numpy as np\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    original_query: str\n    query: str\n    aspect: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n        \ndef rewrite_query(state,verbose,llm):\n    if verbose: \n        print(\"---REWRITING QUERY---\")\n        print(f\"State: {state}\")\n    original_query = state[\"original_query\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/query_rewriting.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/query_rewriting_examples.txt\")\n    chain = call_model(llm, prompt, [\"original_query\",\"aspect\"], examples)\n    generation = chain.invoke({\"original_query\": original_query, \"aspect\": aspect})\n    #print(list(generation.values())) #Debug\n    return {\"query\": generation}\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fairness):\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # RAG generation\n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/generating_answer.txt\")\n    chain = call_model(llm, prompt, [\"context\",\"question\"])\n    generation = chain.invoke({\"context\": documents, \"question\": query})\n    \n    aspect_id = state[\"aspect_id\"]\n    #print(f\"Aspect agent {aspect_id} generates: {generation}\") #Debug\n    if fairness:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation], \"ord_aspects\": [state[\"aspect\"]]}\n\n\ndef confirm_answer(state,verbose):\n    if verbose:\n        print(\"---CONFIRM ANSWER---\")\n        print(f\"State: {state}\")\n    my_answer = state[\"my_answer\"]\n\n    return {\"answers_agent\": [my_answer], \"ord_aspects\": [state[\"aspect\"]]}\n\n\ndef grade_documents(state,verbose,llm):\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/retrieval_grader.txt\")\n    chain = call_model(llm, prompt, [\"question\",\"document\"])\n\n    # Score each doc\n    filtered_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = chain.invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if \"yes\" in score.lower():\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            web_search = \"Yes\"\n            continue\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose):\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    #print(f\"docs from web: {docs}\") #Debug\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents([web_results])\n    for doc in doc_splits:\n        if documents is None:\n            documents = [doc]\n        else:\n            documents.append(doc)\n    return {\"documents\": documents, \"query\": query}\n\n\ndef hate_speech_filter(state,verbose,llm):\n    if verbose:\n        print(\"---HATE SPEECH FILTER---\")\n        print(f\"State: {state}\")\n    documents = state[\"documents\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/hate_speech.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/hate_speech_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    for d in documents:\n        score = chain.invoke(\n            {\"input\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        if \"no\" in score.lower():\n            if verbose: print(\"---DOCUMENT ACCEPTED---\")\n            filtered_docs.append(d)\n    return {\"documents\": filtered_docs}\n\n\ndef entailment_filter(state,BART_model,strategy_entailment,neutral_acceptance,verbose,test_mode,observer,llm):    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n    aspect_id = state[\"aspect_id\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/entailment_checker.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/entailment_checker_examples.txt\")\n    chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    #counter_docs = 0 # Debug\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"contradiction\" in score.lower():\n                    # contradiction found\n                    break\n            if (not neutral or neutral_acceptance) and score.lower() != \"contradiction\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug            \n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                # Document entailed\n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"entailment\" in score.lower():\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                    filtered_docs.append(d)\n                    break\n            if (neutral and neutral_acceptance) and score.lower() != \"entailment\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    \n    if test_mode:\n        #todo misura metriche con sklearn classifier\n        #0 tweet veri, #1 tweet falsi\n        y_true = [int(document.metadata.get(\"label\")) for document in documents]\n        y_pred = [0 if document in filtered_docs else 1 for document in documents]\n        #print(f\"Real docs with aspect {aspect_id} ,y_true: {y_true}\") #Debug\n        #print(f\"Filtered docs with aspect {aspect_id} ,y_pred: {y_pred}\") #Debug\n        report = sklearn.metrics.classification_report(y_true,y_pred,labels=[0,1],\n                                                       output_dict=True,zero_division=0)\n        my_dict = {f\"aspect_{aspect_id}\": {\"query\": query, \"report\": report}}\n        observer.generated_queries.update(my_dict)\n        \n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose,llm):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/debiasing_answer.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/debiasing_answer_examples.txt\")\n    chain = call_model(llm, prompt, [\"text\"], examples)\n    \n    unbiased_answer = chain.invoke({\"text\": answer})\n    \n    return {\"answers_agent\": [unbiased_answer], \"ord_aspects\": [state[\"aspect\"]]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose,llm,bias_encoder_model):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    response = \"biased\"\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/bias_detection.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/bias_detection_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n    \n    if bias_encoder_model:\n        bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n        response = bias_detection(answer)[0]['label'].lower()\n    else:\n        response = chain.invoke({\"input\": answer})\n        if \"biased\" in response: response = \"biased\"\n        if \"non-biased\" in response: response = \"non-biased\"\n        \n    if verbose: print(response) #biased, non-biased\n    \n    return response\n\n\ndef decide_to_generate(state,verbose):\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: RELEVANT---\")\n        return \"relevant\"","metadata":{"id":"6iV1oJJtLcFM","execution":{"iopub.status.busy":"2024-08-18T15:53:35.719247Z","iopub.execute_input":"2024-08-18T15:53:35.719662Z","iopub.status.idle":"2024-08-18T15:53:35.765559Z","shell.execute_reply.started":"2024-08-18T15:53:35.719633Z","shell.execute_reply":"2024-08-18T15:53:35.764809Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Workflow condizionale\ndef workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    workflow.add_node(\"rewrite_query\", functools.partial(rewrite_query, verbose=configs.verbose, \n                                                    llm=configs.local_llm))  # query rewriting\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, \n                                                    retriever=configs.retriever, retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, \n                                                    llm=configs.local_llm, fairness=configs.fairness))  # generatae\n    \n    if configs.web_search:\n        workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose))  # web search\n        workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, \n                                                               llm=configs.local_llm))  # grade documents\n    if configs.safeness:\n        workflow.add_node(\"hate_speech_filter\", functools.partial(hate_speech_filter, verbose=configs.verbose, \n                                                                  llm=configs.local_llm))\n    if configs.trustworthiness:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, BART_model=configs.BART_model, strategy_entailment=configs.strategy_entailment, \n                                                                 neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.local_llm, \n                                                                 test_mode=configs.test_mode, observer=configs.observer))  # entailment\n    if configs.fairness:  \n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose, llm=configs.local_llm))\n        workflow.add_node(\"confirm_answer\", functools.partial(confirm_answer, verbose=configs.verbose))\n\n    # Non applichiamo il routing\n    \"\"\"\n    workflow.add_conditional_edges(\n        START,\n        route_question,\n        {\n            \"websearch\": \"websearch\", #se la risposta è websearch, allora vai al nodo websearch\n            \"vectorstore\": \"retrieve\", #se la risposta è vectorstore, allora vai al nodo retrieve\n        },\n    )\n    \"\"\"\n    workflow.add_edge(START, \"rewrite_query\")\n    workflow.add_edge(\"rewrite_query\", \"retrieve\")\n    \n    if configs.web_search:\n        workflow.add_edge(\"retrieve\", \"grade_documents\")  \n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            functools.partial(decide_to_generate, verbose=configs.verbose),\n            {\n                \"websearch\": \"websearch\",\n                \"relevant\": \"hate_speech_filter\" if configs.safeness else \"entailment_filter\" if configs.trustworthiness else \"generate\"\n            },\n        )\n    \n    if configs.safeness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"hate_speech_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"hate_speech_filter\")\n        if configs.trustworthiness:\n            workflow.add_edge(\"hate_speech_filter\", \"entailment_filter\")\n            workflow.add_edge(\"entailment_filter\", \"generate\")\n        else:\n            workflow.add_edge(\"hate_speech_filter\",\"generate\")\n    elif configs.trustworthiness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"entailment_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n    else:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"generate\")\n        else: \n            workflow.add_edge(\"retrieve\", \"generate\")\n    \n    if configs.fairness:\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose, llm=configs.local_llm, bias_encoder_model=configs.bias_encoder_model),\n            {\n                \"biased\": \"debiasing_filter\",\n                \"non-biased\": \"confirm_answer\",\n            },\n        )\n        workflow.add_edge(\"confirm_answer\", END)\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"generate\", END)\n\n    # Non faccio il controllo sulle allucinazioni\n    \"\"\"\n    workflow.add_conditional_edges(\n        \"generate\",\n        grade_generation_v_documents_and_question,\n        {\n            \"not supported\": \"generate\",\n            \"useful\": END,\n            \"not useful\": \"websearch\",\n        },\n    )\n    \"\"\"    \n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","execution":{"iopub.status.busy":"2024-08-18T15:53:36.750225Z","iopub.execute_input":"2024-08-18T15:53:36.750569Z","iopub.status.idle":"2024-08-18T15:53:36.768949Z","shell.execute_reply.started":"2024-08-18T15:53:36.750539Z","shell.execute_reply":"2024-08-18T15:53:36.767911Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    final_answer: str\n\ndef send_aspects(state,verbose):\n    if verbose: \n        print(\"---SEND ASPECT TO EACH ASPECT-AGENT---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"original_query\": state[\"question\"], \"aspect\": a, \"aspect_id\": state[\"aspects\"].index(a)}) for a in state[\"aspects\"]]\n\ndef organize_answers(state,verbose,llm,organize):\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n    ord_aspects = state[\"ord_aspects\"]\n    \n    #print(f\"I'm the master agent and I received: {answers_agent}\") #Debug\n    if organize: #with sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer_with_sections.txt\")\n        chain = call_model(llm, prompt, [\"answers\",\"aspects\"])\n        final_output=chain.invoke({\"answers\": answers_agent, \"aspects\": ord_aspects})\n    else: #without sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer.txt\")\n        chain = call_model(llm, prompt, [\"answers\"])\n        final_output=chain.invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","execution":{"iopub.status.busy":"2024-08-18T15:53:41.644227Z","iopub.execute_input":"2024-08-18T15:53:41.645160Z","iopub.status.idle":"2024-08-18T15:53:41.655224Z","shell.execute_reply.started":"2024-08-18T15:53:41.645126Z","shell.execute_reply":"2024-08-18T15:53:41.654370Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"organize_answers\", functools.partial(organize_answers, verbose=configs.verbose, llm=configs.local_llm, organize=configs.organize))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_conditional_edges(START, functools.partial(send_aspects, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_answers\")\n    master_flow.add_edge(\"organize_answers\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:53:42.151958Z","iopub.execute_input":"2024-08-18T15:53:42.152971Z","iopub.status.idle":"2024-08-18T15:53:42.159598Z","shell.execute_reply.started":"2024-08-18T15:53:42.152937Z","shell.execute_reply":"2024-08-18T15:53:42.158540Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"markdown","source":"Vectorstore configuration","metadata":{}},{"cell_type":"code","source":"index_name = \"entailment-test\"\naspects = [\"Health\",\"Technology\",\"Society\"] #\"Technology\", \"Society\"\n\ntop_retriever = 10 #documents retrieved by retriever\ntop_KBT = 5 #documents retrievede by KBT retriever\n\nretriever = create_retriever(index_name, top_retriever)\nretrievers_KBT = create_KBT_retrievers(aspects, top_KBT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,retriever,retrievers_KBT,aspects):\n        self.local_llm = \"gemma2\" #\"llama3.1\"\n        \n        # retrievers\n        self.retriever = retriever\n        self.retrievers_KBT = retrievers_KBT\n        self.aspects = aspects\n        \n        #if we want print all the process: True\n        self.verbose = False \n        \n        #if we want to include websearch in the workflow\n        self.web_search = True\n\n        # Controlling properties\n        self.safeness = True # if we want to add hate speech detection module\n        self.trustworthiness = True # if we want to add entailment module with KBT\n        self.fairness = True  # if we want to add debiasing module.\n\n        #Controlling entailment\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = True\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True   \n        # True: uses BART model for the entailment, False: uses LLM\n        self.BART_model = False\n        \n        #controlling bias detection\n        # True: uses encoder model. False: uses LLM\n        self.bias_encoder_model = False\n        \n        #controlling final generation \n        # True: organize final output in section. False: organize output without sections\n        self.organize = True\n        \n        # For testing\n        self.test_mode = False\n        self.observer = None\n    \nconfigs = Config(retriever,retrievers_KBT,aspects)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting language model","metadata":{}},{"cell_type":"code","source":"start_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:53:46.909110Z","iopub.execute_input":"2024-08-18T15:53:46.910073Z","iopub.status.idle":"2024-08-18T15:53:48.381306Z","shell.execute_reply.started":"2024-08-18T15:53:46.910036Z","shell.execute_reply":"2024-08-18T15:53:48.380058Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nError: listen tcp 127.0.0.1:11434: bind: address already in use\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/18 - 15:53:47 | 200 |       39.45µs |       127.0.0.1 | HEAD     \"/\"\n\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h[GIN] 2024/08/18 - 15:53:48 | 200 |  383.474316ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u001b[?25h\nError: pull model manifest: file does not exist\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = {\"question\": \"Covid19 was a hoax?\", \"aspects\": configs.aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\nanswer = value[\"final_answer\"]\nprint(answer)","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","execution":{"iopub.status.busy":"2024-08-18T15:53:48.383608Z","iopub.execute_input":"2024-08-18T15:53:48.383934Z","iopub.status.idle":"2024-08-18T15:56:52.018584Z","shell.execute_reply.started":"2024-08-18T15:53:48.383906Z","shell.execute_reply":"2024-08-18T15:56:52.017588Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"[GIN] 2024/08/18 - 15:53:48 | 200 |      28.982µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2024/08/18 - 15:53:48 | 200 |   43.454283ms |       127.0.0.1 | POST     \"/api/show\"\n[GIN] 2024/08/18 - 15:53:48 | 200 |   48.539846ms |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[?25l\u001b[?25l\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/18 - 15:53:50 | 200 |  1.507889849s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:50 | 200 |  1.661098015s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:50 | 200 |  1.668304788s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:53 | 200 |  949.417214ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:53 | 200 |  1.314046449s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:53 | 200 |  1.225249761s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:54 | 200 |    764.6558ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:54 | 200 |  582.300565ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:54 | 200 |  602.271572ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:54 | 200 |  581.262121ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:54 | 200 |  560.721904ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:55 | 200 |  643.626125ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:55 | 200 |  625.556977ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:55 | 200 |  603.680178ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:55 | 200 |  650.374346ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:55 | 200 |  597.033603ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:56 | 200 |  612.102801ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:56 | 200 |  660.726091ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:56 | 200 |  625.789717ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:56 | 200 |  609.727397ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:57 | 200 |   586.20065ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:57 | 200 |  650.281968ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:57 | 200 |  662.108351ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:57 | 200 |  710.065264ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:57 | 200 |  691.407558ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:58 | 200 |  679.392274ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:58 | 200 |   681.17201ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:58 | 200 |  650.879591ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:58 | 200 |  638.983246ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:59 | 200 |  607.757836ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:59 | 200 |  656.318796ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:59 | 200 |  653.015136ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:53:59 | 200 |  500.188595ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:03 | 200 |  1.268660546s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:04 | 200 |  1.441920172s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:04 | 200 |  1.518768743s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:04 | 200 |  1.080568057s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:04 | 200 |  672.524392ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:05 | 200 |  645.971323ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:05 | 200 |    711.4673ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:05 | 200 |   600.73848ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:05 | 200 |  691.862906ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:06 | 200 |  630.934409ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:06 | 200 |  657.144603ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:06 | 200 |  603.140085ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:06 | 200 |  657.697212ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:06 | 200 |  674.190008ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:07 | 200 |  723.523979ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:07 | 200 |  621.553709ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:07 | 200 |   656.53963ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:08 | 200 |    823.7892ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:08 | 200 |  1.197700678s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:08 | 200 |  1.159891329s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:09 | 200 |  1.284189569s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:09 | 200 |  1.118310732s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:10 | 200 |    1.4244441s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:10 | 200 |  1.061386433s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:10 | 200 |  1.195108495s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:11 | 200 |  839.689676ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:11 | 200 |  1.167196796s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:13 | 200 |  2.422700187s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:13 | 200 |  2.760698848s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:14 | 200 |  2.468513038s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:14 | 200 |  1.073323029s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:14 | 200 |  943.780403ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:15 | 200 |  1.757352121s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:16 | 200 |   1.39937867s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:16 | 200 |  1.358380712s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:16 | 200 |  986.955121ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:17 | 200 |  923.225381ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:17 | 200 |  1.023907603s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:18 | 200 |  1.909879783s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:19 | 200 |  2.282105384s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:19 | 200 |  2.397482598s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:20 | 200 |  1.246344598s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:20 | 200 |  1.103227697s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:20 | 200 |  1.220079047s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:21 | 200 |  1.273021363s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:21 | 200 |  1.352784094s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:22 | 200 |  1.353788217s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:22 | 200 |  1.118978093s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:23 | 200 |  1.133696907s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:23 | 200 |  1.150050018s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:23 | 200 |  1.198441482s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:24 | 200 |  1.084929481s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:24 | 200 |  1.150136193s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:24 | 200 |  1.101266827s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:25 | 200 |  1.157834689s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:25 | 200 |  1.198097346s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:25 | 200 |  1.036187841s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:26 | 200 |  1.088672785s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:26 | 200 |  1.031088028s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:27 | 200 |  1.287786377s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:27 | 200 |  1.247309165s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:28 | 200 |   1.25058102s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:28 | 200 |  1.270056633s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:29 | 200 |  1.379784741s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:29 | 200 |  1.253947995s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:31 | 200 |  2.849885786s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:31 | 200 |    2.4300255s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:31 | 200 |  2.202478155s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:32 | 200 |  1.396669883s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:32 | 200 |   1.38521579s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:32 | 200 |  1.376248294s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:34 | 200 |   1.82480395s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:34 | 200 |  1.964285082s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:35 | 200 |  1.092701707s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:36 | 200 |  1.151639303s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:37 | 200 |  1.244242401s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:37 | 200 |  1.264617884s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:38 | 200 |  1.386625421s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:38 | 200 |  1.395156823s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:40 | 200 |  1.468193362s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:40 | 200 |  7.399982158s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:40 | 200 |  1.409187752s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:41 | 200 |  1.790828686s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:42 | 200 |  1.813199006s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:42 | 200 |  2.391098928s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:43 | 200 |  1.212964159s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:43 | 200 |   1.65264822s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:44 | 200 |  1.591952497s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:45 | 200 |  1.162799369s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:46 | 200 |   1.30365238s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:46 | 200 |  1.299937869s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:47 | 200 |  1.350059959s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:47 | 200 |  1.350125873s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:48 | 200 |  1.149275084s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:48 | 200 |  1.156012937s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:49 | 200 |  7.176088375s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:49 | 200 |  1.238932289s |       127.0.0.1 | POST     \"/api/generate\"\n'Finished running: aspect_agent_node:'\n[GIN] 2024/08/18 - 15:54:50 | 200 |  1.242101392s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:51 | 200 |  1.346297806s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:51 | 200 |  1.194312615s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:52 | 200 |   1.33797735s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:52 | 200 |  1.374541804s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:54 | 200 |  1.409081838s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:54 | 200 |  1.453938445s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:55 | 200 |  1.205861491s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:55 | 200 |  1.129043145s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:56 | 200 |  1.230546244s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:56 | 200 |  1.272320449s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:57 | 200 |  1.117603463s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:57 | 200 |  1.249398521s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:59 | 200 |  1.278440617s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:54:59 | 200 |  1.265871253s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:00 | 200 |  1.348099862s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:00 | 200 |  1.343964219s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:01 | 200 |  1.432347291s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:02 | 200 |  1.441285814s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:03 | 200 |  1.584084545s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:03 | 200 |  1.591212223s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:05 | 200 |    1.4651817s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:05 | 200 |  1.454812608s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:06 | 200 |  1.623153233s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:06 | 200 |  1.627677474s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:08 | 200 |  1.682803852s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:08 | 200 |  1.681226119s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:09 | 200 |  1.432353725s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:10 | 200 |  1.440160666s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:11 | 200 |  1.541841739s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:11 | 200 |  1.541526313s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:13 | 200 |  1.763103963s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:13 | 200 |  1.669028591s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:15 | 200 |  1.918512286s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:15 | 200 |  1.966574877s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:17 | 200 |  1.806280936s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:17 | 200 |   2.00392312s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:18 | 200 |  1.307122254s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:18 | 200 |  1.253143982s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:19 | 200 |  1.318370268s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:19 | 200 |  1.321555459s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:20 | 200 |  1.004879439s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:21 | 200 |  1.429213588s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:21 | 200 |  677.252081ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:22 | 200 |  1.353983783s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:22 | 200 |  1.208246516s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:24 | 200 |  1.569679588s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:24 | 200 |  1.569740394s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:25 | 200 |  1.401303192s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:25 | 200 |  1.401218564s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:27 | 200 |  1.422714376s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:27 | 200 |  1.479570328s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:31 | 200 |   4.31795159s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:32 | 200 |  1.169686455s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:33 | 200 |   1.08929905s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:35 | 200 |  1.168528956s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:36 | 200 |  1.194607305s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:37 | 200 |  1.162898063s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:38 | 200 |      1.19971s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:39 | 200 |  1.101856457s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:39 | 200 | 12.523497931s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:41 | 200 |  1.672012895s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:41 | 200 |  1.795250567s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:42 | 200 |   1.17404557s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:43 | 200 |  1.087332852s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:45 | 200 |  1.100987299s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:45 | 200 |  774.776926ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:46 | 200 |   824.93535ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:47 | 200 |  840.693385ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:48 | 200 |  822.503169ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:48 | 200 |  6.828131993s |       127.0.0.1 | POST     \"/api/generate\"\n'Finished running: aspect_agent_node:'\n[GIN] 2024/08/18 - 15:55:49 | 200 |   746.82143ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:49 | 200 |  745.053196ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:50 | 200 |  821.411588ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:51 | 200 |  864.307537ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:52 | 200 |  816.606687ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:53 | 200 |  886.294217ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:54 | 200 |  760.296599ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:55 | 200 |  803.557575ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:55 | 200 |  812.590901ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:56 | 200 |  803.320402ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:57 | 200 |  821.094048ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:58 | 200 |  802.166666ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:55:59 | 200 |  835.163376ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:00 | 200 |  887.966922ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:01 | 200 |   838.85229ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:01 | 200 |  891.859372ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:02 | 200 |  815.318594ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:03 | 200 |  884.539747ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:04 | 200 |  889.763851ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:05 | 200 |   883.19814ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:06 | 200 |  900.052425ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:07 | 200 |  665.173379ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:07 | 200 |   767.76548ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:08 | 200 |  782.698798ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:09 | 200 |  770.933303ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:10 | 200 |  801.951988ms |       127.0.0.1 | POST     \"/api/generate\"\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1847 n_keep=4 n_left=2044 n_shift=1022 tid=\"132674637438976\" timestamp=1723996570\n[GIN] 2024/08/18 - 15:56:27 | 200 | 17.291732268s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:28 | 200 |  962.921484ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/18 - 15:56:37 | 200 |  8.902757011s |       127.0.0.1 | POST     \"/api/generate\"\n'Finished running: aspect_agent_node:'\n[GIN] 2024/08/18 - 15:56:51 | 200 | 14.382693182s |       127.0.0.1 | POST     \"/api/generate\"\n'Finished running: organize_answers:'\n- Health section: Long COVID can cause various lasting health problems, including chronic fatigue, brain fog, and autonomic dysfunction. These effects can significantly impact an individual's ability to work or attend school for months or even years after the initial infection.  Research is ongoing to better understand the long-term consequences of COVID-19. \n- Technology section: COVID-19 vaccines do not contain microchips. The FDA has stated this clearly, and multiple sources have debunked this myth. There is no evidence to support the claim that vaccines contain tracking devices.  \n- Society section: The COVID-19 pandemic had a profound impact on social interactions, disrupting many aspects of how people connect and relate to each other.  Physical distancing measures limited face-to-face interactions, gatherings, and even simple acts like hugging or shaking hands, leading to feelings of isolation and loneliness for some.  Mask wearing obscured facial expressions, making it harder to read emotions and understand social cues, which could lead to misinterpretations and difficulties in building rapport.  The pandemic itself generated fear and anxiety about contracting the virus, making people hesitant to engage socially or feel uncomfortable in public spaces.  While the pandemic accelerated the shift towards online communication, there were also examples of community support during challenging times.  It's important to note that the impact of COVID-19 on social interactions varied greatly depending on individual circumstances, cultural context, and access to resources.   \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Funzioni di supporto:\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n    \nqueries_gpt = [\"Does the COVID-19 vaccine contain a microchip for controlling people?\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Does inhaling hot steam kill the COVID-19 virus?\",\n\"Was COVID-19 created in a lab as a biological weapon?\",\n\"Does drinking alcohol help prevent COVID-19 infection?\",\n\"Is 5G responsible for spreading COVID-19?\",\n\"Do masks cause a reduction in blood oxygen levels?\",\n\"Do COVID-19 vaccines cause infertility in women?\",\n\"Can high doses of vitamin C cure COVID-19?\",\n\"Can pets transmit COVID-19 to humans?\"]\n\n#write_file(\"/kaggle/working/gpt_queries.json\",queries_gpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predisposizione dell'observer per salvare i risultati","metadata":{}},{"cell_type":"code","source":"class Observer(object):\n    def __init__(self):\n        self.query=\"\"\n        self.type_of_acceptance=\"\"\n        self.neutral_acceptance=False\n        self.generated_queries={}\n        self.final_answer=\"\"\n    \n    def generate_dict(self):\n        return {\"query\": self.query,\n               \"type_of_acceptance\": self.type_of_acceptance,\n               \"neutral_acceptance\": self.neutral_acceptance,\n               \"generated_queries\": self.generated_queries,\n               \"final_answer\": self.final_answer}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combinazioni di configurazione con skeptical/credulous e neutral","metadata":{}},{"cell_type":"markdown","source":"Multiple combination:","metadata":{}},{"cell_type":"code","source":"combination = [(True,True), (True,False), (False,True),(False,False)] #Skeptical-Neutral #Skeptical-No-Neutral #Cred-Neu #Cred-No-Neu\n\nfor comb in combination:\n    print(\"Start combination\")\n    configs.strategy_entailment =  comb[0]\n    configs.neutral_acceptance = comb[1]\n    \n    queries_list = queries_gpt\n\n    attempt = 1\n    ret_dict = {}\n    for query in queries_list:\n        inputs = {\"question\": query, \"aspects\": configs.aspects}\n        print(f\"Attempt {attempt} start\")\n    \n        configs.observer = Observer()\n        configs.observer.query=query\n        if configs.strategy_entailment:\n            configs.observer.type_of_acceptance=\"Skeptical\"\n        else:\n            configs.observer.type_of_acceptance=\"Credulous\"\n        configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n        for output in master_flow(configs).stream(inputs):\n            for key, value in output.items():\n                pass\n                #pprint(f\"Finished running: {key}:\")\n        answer = value[\"final_answer\"]\n    \n        configs.observer.final_answer= answer\n        ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n        attempt = attempt + 1\n\n    stringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\n    write_file(f\"/kaggle/working/test_llama31_third_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Single combination:","metadata":{}},{"cell_type":"code","source":"#queries_list = load_queries(\"/kaggle/input/preference/gpt_queries.json\")\nqueries_list = queries_gpt\n\nattempt = 1\nret_dict = {}\nfor query in queries_list:\n    inputs = {\"question\": query, \"aspects\": configs.aspects}\n    print(f\"Attempt {attempt} start\")\n    \n    configs.observer = Observer()\n    configs.observer.query=query\n    if configs.strategy_entailment:\n        configs.observer.type_of_acceptance=\"Skeptical\"\n    else:\n        configs.observer.type_of_acceptance=\"Credulous\"\n    configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n    for output in master_flow(configs).stream(inputs):\n        for key, value in output.items():\n            pass\n            #pprint(f\"Finished running: {key}:\")\n    answer = value[\"final_answer\"]\n    \n    configs.observer.final_answer= answer\n    ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n    attempt = attempt + 1\n\nstringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\nwrite_file(f\"/kaggle/working/test_llama31_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Entailment evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione su notizie vere (0) e notizie false (1)","metadata":{}},{"cell_type":"code","source":"# Label 0, notizie vere\n# Label 1, notizie false\n\ndef results_on_label(label, num_attempts, num_aspects, dict_input):\n    recall = 0\n    precision = 0\n    f1 = 0\n    support = 0\n\n    for i in range(1,num_attempts+1):\n        for j in range(num_aspects):\n            recall = recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][label][\"recall\"]\n            precision = precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][label][\"precision\"]\n            f1 = f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][label][\"f1-score\"]\n            support = support + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{j}\"][\"report\"][label][\"support\"]\n\n    return {\"recall\":recall/(num_attempts*num_aspects), \n            \"precision\":precision/(num_attempts*num_aspects),\n            \"f1\":f1/(num_attempts*num_aspects), \n            \"support\":support/(num_attempts*num_aspects)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione per aspetto","metadata":{}},{"cell_type":"code","source":"def results_on_aspects(aspect_id, num_attempts, num_aspects, dict_input):\n    recall = 0\n    precision = 0\n    f1 = 0\n\n    weight_recall = 0\n    weight_precision = 0\n    weight_f1 = 0\n    \n    macro_recall = 0\n    macro_precision = 0\n    macro_f1 = 0\n    \n    accuracy = 0\n\n    for i in range(1,num_attempts+1):\n        recall = recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"recall\"]\n        precision = precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"precision\"]\n        f1 = f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"f1-score\"]\n        \n        macro_recall = macro_recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"recall\"]\n        macro_precision = macro_precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"precision\"]\n        macro_f1 = macro_f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"macro avg\"][\"f1-score\"]\n        \n        weight_recall = weight_recall + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"weighted avg\"][\"recall\"]\n        weight_precision = weight_precision + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"weighted avg\"][\"precision\"]\n        weight_f1 = weight_f1 + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"weighted avg\"][\"f1-score\"]\n        try:\n            accuracy = accuracy + dict_input[f\"attempt {i}\"][\"generated_queries\"][f\"aspect_{aspect_id}\"][\"report\"][\"accuracy\"]\n        except:\n            accuracy = accuracy + 1 #se l'accuracy non è presente nel dizionario, è perché vale 1 (vedi esempio 4 di llama_second_skeptical_neutral)\n    \n    return {\"recall\": recall/(num_attempts),\n            \"precision\": precision/(num_attempts), \n            \"f1\": f1/(num_attempts),\n            \"macro_recall\": macro_recall/(num_attempts),\n            \"macro_precision\": macro_precision/(num_attempts),\n            \"macro_f1\": macro_f1/(num_attempts),\n            \"w_recall\": weight_recall/(num_attempts),\n            \"w_precision\": weight_precision/(num_attempts), \n            \"w_f1\":weight_f1/(num_attempts), \n            \"accuracy\":accuracy/(num_attempts)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label 0: notizie vere (negative class)\n# label 1: notizie false (positive class)\n\ndef compute_result_on_label(model, shot, acceptance, neutral, attempts, num_aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n\n    results_label_0 = results_on_label(\"0\", attempts, num_aspects, dict_input)\n    results_label_1 = results_on_label(\"1\", attempts, num_aspects, dict_input)\n    \n\n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n\n    df[\"precision_0\"] = results_label_0[\"precision\"]\n    df[\"recall_0\"] = results_label_0[\"recall\"]\n    df[\"f1_0\"] = results_label_0[\"f1\"]\n    df[\"support_0\"] = results_label_0[\"support\"]\n    \n\n    df[\"precision_1\"] = results_label_1[\"precision\"]\n    df[\"recall_1\"] = results_label_1[\"recall\"]\n    df[\"f1_1\"] = results_label_1[\"f1\"]\n    df[\"support_1\"] = results_label_1[\"support\"]\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_result_on_aspects(model, shot, acceptance, neutral, attempts, num_aspects, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n\n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n    \n    aspect_id = 0\n    for aspect in aspects:\n        result = results_on_aspects(aspect_id, attempts, num_aspects, dict_input)\n        df[f\"precision_{aspect}\"] = result[\"precision\"]\n        df[f\"recall_{aspect}\"] = result[\"recall\"]\n        df[f\"f1_{aspect}\"] = result[\"f1\"]\n        df[f\"accuracy_{aspect}\"] = result[\"accuracy\"]\n        \n        df[f\"macro_precision_{aspect}\"] = result[\"macro_precision\"]\n        df[f\"macro_recall_{aspect}\"] = result[\"macro_recall\"]\n        df[f\"macro_f1_{aspect}\"] = result[\"macro_f1\"]\n        \n        df[f\"w_precision_{aspect}\"] = result[\"w_precision\"]\n        df[f\"w_recall_{aspect}\"] = result[\"w_recall\"]\n        df[f\"w_f1_{aspect}\"] = result[\"w_f1\"]\n        \n        aspect_id = aspect_id+1    \n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import openpyxl\n\nattempts = 10\nnum_aspects = 3\n\nmodels = [\"BART\",\"llama31\"]\nshots = [\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Technology\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_results_labels.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_label(model, shot, acceptance, neutral, attempts, num_aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\nresults.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excel_file = \"/kaggle/working/test_entailment_results_aspects.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_aspects(model, shot, acceptance, neutral, attempts, num_aspects, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\nresults.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ragas (application evaluation)","metadata":{}},{"cell_type":"markdown","source":"https://docs.ragas.io/en/stable/","metadata":{}},{"cell_type":"code","source":"#todo","metadata":{},"execution_count":null,"outputs":[]}]}