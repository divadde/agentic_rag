{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9212052,"sourceType":"datasetVersion","datasetId":5570293},{"sourceId":9223440,"sourceType":"datasetVersion","datasetId":5560790},{"sourceId":98659,"sourceType":"modelInstanceVersion","modelInstanceId":82777,"modelId":107076}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","execution":{"iopub.status.busy":"2024-08-23T09:38:01.072379Z","iopub.execute_input":"2024-08-23T09:38:01.072735Z","iopub.status.idle":"2024-08-23T09:38:04.902639Z","shell.execute_reply.started":"2024-08-23T09:38:01.072705Z","shell.execute_reply":"2024-08-23T09:38:04.901454Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 CLI\n######################################################################## 100.0%#=#=#                                                                                                         44.0%\n>>> Making ollama accessible in the PATH in /usr/local/bin\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","execution":{"iopub.status.busy":"2024-08-23T09:38:04.904700Z","iopub.execute_input":"2024-08-23T09:38:04.905014Z","iopub.status.idle":"2024-08-23T09:38:04.910792Z","shell.execute_reply.started":"2024-08-23T09:38:04.904968Z","shell.execute_reply":"2024-08-23T09:38:04.909452Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:04.912060Z","iopub.execute_input":"2024-08-23T09:38:04.912314Z","iopub.status.idle":"2024-08-23T09:38:04.920354Z","shell.execute_reply.started":"2024-08-23T09:38:04.912282Z","shell.execute_reply":"2024-08-23T09:38:04.919362Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","execution":{"iopub.status.busy":"2024-08-23T09:38:04.922461Z","iopub.execute_input":"2024-08-23T09:38:04.922767Z","iopub.status.idle":"2024-08-23T09:38:04.927887Z","shell.execute_reply.started":"2024-08-23T09:38:04.922744Z","shell.execute_reply":"2024-08-23T09:38:04.927023Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 ragas langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","execution":{"iopub.status.busy":"2024-08-23T09:38:04.928988Z","iopub.execute_input":"2024-08-23T09:38:04.929259Z","iopub.status.idle":"2024-08-23T09:38:22.543262Z","shell.execute_reply.started":"2024-08-23T09:38:04.929237Z","shell.execute_reply":"2024-08-23T09:38:22.541916Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","execution":{"iopub.status.busy":"2024-08-23T09:38:22.545053Z","iopub.execute_input":"2024-08-23T09:38:22.545436Z","iopub.status.idle":"2024-08-23T09:38:22.551641Z","shell.execute_reply.started":"2024-08-23T09:38:22.545397Z","shell.execute_reply":"2024-08-23T09:38:22.550615Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Bias detection model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:22.553111Z","iopub.execute_input":"2024-08-23T09:38:22.553499Z","iopub.status.idle":"2024-08-23T09:38:33.309185Z","shell.execute_reply.started":"2024-08-23T09:38:22.553469Z","shell.execute_reply":"2024-08-23T09:38:33.308446Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-08-23 09:38:25.997173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-23 09:38:25.997225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-23 09:38:25.998672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAll TF 2.0 model weights were used when initializing DistilBertForSequenceClassification.\n\nAll the weights of DistilBertForSequenceClassification were initialized from the TF 2.0 model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_entailment/Textual%20Entailment%20Explanation%20Demo.html\n- https://huggingface.co/facebook/bart-large-mnli","metadata":{}},{"cell_type":"markdown","source":"Entailment model (BART):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbart_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\",device=device)\nbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:33.310189Z","iopub.execute_input":"2024-08-23T09:38:33.310478Z","iopub.status.idle":"2024-08-23T09:38:35.346458Z","shell.execute_reply.started":"2024-08-23T09:38:33.310439Z","shell.execute_reply":"2024-08-23T09:38:35.345367Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def BART_prediction(premise,hypothesis):\n    #print(f\"Premise: {premise}\")\n    #print(f\"Hypo: {hypothesis}\")\n    input_ids = bart_tokenizer.encode(premise, hypothesis, return_tensors=\"pt\")\n    logits = bart_model(input_ids)[0]\n    probs = logits.softmax(dim=1)\n\n    max_index = torch.argmax(probs).item()\n\n    bart_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n    return bart_label_map[max_index]","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:35.347661Z","iopub.execute_input":"2024-08-23T09:38:35.347951Z","iopub.status.idle":"2024-08-23T09:38:35.354211Z","shell.execute_reply.started":"2024-08-23T09:38:35.347926Z","shell.execute_reply":"2024-08-23T09:38:35.353271Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nBERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nBERT_model = BertForSequenceClassification.from_pretrained('/kaggle/input/bert_hate_speech/pytorch/default/1/fine_tuned_bert')\nBERT_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:35.358622Z","iopub.execute_input":"2024-08-23T09:38:35.358906Z","iopub.status.idle":"2024-08-23T09:38:35.744903Z","shell.execute_reply.started":"2024-08-23T09:38:35.358884Z","shell.execute_reply":"2024-08-23T09:38:35.744015Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def BERT_hate_speech(text):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BERT_model.to(device)\n\n    encoded_dict = BERT_tokenizer.encode_plus(\n                    text,\n                    add_special_tokens = True,\n                    max_length = None, #Uso della massima lunghezza del modello, nel caso di BERT 512 tokens\n                    padding = \"max_length\",\n                    truncation = True,\n                    return_attention_mask = True,\n                    return_tensors = 'pt',\n                )\n\n    text_ids = encoded_dict['input_ids'].to(device)\n    attention_mask = encoded_dict['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = BERT_model(input_ids=text_ids, attention_mask=attention_mask)\n\n    # Estrai i logits (output non normalizzati del modello)\n    logits = outputs.logits\n\n    # Converti i logits in probabilità (se necessario)\n    probs = torch.softmax(logits, dim=1)\n\n    # Identifica la classe con la probabilità più alta\n    predicted_class = torch.argmax(probs, dim=1)\n    \n    #Label 0, no hate speech, Label 1 hate speech\n    if predicted_class==0: response = \"no\"\n    else: response = \"yes\"\n\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:35.745883Z","iopub.execute_input":"2024-08-23T09:38:35.746156Z","iopub.status.idle":"2024-08-23T09:38:35.754388Z","shell.execute_reply.started":"2024-08-23T09:38:35.746133Z","shell.execute_reply":"2024-08-23T09:38:35.753095Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Web Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nweb_search_tool = TavilySearchResults(k=2)","metadata":{"id":"37TQnYfuSYjJ","execution":{"iopub.status.busy":"2024-08-23T09:38:35.755792Z","iopub.execute_input":"2024-08-23T09:38:35.756344Z","iopub.status.idle":"2024-08-23T09:38:36.715165Z","shell.execute_reply.started":"2024-08-23T09:38:35.756319Z","shell.execute_reply":"2024-08-23T09:38:36.714394Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"markdown","source":"osservazione sul chuncking: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338","metadata":{}},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\ndef create_retriever(index_name,top_k):\n    vectorstore = PineconeVectorStore(\n        index_name=index_name,\n        embedding=AI21Embeddings(device=\"cuda\")\n    )\n    return vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\ndef create_KBT_retrievers(aspects,top_k):\n    retrievers = []\n    for aspect in aspects:\n        retriever = create_retriever(f\"{aspect.lower()}-kbt\",top_k)\n        retrievers.append(retriever)\n    return retrievers","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:36.716270Z","iopub.execute_input":"2024-08-23T09:38:36.716773Z","iopub.status.idle":"2024-08-23T09:38:36.974195Z","shell.execute_reply.started":"2024-08-23T09:38:36.716748Z","shell.execute_reply":"2024-08-23T09:38:36.973378Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Prompt support","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import Ollama\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\ndef create_prompt(system_prompt, input_variables, model, examples=None):\n    input_var_str = [f\"{input_var}\" for input_var in input_variables]\n    if \"llama3.1\" in model:\n        prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \"+ system_prompt\n    else:\n        prompt = system_prompt\n    \n    if examples!=None:\n        prompt = prompt + \"\\nHere some examples: \\n\"\n        for example in examples:\n            prompt = prompt + \"\\n\"\n            for key in input_var_str:\n                prompt = prompt + key + \": \" + example[key] +\"\\n\"\n            prompt = prompt + \"ouput: \" + example[\"output\"] +\"\\n\"   \n        prompt = prompt + \"\\n\"\n    \n    if \"llama3.1\" in model:\n        prompt = prompt + \"<|eot_id|><|start_header_id|>user<|end_header_id|> \\n\"\n    for key in input_var_str:\n            prompt = prompt + key + \": \" +f\"{{{key}}}\" +\"\\n\"\n    if \"llama3.1\" in model:\n        prompt = prompt + \"output:\" + \" <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n    prompt_template = PromptTemplate(template = prompt, input_variables=input_var_str)\n    \n    #Debug:\n    \"\"\"\n    dict_input = {}\n    for input_var in input_variables:\n        dict_input[f'{input_var}'] = \"USER_SUBMISSION\"\n    print(prompt_template.format(**dict_input))\n    \"\"\"\n    \n    return prompt_template","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:36.975239Z","iopub.execute_input":"2024-08-23T09:38:36.975499Z","iopub.status.idle":"2024-08-23T09:38:37.003505Z","shell.execute_reply.started":"2024-08-23T09:38:36.975476Z","shell.execute_reply":"2024-08-23T09:38:37.002698Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def call_model(llm: str, prompt: str, input_variables:list[str], examples=None):\n    model = Ollama(model=llm, temperature=0)\n    prompt_final = create_prompt(prompt, input_variables, llm, examples)\n    return prompt_final | model | StrOutputParser()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:37.004549Z","iopub.execute_input":"2024-08-23T09:38:37.004886Z","iopub.status.idle":"2024-08-23T09:38:37.009670Z","shell.execute_reply.started":"2024-08-23T09:38:37.004852Z","shell.execute_reply":"2024-08-23T09:38:37.008816Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_prompt(path):\n    with open(path, 'r') as file:\n        prompt = file.read()\n    return prompt\n\ndef get_examples(path):\n    with open(path, 'r') as file:\n        examples = file.read()\n    return eval(examples)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:37.010845Z","iopub.execute_input":"2024-08-23T09:38:37.011238Z","iopub.status.idle":"2024-08-23T09:38:37.018045Z","shell.execute_reply.started":"2024-08-23T09:38:37.011204Z","shell.execute_reply":"2024-08-23T09:38:37.016998Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"https://learnprompting.org/docs/reliability/debiasing","metadata":{}},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\nimport sklearn.metrics\nimport numpy as np\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    original_query: str\n    query: str\n    aspect: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n        \ndef rewrite_query(state,verbose,llm,observer):\n    if verbose: \n        print(\"---REWRITING QUERY---\")\n        print(f\"State: {state}\")\n    original_query = state[\"original_query\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/query_rewriting.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/query_rewriting_examples.txt\")\n    chain = call_model(llm, prompt, [\"original_query\",\"aspect\"], examples)\n    generation = chain.invoke({\"original_query\": original_query, \"aspect\": aspect})\n    #print(f\"Aspect {aspect} query: {generation}\")\n    #print(list(generation.values())) #Debug\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"] = {\"aspect_query\": generation}\n    return {\"query\": generation}\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fairness,observer):\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n\n    # RAG generation\n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/generating_answer.txt\")\n    chain = call_model(llm, prompt, [\"context\",\"question\"])\n    documents_content = [d.page_content for d in documents]\n    generation = chain.invoke({\"context\": documents_content, \"question\": query})\n    \n    aspect_id = state[\"aspect_id\"]\n    #print(f\"Aspect agent {aspect_id} generates: {generation}\") #Debug\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"documents_for_generation\"] = documents_content\n            observer.aspects[f\"{aspect}\"][\"answer\"] = generation\n            observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = \"//\"\n    if fairness:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation], \"ord_aspects\": [state[\"aspect\"]]}\n\n\ndef confirm_answer(state,verbose):\n    if verbose:\n        print(\"---CONFIRM ANSWER---\")\n        print(f\"State: {state}\")\n    my_answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n\n    #print(f\"Answer for aspect {aspect}: {my_answer}\")\n    return {\"answers_agent\": [my_answer], \"ord_aspects\": [aspect]}\n\n\ndef grade_documents(state,verbose,llm,observer):\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/retrieval_grader.txt\")\n    chain = call_model(llm, prompt, [\"question\",\"document\"])\n\n    # Score each doc\n    filtered_docs = []\n    not_relevant_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = chain.invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if \"yes\" in score.lower():\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            not_relevant_docs.append(d.page_content)\n            web_search = \"Yes\"\n            continue\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"web_search\"] = web_search\n            observer.aspects[f\"{aspect}\"][\"not_relevants_docs\"] = not_relevant_docs\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose):\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    #print(f\"docs from web: {docs}\") #Debug\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents([web_results])\n    for doc in doc_splits:\n        if documents is None:\n            documents = [doc]\n        else:\n            documents.append(doc)\n    return {\"documents\": documents, \"query\": query}\n\n\ndef hate_speech_filter(state,verbose,llm,encoder,observer):\n    if verbose:\n        print(\"---HATE SPEECH FILTER---\")\n        print(f\"State: {state}\")\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/hate_speech.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/hate_speech_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    hate_speech_docs = []\n    for d in documents:\n        if encoder:\n            score = BERT_hate_speech(d.page_content)\n        else:\n            score = chain.invoke(\n                {\"input\": d.page_content}\n            )\n        #grade = score[\"score\"]\n        if \"no\" in score.lower():\n            if verbose: print(\"---DOCUMENT ACCEPTED---\")\n            filtered_docs.append(d)\n        else:\n            if verbose: print(\"---DOCUMENT HATEFUL---\")\n            hate_speech_docs.append(d.page_content)\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"hate_speech_docs\"] = hate_speech_docs\n    return {\"documents\": filtered_docs}\n\n\ndef entailment_filter(state,BART_model,strategy_entailment,neutral_acceptance,verbose,observer,llm,shots):    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n    aspect_id = state[\"aspect_id\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/entailment_checker.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/entailment_checker_examples.txt\")\n    if shots>0:\n        chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"], examples[:shots])\n    else:\n        chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"])\n\n    # Score each doc\n    filtered_docs = []\n    #counter_docs = 0 # Debug\n    #La procedura si può semplificare: TODO\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"contradiction\" in score.lower():\n                    # contradiction found\n                    break\n            if (not neutral or neutral_acceptance) and score.lower() != \"contradiction\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug            \n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                # Document entailed\n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"entailment\" in score.lower():\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                    filtered_docs.append(d)\n                    break\n            if (neutral and neutral_acceptance) and score.lower() != \"entailment\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    \n    if observer!=None:\n        #todo misura metriche con sklearn classifier\n        #0 tweet veri, #1 tweet falsi\n        y_true = [int(document.metadata.get(\"label\")) for document in documents if \"label\" in document.metadata]\n        y_pred = [0 if document in filtered_docs else 1 for document in documents if \"label\" in document.metadata]\n        #print(f\"Real docs with aspect {aspect_id} ,y_true: {y_true}\") #Debug\n        #print(f\"Filtered docs with aspect {aspect_id} ,y_pred: {y_pred}\") #Debug\n        report = sklearn.metrics.classification_report(y_true,y_pred,labels=[0,1],\n                                                       output_dict=True,zero_division=0)\n        observer.aspects[f\"{aspect}\"][\"report_entailment\"] = report\n        \n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose,llm,observer):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/debiasing_answer.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/debiasing_answer_examples.txt\")\n    chain = call_model(llm, prompt, [\"text\"], examples)\n    \n    unbiased_answer = chain.invoke({\"text\": answer})\n    \n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = unbiased_answer\n    \n    #print(f\"Answer for aspect {aspect}: {unbiased_answer}\")\n    \n    return {\"answers_agent\": [unbiased_answer], \"ord_aspects\": [aspect]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose,llm,bias_encoder_model):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    response = \"biased\"\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/bias_detection.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/bias_detection_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n    \n    if bias_encoder_model:\n        bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n        response = bias_detection(answer)[0]['label'].lower()\n    else:\n        response = chain.invoke({\"input\": answer})\n        if \"biased\" in response: response = \"biased\"\n        if \"non-biased\" in response: response = \"non-biased\"\n        \n    if verbose: print(response) #biased, non-biased\n    \n    return response\n\n\ndef decide_to_generate(state,verbose):\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: RELEVANT---\")\n        return \"relevant\"","metadata":{"id":"6iV1oJJtLcFM","execution":{"iopub.status.busy":"2024-08-23T09:38:37.019571Z","iopub.execute_input":"2024-08-23T09:38:37.020100Z","iopub.status.idle":"2024-08-23T09:38:37.122931Z","shell.execute_reply.started":"2024-08-23T09:38:37.020066Z","shell.execute_reply":"2024-08-23T09:38:37.121915Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Workflow condizionale\ndef workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    workflow.add_node(\"rewrite_query\", functools.partial(rewrite_query, verbose=configs.verbose, \n                                                    llm=configs.local_llm, observer=configs.observer))  # query rewriting\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, \n                                                    retriever=configs.retriever, \n                                                    retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, \n                                                    llm=configs.local_llm, \n                                                    fairness=configs.fairness, observer=configs.observer))  # generatae\n    \n    if configs.web_search:\n        workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose))  # web search\n        workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, \n                                                               llm=configs.local_llm,\n                                                               observer=configs.observer))  # grade documents\n    if configs.safeness:\n        workflow.add_node(\"hate_speech_filter\", functools.partial(hate_speech_filter, verbose=configs.verbose, \n                                                                  llm=configs.local_llm, \n                                                                  encoder=configs.hate_encoder_model,\n                                                                  observer=configs.observer))\n    if configs.trustworthiness:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, BART_model=configs.BART_model, strategy_entailment=configs.strategy_entailment, \n                                                                 neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.local_llm, \n                                                                 observer=configs.observer,\n                                                                 shots=configs.shots))  # entailment\n    if configs.fairness:  \n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose, \n                                                                llm=configs.local_llm,\n                                                                observer=configs.observer))\n        workflow.add_node(\"confirm_answer\", functools.partial(confirm_answer, verbose=configs.verbose))\n\n    workflow.add_edge(START, \"rewrite_query\")\n    workflow.add_edge(\"rewrite_query\", \"retrieve\")\n    \n    if configs.web_search:\n        workflow.add_edge(\"retrieve\", \"grade_documents\")  \n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            functools.partial(decide_to_generate, verbose=configs.verbose),\n            {\n                \"websearch\": \"websearch\",\n                \"relevant\": \"hate_speech_filter\" if configs.safeness else \"entailment_filter\" if configs.trustworthiness else \"generate\"\n            },\n        )\n    \n    if configs.safeness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"hate_speech_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"hate_speech_filter\")\n        if configs.trustworthiness:\n            workflow.add_edge(\"hate_speech_filter\", \"entailment_filter\")\n            workflow.add_edge(\"entailment_filter\", \"generate\")\n        else:\n            workflow.add_edge(\"hate_speech_filter\",\"generate\")\n    elif configs.trustworthiness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"entailment_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n    else:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"generate\")\n        else: \n            workflow.add_edge(\"retrieve\", \"generate\")\n    \n    if configs.fairness:\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose, llm=configs.local_llm, bias_encoder_model=configs.bias_encoder_model),\n            {\n                \"biased\": \"debiasing_filter\",\n                \"non-biased\": \"confirm_answer\",\n            },\n        )\n        workflow.add_edge(\"confirm_answer\", END)\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"generate\", END)\n\n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","execution":{"iopub.status.busy":"2024-08-23T09:38:37.124063Z","iopub.execute_input":"2024-08-23T09:38:37.124322Z","iopub.status.idle":"2024-08-23T09:38:37.142274Z","shell.execute_reply.started":"2024-08-23T09:38:37.124299Z","shell.execute_reply":"2024-08-23T09:38:37.141346Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    final_answer: str\n\ndef send_aspects(state,verbose):\n    if verbose: \n        print(\"---SEND ASPECT TO EACH ASPECT-AGENT---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"original_query\": state[\"question\"], \"aspect\": a, \"aspect_id\": state[\"aspects\"].index(a)}) for a in state[\"aspects\"]]\n\ndef organize_answers(state,verbose,llm,organize):\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n    ord_aspects = state[\"ord_aspects\"]\n    \n    #print(f\"I'm the master agent and I received: {answers_agent}\") #Debug\n    if organize: #with sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer_with_sections.txt\")\n        chain = call_model(llm, prompt, [\"answers\",\"aspects\"])\n        final_output=chain.invoke({\"answers\": answers_agent, \"aspects\": ord_aspects})\n    else: #without sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer.txt\")\n        chain = call_model(llm, prompt, [\"answers\"])\n        final_output=chain.invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","execution":{"iopub.status.busy":"2024-08-23T09:38:37.143838Z","iopub.execute_input":"2024-08-23T09:38:37.144110Z","iopub.status.idle":"2024-08-23T09:38:37.153949Z","shell.execute_reply.started":"2024-08-23T09:38:37.144087Z","shell.execute_reply":"2024-08-23T09:38:37.153054Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"organize_answers\", functools.partial(organize_answers, verbose=configs.verbose, llm=configs.local_llm, organize=configs.organize))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_conditional_edges(START, functools.partial(send_aspects, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_answers\")\n    master_flow.add_edge(\"organize_answers\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:37.155295Z","iopub.execute_input":"2024-08-23T09:38:37.156151Z","iopub.status.idle":"2024-08-23T09:38:37.164284Z","shell.execute_reply.started":"2024-08-23T09:38:37.156112Z","shell.execute_reply":"2024-08-23T09:38:37.163453Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"markdown","source":"Vectorstore configuration","metadata":{}},{"cell_type":"code","source":"index_name = \"entailment-test\"\naspects = [\"Health\",\"Governmental\",\"Society\"] #\"Technology\"\n\ntop_retriever = 10 #documents retrieved by retriever\ntop_KBT = 5 #documents retrievede by KBT retriever\n\nretriever = create_retriever(index_name, top_retriever)\nretrievers_KBT = create_KBT_retrievers(aspects, top_KBT)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:37.165292Z","iopub.execute_input":"2024-08-23T09:38:37.165578Z","iopub.status.idle":"2024-08-23T09:38:37.931295Z","shell.execute_reply.started":"2024-08-23T09:38:37.165555Z","shell.execute_reply":"2024-08-23T09:38:37.930507Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,retriever,retrievers_KBT,aspects):\n        self.local_llm = \"llama3.1\" #\"gemma2\"\n        \n        # retrievers\n        self.retriever = retriever\n        self.retrievers_KBT = retrievers_KBT\n        self.aspects = aspects\n        \n        #if we want print all the process: True\n        self.verbose = False \n        \n        #if we want to include websearch in the workflow\n        self.web_search = True\n\n        # Controlling properties\n        self.safeness = True # if we want to add hate speech detection module\n        self.trustworthiness = True # if we want to add entailment module with KBT\n        self.fairness = True  # if we want to add debiasing module.\n\n        #Controlling entailment\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = False\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True   \n        # True: uses BART model for the entailment, False: uses LLM\n        self.BART_model = False\n        self.shots = 6 #few-shot learning per chi fa entailment\n        \n        #controlling bias detection\n        # True: uses encoder model. False: uses LLM\n        self.bias_encoder_model = False\n        \n        #controlling hate speech detection\n        # True: uses encoder model. False: uses LLM\n        self.hate_encoder_model = True\n        \n        #controlling final generation \n        # True: organize final output in section. False: organize output without sections\n        self.organize = True\n        \n        # For testing\n        self.observer = None\n    \nconfigs = Config(retriever,retrievers_KBT,aspects)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:37.932731Z","iopub.execute_input":"2024-08-23T09:38:37.933102Z","iopub.status.idle":"2024-08-23T09:38:37.941247Z","shell.execute_reply.started":"2024-08-23T09:38:37.933072Z","shell.execute_reply":"2024-08-23T09:38:37.940313Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Starting language model","metadata":{}},{"cell_type":"code","source":"start_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:37.942546Z","iopub.execute_input":"2024-08-23T09:38:37.942921Z","iopub.status.idle":"2024-08-23T09:38:45.604048Z","shell.execute_reply.started":"2024-08-23T09:38:37.942885Z","shell.execute_reply":"2024-08-23T09:38:45.602592Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n2024/08/23 09:38:37 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\ntime=2024-08-23T09:38:37.971Z level=INFO source=images.go:782 msg=\"total blobs: 5\"\ntime=2024-08-23T09:38:37.971Z level=INFO source=images.go:790 msg=\"total unused blobs removed: 0\"\ntime=2024-08-23T09:38:37.971Z level=INFO source=routes.go:1172 msg=\"Listening on 127.0.0.1:11434 (version 0.3.6)\"\ntime=2024-08-23T09:38:38.161Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama3389753784/runners\ntime=2024-08-23T09:38:44.841Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]\"\ntime=2024-08-23T09:38:44.841Z level=INFO source=gpu.go:204 msg=\"looking for compatible GPUs\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/23 - 09:38:45 | 200 |      48.605µs |       127.0.0.1 | HEAD     \"/\"\n\u001b[?25lpulling manifest ⠋ \u001b[?25h","output_type":"stream"},{"name":"stderr","text":"time=2024-08-23T09:38:45.059Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-2a53c188-cf51-4809-9529-9ad2b5080292 library=cuda compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"936.1 MiB\"\ntime=2024-08-23T09:38:45.059Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-d5c49a32-53b6-07a7-f517-00cb76078b45 library=cuda compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.5 GiB\"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h[GIN] 2024/08/23 - 09:38:45 | 200 |  434.297875ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u001b[?25h\nError: pull model manifest: file does not exist\n[GIN] 2024/08/23 - 09:38:45 | 200 |      21.685µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2024/08/23 - 09:38:45 | 200 |   22.719484ms |       127.0.0.1 | POST     \"/api/show\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25htime=2024-08-23T09:38:45.887Z level=INFO source=sched.go:710 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe gpu=GPU-d5c49a32-53b6-07a7-f517-00cb76078b45 parallel=4 available=15615524864 required=\"6.2 GiB\"\ntime=2024-08-23T09:38:45.888Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.5 GiB]\" memory.required.full=\"6.2 GiB\" memory.required.partial=\"6.2 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.2 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-08-23T09:38:45.889Z level=INFO source=server.go:393 msg=\"starting llama server\" cmd=\"/tmp/ollama3389753784/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 37489\"\ntime=2024-08-23T09:38:45.889Z level=INFO source=sched.go:445 msg=\"loaded runners\" count=1\ntime=2024-08-23T09:38:45.890Z level=INFO source=server.go:593 msg=\"waiting for llama runner to start responding\"\ntime=2024-08-23T09:38:45.890Z level=INFO source=server.go:627 msg=\"waiting for server to become available\" status=\"llm server error\"\n\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"132129141325824\" timestamp=1724405925\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"132129141325824\" timestamp=1724405925 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"37489\" tid=\"132129141325824\" timestamp=1724405925\n","output_type":"stream"},{"name":"stderr","text":"llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 2\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25hllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\ntime=2024-08-23T09:38:46.141Z level=INFO source=server.go:627 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25hllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\n\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25hllm_load_vocab: special tokens cache size = 256\n\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25hllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25hllm_load_tensors: ggml ctx size =    0.27 MiB\n\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25hllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4156.00 MiB\n\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25hllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25hllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25htime=2024-08-23T09:38:49.582Z level=INFO source=server.go:632 msg=\"llama runner started in 3.69 seconds\"\n\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"132129141325824\" timestamp=1724405929\n[GIN] 2024/08/23 - 09:38:49 | 200 |   3.93889032s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = {\"question\": \"How people stayed during covid19 pandemy?\", \"aspects\": configs.aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\nanswer = value[\"final_answer\"]\nprint(answer)","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Funzioni di supporto:\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n\n\n\"\"\"\nqueries_gpt = [\"Does the COVID-19 vaccine contain a microchip for controlling people?\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Does inhaling hot steam kill the COVID-19 virus?\",\n\"Was COVID-19 created in a lab as a biological weapon?\",\n\"Does drinking alcohol help prevent COVID-19 infection?\",\n\"Is 5G responsible for spreading COVID-19?\",\n\"Do masks cause a reduction in blood oxygen levels?\",\n\"Do COVID-19 vaccines cause infertility in women?\",\n\"Can high doses of vitamin C cure COVID-19?\",\n\"Can pets transmit COVID-19 to humans?\"]\n\"\"\"\n\n# Query più generali\nqueries_gpt = [\"Tell me about the Covid-19 pandemic.\",\n\"What measures were taken to manage the Covid-19 emergency?\",\n\"What were the repercussions of the Covid-19 pandemic?\",\n\"How did people experience the Covid-19 pandemic?\",\n\"Tell me about the misinformation spread during the Covid-19 pandemic.\",\n\"How important was the role of doctors during the covid19 pandemic?\",\n\"What origin does covid19 have?\",\n\"Effectiveness of COVID-19 vaccines\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Was COVID-19 created in a lab as a biological weapon?\"]\n\n#write_file(\"/kaggle/working/gpt_queries.json\",queries_gpt)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:50.134603Z","iopub.execute_input":"2024-08-23T09:38:50.135634Z","iopub.status.idle":"2024-08-23T09:38:50.147371Z","shell.execute_reply.started":"2024-08-23T09:38:50.135575Z","shell.execute_reply":"2024-08-23T09:38:50.146146Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Predisposizione dell'observer per salvare i risultati","metadata":{}},{"cell_type":"code","source":"class Observer(object):\n    def __init__(self):\n        self.query=\"\"\n        self.type_of_acceptance=\"\"\n        self.neutral_acceptance=False\n        self.aspects={}\n        self.final_answer=\"\"\n    \n    def generate_dict(self):\n        return {\"query\": self.query,\n                \"type_of_acceptance\": self.type_of_acceptance,\n                \"neutral_acceptance\": self.neutral_acceptance,\n                \"aspects\": self.aspects,\n                \"final_answer\": self.final_answer}","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:38:52.255680Z","iopub.execute_input":"2024-08-23T09:38:52.256691Z","iopub.status.idle":"2024-08-23T09:38:53.448834Z","shell.execute_reply.started":"2024-08-23T09:38:52.256650Z","shell.execute_reply":"2024-08-23T09:38:53.447742Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Combinazioni di configurazione con skeptical/credulous e neutral","metadata":{}},{"cell_type":"markdown","source":"Multiple combination for entailment:","metadata":{}},{"cell_type":"code","source":"shots = [0,3,6,12]\ncombination = [(True,True), (True,False), (False,True), (False,False)] #Skeptical-Neutral #Skeptical-No-Neutral #Cred-Neu #Cred-No-Neu\n\nfor shot in shots:\n    for comb in combination:\n        print(f\"Start combination with shots: {shot}\")\n        configs.shots = shot\n        configs.strategy_entailment =  comb[0]\n        configs.neutral_acceptance = comb[1]\n    \n        queries_list = queries_gpt\n\n        attempt = 1\n        ret_dict = {}\n        for query in queries_list:\n            inputs = {\"question\": query, \"aspects\": configs.aspects}\n            print(f\"Attempt {attempt} start\")\n    \n            configs.observer = Observer()\n            configs.observer.query=query\n            if configs.strategy_entailment:\n                configs.observer.type_of_acceptance=\"Skeptical\"\n            else:\n                configs.observer.type_of_acceptance=\"Credulous\"\n            configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n            for output in master_flow(configs).stream(inputs):\n                for key, value in output.items():\n                    pass\n                    #pprint(f\"Finished running: {key}:\")\n            answer = value[\"final_answer\"]\n    \n            configs.observer.final_answer= answer\n            ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n            attempt = attempt + 1\n\n        stringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\n        write_file(f\"/kaggle/working/llama31_{shot}_shots_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Single test for a list of query (complete test):","metadata":{}},{"cell_type":"code","source":"#queries_list = load_queries(\"/kaggle/input/preference/gpt_queries.json\")\nqueries_list = [\"Talk to me about COVID-19\"] #queries_gpt\n\nattempt = 1\nret_dict = {}\nfor query in queries_list:\n    inputs = {\"question\": query, \"aspects\": configs.aspects}\n    print(f\"Attempt {attempt} start\")\n    \n    configs.observer = Observer()\n    configs.observer.query=query\n    if configs.strategy_entailment:\n        configs.observer.type_of_acceptance=\"Skeptical\"\n    else:\n        configs.observer.type_of_acceptance=\"Credulous\"\n    configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n    for output in master_flow(configs).stream(inputs):\n        for key, value in output.items():\n            pass\n            #pprint(f\"Finished running: {key}:\")\n    answer = value[\"final_answer\"]\n    \n    configs.observer.final_answer= answer\n    ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n    attempt = attempt + 1\n\nstringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\nwrite_file(f\"/kaggle/working/complete_test_llama31.json\",ret_dict)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T09:39:02.566423Z","iopub.execute_input":"2024-08-23T09:39:02.567099Z","iopub.status.idle":"2024-08-23T09:40:42.284664Z","shell.execute_reply.started":"2024-08-23T09:39:02.567064Z","shell.execute_reply":"2024-08-23T09:40:42.283737Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Attempt 1 start\n[GIN] 2024/08/23 - 09:39:03 | 200 |  1.017590551s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:03 | 200 |  1.060300481s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:04 | 200 |  1.218873046s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:06 | 200 |  605.899783ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:06 | 200 |  617.006286ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:06 | 200 |   631.70296ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  345.022903ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |   425.82514ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  324.712528ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  247.495893ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  340.913533ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |   334.28902ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  341.215779ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  309.981585ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:07 | 200 |  388.073991ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  328.527631ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  271.488702ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  415.181277ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  359.464815ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |   319.19361ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  374.709182ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  445.426394ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:08 | 200 |  349.243893ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  265.801008ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  271.369463ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  473.716067ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  335.223932ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  358.948634ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  326.781142ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |   325.83344ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  473.422474ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:09 | 200 |  278.113927ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:10 | 200 |  193.084686ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:15 | 200 |  1.698707731s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:15 | 200 |  1.372191312s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:16 | 200 |   1.93412221s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:16 | 200 |  996.466163ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:17 | 200 |  1.271947591s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:17 | 200 |  911.327135ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:17 | 200 |  995.500902ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:17 | 200 |  723.839631ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:18 | 200 |  1.026677275s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:18 | 200 |  881.868728ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:18 | 200 |  848.459681ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:19 | 200 |  1.127191221s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:19 | 200 |  1.194036919s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:20 | 200 |  1.512415082s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:20 | 200 |  1.277690726s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:20 | 200 |  915.133586ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:21 | 200 |  1.199396756s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:21 | 200 |  1.195253308s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:22 | 200 |  1.205811151s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:22 | 200 |  1.180141646s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:23 | 200 |  1.203411531s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:23 | 200 |  1.174544251s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:23 | 200 |   920.90903ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:24 | 200 |  972.875406ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:24 | 200 |  960.935311ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:24 | 200 |  1.171962004s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:25 | 200 |  1.105135555s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:25 | 200 |  1.171133024s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:25 | 200 |  1.186698231s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:26 | 200 |  1.153703479s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:26 | 200 |  1.075645122s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:26 | 200 |  925.506815ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:27 | 200 |  862.261063ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:29 | 200 |  2.261772068s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:29 | 200 |  2.639305778s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:29 | 200 |  1.793594525s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:30 | 200 |   1.14089358s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:30 | 200 |  1.105114495s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:30 | 200 |  1.215685941s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:31 | 200 |  1.129263119s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:31 | 200 |   1.16462553s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:31 | 200 |  1.151100847s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:33 | 200 |  1.509899157s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:33 | 200 |  2.330639736s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:34 | 200 |  950.153066ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:35 | 200 |  1.011089179s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:35 | 200 |  1.166366425s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:36 | 200 |  1.016719529s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:36 | 200 |  1.102770297s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:37 | 200 |  929.425389ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:37 | 200 |   1.67445433s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:37 | 200 |  916.434602ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:38 | 200 |  961.010709ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:38 | 200 |  972.935006ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:40 | 200 |  1.166012917s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:40 | 200 |  1.215848707s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:40 | 200 |  9.125622449s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:41 | 200 |  1.132405498s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:41 | 200 |  1.136955417s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:42 | 200 |  1.859967089s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:42 | 200 |  1.573664132s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:42 | 200 |  1.607105001s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:43 | 200 |  852.097273ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:43 | 200 |  921.093496ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:44 | 200 |  482.731891ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:44 | 200 |  486.073781ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:45 | 200 |   706.60069ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:45 | 200 |  736.073716ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:45 | 200 |  466.416067ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:45 | 200 |  756.835428ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:46 | 200 |  684.971907ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:46 | 200 |  796.040736ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:47 | 200 |  772.146715ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:47 | 200 |  949.158746ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:47 | 200 |  918.190607ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:48 | 200 |  878.304349ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:48 | 200 |  1.026131706s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:49 | 200 |  1.081055119s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:50 | 200 |  7.397146485s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:50 | 200 |  1.049424249s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:50 | 200 |   957.65952ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:50 | 200 |  870.387835ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:51 | 200 |  999.050469ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:51 | 200 |  969.365117ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:52 | 200 |  681.890722ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:52 | 200 |  624.614729ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:53 | 200 |  710.783563ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:53 | 200 |  830.458942ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:53 | 200 |  894.751078ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:54 | 200 |  902.159375ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:54 | 200 |  846.396754ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:55 | 200 |  1.046766217s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:56 | 200 |  1.178062655s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:56 | 200 |  1.155390175s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:57 | 200 |  1.144338939s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:57 | 200 |  1.109301291s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:58 | 200 |  1.123418225s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:58 | 200 |  1.231267269s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:39:59 | 200 |  1.296795744s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:00 | 200 |  1.290508998s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:00 | 200 |  1.251180399s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:01 | 200 |  1.218210992s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:02 | 200 |  1.316798787s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:02 | 200 |  1.274756259s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:03 | 200 |  1.035167074s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:03 | 200 |  992.983244ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:04 | 200 |  1.001948963s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:05 | 200 |  1.163247637s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:05 | 200 |  1.217950422s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:06 | 200 |  1.155062465s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:06 | 200 |  1.126266111s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:07 | 200 |  1.027012928s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:07 | 200 |   1.15702856s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:08 | 200 |  1.162372635s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:09 | 200 |  1.482489312s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:09 | 200 |  1.013373565s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:10 | 200 |  680.671866ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:10 | 200 |  767.585872ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:10 | 200 |   413.27148ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:11 | 200 |  780.175176ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:11 | 200 |  851.159607ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:12 | 200 |  972.944353ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:12 | 200 |  966.782796ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:12 | 200 |  880.508238ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:13 | 200 |  1.506891655s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:14 | 200 |  1.096446358s |       127.0.0.1 | POST     \"/api/generate\"\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1169 n_keep=4 n_left=2044 n_shift=1022 tid=\"132129141325824\" timestamp=1724406014\n[GIN] 2024/08/23 - 09:40:16 | 200 |  2.128360963s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:17 | 200 |  857.693473ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:21 | 200 |  4.484636338s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:22 | 200 |  942.851154ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:26 | 200 | 12.211065854s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:26 | 200 |  3.781841797s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:26 | 200 |  610.831827ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:33 | 200 |  6.544351129s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/23 - 09:40:42 | 200 |  8.808367153s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Entailment evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione su notizie vere (0) e notizie false (1)","metadata":{}},{"cell_type":"code","source":"# Label 0, notizie vere\n# Label 1, notizie false\n\ndef results_on_label(label, num_attempts, dict_input, aspects):\n    recall = 0\n    precision = 0\n    support = 0\n    valid_attempts = 0\n    recalls = []\n    precisions = []\n\n    for i in range(1,num_attempts+1):\n        for aspect in aspects:\n            recall = recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"]\n            precision = precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"]\n            support_dict = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"support\"]\n            # Il tentativo è valido solo se c'è almeno un documento recuperato!\n            if support_dict != 0:\n                recalls.append(dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"])\n                precisions.append(dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"])\n                valid_attempts = valid_attempts + 1\n                support = support + support_dict\n                \n    e_recall = recall/valid_attempts\n    e_precision = precision/valid_attempts\n    f1 = 2*e_recall*e_precision/(e_recall+e_precision)\n    diff_rec = [(rec - e_recall)**2 for rec in recalls]\n    diff_prec = [(prec - e_precision)**2 for prec in precisions]\n    var_recall = sum(diff_rec)/valid_attempts\n    var_precision =  sum(diff_prec)/valid_attempts\n    return {\"recall\":e_recall, \n            \"var_recall\": var_recall,\n            \"precision\":e_precision,\n            \"var_precision\": var_precision,\n            \"f1\":f1, \n            \"support\":support/(num_attempts*len(aspects))}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione per aspetto","metadata":{}},{"cell_type":"code","source":"def results_on_aspects(aspect, num_attempts, dict_input):\n    recall_0 = 0\n    precision_0 = 0\n    recall_1 = 0\n    precision_1 = 0\n    \n    accuracy = 0\n    \n    weight_recall = 0\n    weight_precision = 0\n    \n    valid_attempts_0 = 0\n    valid_attempts_1 = 0\n\n    for i in range(1,num_attempts+1):\n        recall_0 = recall_0 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"recall\"]\n        precision_0 = precision_0 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"precision\"]\n        support_dict_0 = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"support\"]\n        if support_dict_0 != 0:\n            valid_attempts_0 = valid_attempts_0 + 1\n        \n        recall_1 = recall_1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"recall\"]\n        precision_1 = precision_1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"precision\"]\n        support_dict_1 = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"support\"]\n        if support_dict_1 != 0:\n            valid_attempts_1 = valid_attempts_1 + 1\n            \n        weight_recall = weight_recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"recall\"]\n        weight_precision = weight_precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"precision\"]\n                \n        try:\n            accuracy = accuracy + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"accuracy\"]\n        except:\n            accuracy = accuracy + 1 #se l'accuracy non è presente nel dizionario, è perché vale 1 (vedi esempio 4 di llama_second_skeptical_neutral)\n    \n    e_recall_0 = recall_0/valid_attempts_0\n    e_precision_0 = precision_0/valid_attempts_0\n    \n    e_recall_1 = recall_1/valid_attempts_1\n    e_precision_1 = precision_1/valid_attempts_1\n    \n    macro_recall = (e_recall_0+e_recall_1)/2\n    macro_precision = (e_precision_0+e_precision_1)/2\n    f1 = 2*macro_recall*macro_precision/(macro_recall+macro_precision)\n    \n    w_recall = weight_recall/(num_attempts)\n    w_precision = weight_precision/(num_attempts)\n    weight_f1 = 2*w_recall*w_precision/(w_recall+w_precision)\n    return {\"macro_recall\": macro_recall,\n            \"macro_precision\": macro_precision, \n            \"macro_f1\": f1,\n            \"macro_accuracy\":accuracy/(num_attempts),\n            \"w_recall\": w_recall,\n            \"w_precision\": w_precision, \n            \"w_f1\":weight_f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label 0: notizie vere (negative class)\n# label 1: notizie false (positive class)\nimport pandas as pd\n\ndef compute_result_on_label(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        dict_input = load_json(f\"/kaggle/working/{model}_{shot}_shots_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_gov_10kbt_{acceptance}_{neutral}.json\")\n\n    results_label_0 = results_on_label(\"0\", num_queries, dict_input, aspects)\n    results_label_1 = results_on_label(\"1\", num_queries, dict_input, aspects)\n    \n\n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n\n    df[\"precision_0\"] = results_label_0[\"precision\"]\n    df[\"var_prec_0\"] = results_label_0[\"var_precision\"]\n    df[\"recall_0\"] = results_label_0[\"recall\"]\n    df[\"var_recall_0\"] = results_label_0[\"var_recall\"]\n    df[\"f1_0\"] = results_label_0[\"f1\"]\n    df[\"support_0\"] = results_label_0[\"support\"]\n    \n\n    df[\"precision_1\"] = results_label_1[\"precision\"]\n    df[\"var_prec_1\"] = results_label_1[\"var_precision\"]\n    df[\"recall_1\"] = results_label_1[\"recall\"]\n    df[\"var_recall_1\"] = results_label_1[\"var_recall\"]\n    df[\"f1_1\"] = results_label_1[\"f1\"]\n    df[\"support_1\"] = results_label_1[\"support\"]\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_result_on_aspects(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        dict_input = load_json(f\"/kaggle/working/{model}_{shot}_shots_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_gov_{acceptance}_{neutral}.json\")\n        \n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n    \n    for aspect in aspects:\n        result = results_on_aspects(aspect, num_queries, dict_input)\n        df[f\"precision_{aspect}\"] = result[\"macro_precision\"]\n        df[f\"recall_{aspect}\"] = result[\"macro_recall\"]\n        df[f\"f1_{aspect}\"] = result[\"macro_f1\"]\n        df[f\"accuracy_{aspect}\"] = result[\"macro_accuracy\"]\n        \n        df[f\"w_recall_{aspect}\"]= result[\"w_recall\"]\n        df[f\"w_precision_{aspect}\"]= result[\"w_precision\"]\n        df[f\"w_f1_{aspect}\"]= result[\"w_f1\"]\n        \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import openpyxl\n\nattempts = 10\nnum_aspects = 3\n\nmodels = [\"llama31\"] #[\"gemma2\",\"BART\",]\nshots = [0,3,6,12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Governmental\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_labels_X.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_label(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\nresults.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attempts = 10\nnum_aspects = 3\n\nmodels = [\"llama31\"]#[\"BART\",\"llama31\"] #[\"gemma2\"]\nshots = [0,3,6,12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Governmental\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_aspects_X.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_aspects(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\nresults.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ragas (application evaluation)","metadata":{}},{"cell_type":"markdown","source":"https://docs.ragas.io/en/stable/","metadata":{}},{"cell_type":"code","source":"#todo","metadata":{},"execution_count":null,"outputs":[]}]}