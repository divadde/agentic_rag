{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9197853,"sourceType":"datasetVersion","datasetId":5560790},{"sourceId":9212052,"sourceType":"datasetVersion","datasetId":5570293},{"sourceId":98659,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":82777,"modelId":107076}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 ragas langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bias detection model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_entailment/Textual%20Entailment%20Explanation%20Demo.html\n- https://huggingface.co/facebook/bart-large-mnli","metadata":{}},{"cell_type":"markdown","source":"Entailment model (BART):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbart_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\",device=device)\nbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BART_prediction(premise,hypothesis):\n    #print(f\"Premise: {premise}\")\n    #print(f\"Hypo: {hypothesis}\")\n    input_ids = bart_tokenizer.encode(premise, hypothesis, return_tensors=\"pt\")\n    logits = bart_model(input_ids)[0]\n    probs = logits.softmax(dim=1)\n\n    max_index = torch.argmax(probs).item()\n\n    bart_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n    return bart_label_map[max_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nBERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nBERT_model = BertForSequenceClassification.from_pretrained('/kaggle/input/bert_hate_speech/pytorch/default/1/fine_tuned_bert')\nBERT_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BERT_hate_speech(text):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BERT_model.to(device)\n\n    encoded_dict = BERT_tokenizer.encode_plus(\n                    text,\n                    add_special_tokens = True,\n                    max_length = None, #Uso della massima lunghezza del modello, nel caso di BERT 512 tokens\n                    padding = \"max_length\",\n                    truncation = True,\n                    return_attention_mask = True,\n                    return_tensors = 'pt',\n                )\n\n    text_ids = encoded_dict['input_ids'].to(device)\n    attention_mask = encoded_dict['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = BERT_model(input_ids=text_ids, attention_mask=attention_mask)\n\n    # Estrai i logits (output non normalizzati del modello)\n    logits = outputs.logits\n\n    # Converti i logits in probabilità (se necessario)\n    probs = torch.softmax(logits, dim=1)\n\n    # Identifica la classe con la probabilità più alta\n    predicted_class = torch.argmax(probs, dim=1)\n    \n    #Label 0, no hate speech, Label 1 hate speech\n    if predicted_class==0: response = \"no\"\n    else: response = \"yes\"\n\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Web Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nweb_search_tool = TavilySearchResults(k=2)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"markdown","source":"osservazione sul chuncking: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338","metadata":{}},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\ndef create_retriever(index_name,top_k):\n    vectorstore = PineconeVectorStore(\n        index_name=index_name,\n        embedding=AI21Embeddings(device=\"cuda\")\n    )\n    return vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\ndef create_KBT_retrievers(aspects,top_k):\n    retrievers = []\n    for aspect in aspects:\n        retriever = create_retriever(f\"{aspect.lower()}-kbt\",top_k)\n        retrievers.append(retriever)\n    return retrievers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt support","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import Ollama\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\ndef create_prompt(system_prompt, input_variables, model, examples=None):\n    input_var_str = [f\"{input_var}\" for input_var in input_variables]\n    if \"llama3.1\" in model:\n        prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \"+ system_prompt\n    else:\n        prompt = system_prompt\n    \n    if examples!=None:\n        prompt = prompt + \"\\nHere some examples: \\n\"\n        for example in examples:\n            prompt = prompt + \"\\n\"\n            for key in input_var_str:\n                prompt = prompt + key + \": \" + example[key] +\"\\n\"\n            prompt = prompt + \"ouput: \" + example[\"output\"] +\"\\n\"   \n        prompt = prompt + \"\\n\"\n    \n    if \"llama3.1\" in model:\n        prompt = prompt + \"<|eot_id|><|start_header_id|>user<|end_header_id|> \\n\"\n    for key in input_var_str:\n            prompt = prompt + key + \": \" +f\"{{{key}}}\" +\"\\n\"\n    if \"llama3.1\" in model:\n        prompt = prompt + \"output:\" + \" <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n    prompt_template = PromptTemplate(template = prompt, input_variables=input_var_str)\n    \n    #Debug:\n    \"\"\"\n    dict_input = {}\n    for input_var in input_variables:\n        dict_input[f'{input_var}'] = \"USER_SUBMISSION\"\n    print(prompt_template.format(**dict_input))\n    \"\"\"\n    \n    return prompt_template","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def call_model(llm: str, prompt: str, input_variables:list[str], examples=None):\n    model = Ollama(model=llm, temperature=0)\n    prompt_final = create_prompt(prompt, input_variables, llm, examples)\n    return prompt_final | model | StrOutputParser()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prompt(path):\n    with open(path, 'r') as file:\n        prompt = file.read()\n    return prompt\n\ndef get_examples(path):\n    with open(path, 'r') as file:\n        examples = file.read()\n    return eval(examples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://learnprompting.org/docs/reliability/debiasing","metadata":{}},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\nimport sklearn.metrics\nimport numpy as np\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    original_query: str\n    query: str\n    aspect: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n        \ndef rewrite_query(state,verbose,llm,observer):\n    if verbose: \n        print(\"---REWRITING QUERY---\")\n        print(f\"State: {state}\")\n    original_query = state[\"original_query\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/query_rewriting.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/query_rewriting_examples.txt\")\n    chain = call_model(llm, prompt, [\"original_query\",\"aspect\"], examples)\n    generation = chain.invoke({\"original_query\": original_query, \"aspect\": aspect})\n    #print(f\"Aspect {aspect} query: {generation}\")\n    #print(list(generation.values())) #Debug\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"] = {\"aspect_query\": generation}\n    return {\"query\": generation}\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fairness,observer):\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n\n    # RAG generation\n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/generating_answer.txt\")\n    chain = call_model(llm, prompt, [\"context\",\"question\"])\n    documents_content = [d.page_content for d in documents]\n    generation = chain.invoke({\"context\": documents_content, \"question\": query})\n    \n    aspect_id = state[\"aspect_id\"]\n    #print(f\"Aspect agent {aspect_id} generates: {generation}\") #Debug\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"documents_for_generation\"] = documents_content\n            observer.aspects[f\"{aspect}\"][\"answer\"] = generation\n            observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = \"//\"\n    if fairness:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation], \"ord_aspects\": [state[\"aspect\"]]}\n\n\ndef confirm_answer(state,verbose):\n    if verbose:\n        print(\"---CONFIRM ANSWER---\")\n        print(f\"State: {state}\")\n    my_answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n\n    #print(f\"Answer for aspect {aspect}: {my_answer}\")\n    return {\"answers_agent\": [my_answer], \"ord_aspects\": [aspect]}\n\n\ndef grade_documents(state,verbose,llm,observer):\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/retrieval_grader.txt\")\n    chain = call_model(llm, prompt, [\"question\",\"document\"])\n\n    # Score each doc\n    filtered_docs = []\n    not_relevant_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = chain.invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if \"yes\" in score.lower():\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            not_relevant_docs.append(d.page_content)\n            web_search = \"Yes\"\n            continue\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"web_search\"] = web_search\n            observer.aspects[f\"{aspect}\"][\"not_relevants_docs\"] = not_relevant_docs\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose):\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    #print(f\"docs from web: {docs}\") #Debug\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents([web_results])\n    for doc in doc_splits:\n        if documents is None:\n            documents = [doc]\n        else:\n            documents.append(doc)\n    return {\"documents\": documents, \"query\": query}\n\n\ndef hate_speech_filter(state,verbose,llm,encoder,observer):\n    if verbose:\n        print(\"---HATE SPEECH FILTER---\")\n        print(f\"State: {state}\")\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/hate_speech.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/hate_speech_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    hate_speech_docs = []\n    for d in documents:\n        if encoder:\n            score = BERT_hate_speech(d.page_content)\n        else:\n            score = chain.invoke(\n                {\"input\": d.page_content}\n            )\n        #grade = score[\"score\"]\n        if \"no\" in score.lower():\n            if verbose: print(\"---DOCUMENT ACCEPTED---\")\n            filtered_docs.append(d)\n        else:\n            if verbose: print(\"---DOCUMENT HATEFUL---\")\n            hate_speech_docs.append(d.page_content)\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"hate_speech_docs\"] = hate_speech_docs\n    return {\"documents\": filtered_docs}\n\n\ndef entailment_filter(state,BART_model,strategy_entailment,neutral_acceptance,verbose,observer,llm):    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n    aspect_id = state[\"aspect_id\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/entailment_checker.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/entailment_checker_examples.txt\")\n    chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    #counter_docs = 0 # Debug\n    #La procedura si può semplificare: TODO\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"contradiction\" in score.lower():\n                    # contradiction found\n                    break\n            if (not neutral or neutral_acceptance) and score.lower() != \"contradiction\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug            \n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                # Document entailed\n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"entailment\" in score.lower():\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                    filtered_docs.append(d)\n                    break\n            if (neutral and neutral_acceptance) and score.lower() != \"entailment\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    \n    if observer!=None:\n        #todo misura metriche con sklearn classifier\n        #0 tweet veri, #1 tweet falsi\n        y_true = [int(document.metadata.get(\"label\")) for document in documents if \"label\" in document.metadata]\n        y_pred = [0 if document in filtered_docs else 1 for document in documents if \"label\" in document.metadata]\n        #print(f\"Real docs with aspect {aspect_id} ,y_true: {y_true}\") #Debug\n        #print(f\"Filtered docs with aspect {aspect_id} ,y_pred: {y_pred}\") #Debug\n        report = sklearn.metrics.classification_report(y_true,y_pred,labels=[0,1],\n                                                       output_dict=True,zero_division=0)\n        observer.aspects[f\"{aspect}\"][\"report_entailment\"] = report\n        \n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose,llm,observer):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/debiasing_answer.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/debiasing_answer_examples.txt\")\n    chain = call_model(llm, prompt, [\"text\"], examples)\n    \n    unbiased_answer = chain.invoke({\"text\": answer})\n    \n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = unbiased_answer\n    \n    #print(f\"Answer for aspect {aspect}: {unbiased_answer}\")\n    \n    return {\"answers_agent\": [unbiased_answer], \"ord_aspects\": [aspect]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose,llm,bias_encoder_model):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    response = \"biased\"\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/bias_detection.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/bias_detection_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n    \n    if bias_encoder_model:\n        bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n        response = bias_detection(answer)[0]['label'].lower()\n    else:\n        response = chain.invoke({\"input\": answer})\n        if \"biased\" in response: response = \"biased\"\n        if \"non-biased\" in response: response = \"non-biased\"\n        \n    if verbose: print(response) #biased, non-biased\n    \n    return response\n\n\ndef decide_to_generate(state,verbose):\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: RELEVANT---\")\n        return \"relevant\"","metadata":{"id":"6iV1oJJtLcFM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Workflow condizionale\ndef workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    workflow.add_node(\"rewrite_query\", functools.partial(rewrite_query, verbose=configs.verbose, \n                                                    llm=configs.local_llm, observer=configs.observer))  # query rewriting\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, \n                                                    retriever=configs.retriever, \n                                                    retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, \n                                                    llm=configs.local_llm, \n                                                    fairness=configs.fairness, observer=configs.observer))  # generatae\n    \n    if configs.web_search:\n        workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose))  # web search\n        workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, \n                                                               llm=configs.local_llm,\n                                                               observer=configs.observer))  # grade documents\n    if configs.safeness:\n        workflow.add_node(\"hate_speech_filter\", functools.partial(hate_speech_filter, verbose=configs.verbose, \n                                                                  llm=configs.local_llm, \n                                                                  encoder=configs.hate_encoder_model,\n                                                                  observer=configs.observer))\n    if configs.trustworthiness:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, BART_model=configs.BART_model, strategy_entailment=configs.strategy_entailment, \n                                                                 neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.local_llm, \n                                                                 observer=configs.observer))  # entailment\n    if configs.fairness:  \n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose, \n                                                                llm=configs.local_llm,\n                                                                observer=configs.observer))\n        workflow.add_node(\"confirm_answer\", functools.partial(confirm_answer, verbose=configs.verbose))\n\n    workflow.add_edge(START, \"rewrite_query\")\n    workflow.add_edge(\"rewrite_query\", \"retrieve\")\n    \n    if configs.web_search:\n        workflow.add_edge(\"retrieve\", \"grade_documents\")  \n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            functools.partial(decide_to_generate, verbose=configs.verbose),\n            {\n                \"websearch\": \"websearch\",\n                \"relevant\": \"hate_speech_filter\" if configs.safeness else \"entailment_filter\" if configs.trustworthiness else \"generate\"\n            },\n        )\n    \n    if configs.safeness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"hate_speech_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"hate_speech_filter\")\n        if configs.trustworthiness:\n            workflow.add_edge(\"hate_speech_filter\", \"entailment_filter\")\n            workflow.add_edge(\"entailment_filter\", \"generate\")\n        else:\n            workflow.add_edge(\"hate_speech_filter\",\"generate\")\n    elif configs.trustworthiness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"entailment_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n    else:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"generate\")\n        else: \n            workflow.add_edge(\"retrieve\", \"generate\")\n    \n    if configs.fairness:\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose, llm=configs.local_llm, bias_encoder_model=configs.bias_encoder_model),\n            {\n                \"biased\": \"debiasing_filter\",\n                \"non-biased\": \"confirm_answer\",\n            },\n        )\n        workflow.add_edge(\"confirm_answer\", END)\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"generate\", END)\n\n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    final_answer: str\n\ndef send_aspects(state,verbose):\n    if verbose: \n        print(\"---SEND ASPECT TO EACH ASPECT-AGENT---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"original_query\": state[\"question\"], \"aspect\": a, \"aspect_id\": state[\"aspects\"].index(a)}) for a in state[\"aspects\"]]\n\ndef organize_answers(state,verbose,llm,organize):\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n    ord_aspects = state[\"ord_aspects\"]\n    \n    #print(f\"I'm the master agent and I received: {answers_agent}\") #Debug\n    if organize: #with sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer_with_sections.txt\")\n        chain = call_model(llm, prompt, [\"answers\",\"aspects\"])\n        final_output=chain.invoke({\"answers\": answers_agent, \"aspects\": ord_aspects})\n    else: #without sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer.txt\")\n        chain = call_model(llm, prompt, [\"answers\"])\n        final_output=chain.invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"organize_answers\", functools.partial(organize_answers, verbose=configs.verbose, llm=configs.local_llm, organize=configs.organize))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_conditional_edges(START, functools.partial(send_aspects, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_answers\")\n    master_flow.add_edge(\"organize_answers\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"markdown","source":"Vectorstore configuration","metadata":{}},{"cell_type":"code","source":"index_name = \"entailment-test\"\naspects = [\"Health\",\"Technology\",\"Society\"] #\"Technology\", \"Society\"\n\ntop_retriever = 10 #documents retrieved by retriever\ntop_KBT = 5 #documents retrievede by KBT retriever\n\nretriever = create_retriever(index_name, top_retriever)\nretrievers_KBT = create_KBT_retrievers(aspects, top_KBT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,retriever,retrievers_KBT,aspects):\n        self.local_llm = \"llama3.1\" #\"gemma2\"\n        \n        # retrievers\n        self.retriever = retriever\n        self.retrievers_KBT = retrievers_KBT\n        self.aspects = aspects\n        \n        #if we want print all the process: True\n        self.verbose = False \n        \n        #if we want to include websearch in the workflow\n        self.web_search = True\n\n        # Controlling properties\n        self.safeness = True # if we want to add hate speech detection module\n        self.trustworthiness = True # if we want to add entailment module with KBT\n        self.fairness = True  # if we want to add debiasing module.\n\n        #Controlling entailment\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = True\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True   \n        # True: uses BART model for the entailment, False: uses LLM\n        self.BART_model = False\n        \n        #controlling bias detection\n        # True: uses encoder model. False: uses LLM\n        self.bias_encoder_model = False\n        \n        #controlling hate speech detection\n        # True: uses encoder model. False: uses LLM\n        self.hate_encoder_model = True\n        \n        #controlling final generation \n        # True: organize final output in section. False: organize output without sections\n        self.organize = False\n        \n        # For testing\n        self.observer = None\n    \nconfigs = Config(retriever,retrievers_KBT,aspects)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting language model","metadata":{}},{"cell_type":"code","source":"start_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = {\"question\": \"How people stayed during covid19 pandemy?\", \"aspects\": configs.aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\nanswer = value[\"final_answer\"]\nprint(answer)","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Funzioni di supporto:\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n\n\"\"\"\nqueries_gpt = [\"Does the COVID-19 vaccine contain a microchip for controlling people?\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Does inhaling hot steam kill the COVID-19 virus?\",\n\"Was COVID-19 created in a lab as a biological weapon?\",\n\"Does drinking alcohol help prevent COVID-19 infection?\",\n\"Is 5G responsible for spreading COVID-19?\",\n\"Do masks cause a reduction in blood oxygen levels?\",\n\"Do COVID-19 vaccines cause infertility in women?\",\n\"Can high doses of vitamin C cure COVID-19?\",\n\"Can pets transmit COVID-19 to humans?\"]\n\"\"\"\n\nqueries_gpt = [\"Tell me about the Covid-19 pandemic.\",\n\"What measures were taken to manage the Covid-19 emergency?\",\n\"What were the repercussions of the Covid-19 pandemic?\",\n\"How did people experience the Covid-19 pandemic?\",\n\"Tell me about the misinformation spread during the Covid-19 pandemic.\"]\n\n#write_file(\"/kaggle/working/gpt_queries.json\",queries_gpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predisposizione dell'observer per salvare i risultati","metadata":{}},{"cell_type":"code","source":"class Observer(object):\n    def __init__(self):\n        self.query=\"\"\n        self.type_of_acceptance=\"\"\n        self.neutral_acceptance=False\n        self.aspects={}\n        self.final_answer=\"\"\n    \n    def generate_dict(self):\n        return {\"query\": self.query,\n                \"type_of_acceptance\": self.type_of_acceptance,\n                \"neutral_acceptance\": self.neutral_acceptance,\n                \"aspects\": self.aspects,\n                \"final_answer\": self.final_answer}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combinazioni di configurazione con skeptical/credulous e neutral","metadata":{}},{"cell_type":"markdown","source":"Multiple combination for entailment:","metadata":{}},{"cell_type":"code","source":"combination = [(True,True), (True,False), (False,True), (False,False)] #Skeptical-Neutral #Skeptical-No-Neutral #Cred-Neu #Cred-No-Neu\n\nfor comb in combination:\n    print(\"Start combination\")\n    configs.strategy_entailment =  comb[0]\n    configs.neutral_acceptance = comb[1]\n    \n    queries_list = queries_gpt\n\n    attempt = 1\n    ret_dict = {}\n    for query in queries_list:\n        inputs = {\"question\": query, \"aspects\": configs.aspects}\n        print(f\"Attempt {attempt} start\")\n    \n        configs.observer = Observer()\n        configs.observer.query=query\n        if configs.strategy_entailment:\n            configs.observer.type_of_acceptance=\"Skeptical\"\n        else:\n            configs.observer.type_of_acceptance=\"Credulous\"\n        configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n        for output in master_flow(configs).stream(inputs):\n            for key, value in output.items():\n                pass\n                #pprint(f\"Finished running: {key}:\")\n        answer = value[\"final_answer\"]\n    \n        configs.observer.final_answer= answer\n        ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n        attempt = attempt + 1\n\n    stringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\n    write_file(f\"/kaggle/working/test_llama31_X_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Single test for a list of query (complete test):","metadata":{}},{"cell_type":"code","source":"#queries_list = load_queries(\"/kaggle/input/preference/gpt_queries.json\")\nqueries_list = [\"Talk to me about COVID-19\"] #queries_gpt\n\nattempt = 1\nret_dict = {}\nfor query in queries_list:\n    inputs = {\"question\": query, \"aspects\": configs.aspects}\n    print(f\"Attempt {attempt} start\")\n    \n    configs.observer = Observer()\n    configs.observer.query=query\n    if configs.strategy_entailment:\n        configs.observer.type_of_acceptance=\"Skeptical\"\n    else:\n        configs.observer.type_of_acceptance=\"Credulous\"\n    configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n    for output in master_flow(configs).stream(inputs):\n        for key, value in output.items():\n            pass\n            #pprint(f\"Finished running: {key}:\")\n    answer = value[\"final_answer\"]\n    \n    configs.observer.final_answer= answer\n    ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n    attempt = attempt + 1\n\nstringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\nwrite_file(f\"/kaggle/working/complete_test_llama31.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Entailment evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione su notizie vere (0) e notizie false (1)","metadata":{}},{"cell_type":"code","source":"# Label 0, notizie vere\n# Label 1, notizie false\n\ndef results_on_label(label, num_attempts, dict_input, aspects):\n    recall = 0\n    precision = 0\n    f1 = 0\n    support = 0\n\n    for i in range(1,num_attempts+1):\n        for aspect in aspects:\n            recall = recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"]\n            precision = precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"]\n            f1 = f1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"f1-score\"]\n            support = support + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"support\"]\n\n    return {\"recall\":recall/(num_attempts*len(aspects)), \n            \"precision\":precision/(num_attempts*len(aspects)),\n            \"f1\":f1/(num_attempts*len(aspects)), \n            \"support\":support/(num_attempts*len(aspects))}","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:39:45.213144Z","iopub.execute_input":"2024-08-21T17:39:45.213566Z","iopub.status.idle":"2024-08-21T17:39:45.223226Z","shell.execute_reply.started":"2024-08-21T17:39:45.213533Z","shell.execute_reply":"2024-08-21T17:39:45.222090Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione per aspetto","metadata":{}},{"cell_type":"code","source":"def results_on_aspects(aspect, num_attempts, dict_input):\n    recall = 0\n    precision = 0\n    f1 = 0\n\n    weight_recall = 0\n    weight_precision = 0\n    weight_f1 = 0\n    \n    macro_recall = 0\n    macro_precision = 0\n    macro_f1 = 0\n    \n    accuracy = 0\n\n    for i in range(1,num_attempts+1):\n        recall = recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"macro avg\"][\"recall\"]\n        precision = precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"macro avg\"][\"precision\"]\n        f1 = f1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"macro avg\"][\"f1-score\"]\n        \n        macro_recall = macro_recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"macro avg\"][\"recall\"]\n        macro_precision = macro_precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"macro avg\"][\"precision\"]\n        macro_f1 = macro_f1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"macro avg\"][\"f1-score\"]\n        \n        weight_recall = weight_recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"recall\"]\n        weight_precision = weight_precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"precision\"]\n        weight_f1 = weight_f1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"f1-score\"]\n        try:\n            accuracy = accuracy + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"accuracy\"]\n        except:\n            accuracy = accuracy + 1 #se l'accuracy non è presente nel dizionario, è perché vale 1 (vedi esempio 4 di llama_second_skeptical_neutral)\n    \n    return {\"recall\": recall/(num_attempts),\n            \"precision\": precision/(num_attempts), \n            \"f1\": f1/(num_attempts),\n            \"macro_recall\": macro_recall/(num_attempts),\n            \"macro_precision\": macro_precision/(num_attempts),\n            \"macro_f1\": macro_f1/(num_attempts),\n            \"w_recall\": weight_recall/(num_attempts),\n            \"w_precision\": weight_precision/(num_attempts), \n            \"w_f1\":weight_f1/(num_attempts), \n            \"accuracy\":accuracy/(num_attempts)}","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:46:21.078273Z","iopub.execute_input":"2024-08-21T17:46:21.078960Z","iopub.status.idle":"2024-08-21T17:46:21.092192Z","shell.execute_reply.started":"2024-08-21T17:46:21.078930Z","shell.execute_reply":"2024-08-21T17:46:21.091280Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:46:22.569247Z","iopub.execute_input":"2024-08-21T17:46:22.569652Z","iopub.status.idle":"2024-08-21T17:46:22.575337Z","shell.execute_reply.started":"2024-08-21T17:46:22.569621Z","shell.execute_reply":"2024-08-21T17:46:22.574385Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"# label 0: notizie vere (negative class)\n# label 1: notizie false (positive class)\nimport pandas as pd\n\ndef compute_result_on_label(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_newq_{acceptance}_{neutral}.json\")\n        dict_input = load_json(\"/kaggle/working/complete_test_llama31.json\")\n\n    results_label_0 = results_on_label(\"0\", num_queries, dict_input, aspects)\n    results_label_1 = results_on_label(\"1\", num_queries, dict_input, aspects)\n    \n\n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n\n    df[\"precision_0\"] = results_label_0[\"precision\"]\n    df[\"recall_0\"] = results_label_0[\"recall\"]\n    df[\"f1_0\"] = results_label_0[\"f1\"]\n    df[\"support_0\"] = results_label_0[\"support\"]\n    \n\n    df[\"precision_1\"] = results_label_1[\"precision\"]\n    df[\"recall_1\"] = results_label_1[\"recall\"]\n    df[\"f1_1\"] = results_label_1[\"f1\"]\n    df[\"support_1\"] = results_label_1[\"support\"]\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:46:22.735945Z","iopub.execute_input":"2024-08-21T17:46:22.736403Z","iopub.status.idle":"2024-08-21T17:46:22.745279Z","shell.execute_reply.started":"2024-08-21T17:46:22.736352Z","shell.execute_reply":"2024-08-21T17:46:22.744404Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"def compute_result_on_aspects(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_newq_{acceptance}_{neutral}.json\")\n        dict_input = load_json(\"/kaggle/working/complete_test_llama31.json\")\n        \n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n    \n    for aspect in aspects:\n        result = results_on_aspects(aspect, num_queries, dict_input)\n        df[f\"precision_{aspect}\"] = result[\"precision\"]\n        df[f\"recall_{aspect}\"] = result[\"recall\"]\n        df[f\"f1_{aspect}\"] = result[\"f1\"]\n        df[f\"accuracy_{aspect}\"] = result[\"accuracy\"]\n        \n        df[f\"macro_precision_{aspect}\"] = result[\"macro_precision\"]\n        df[f\"macro_recall_{aspect}\"] = result[\"macro_recall\"]\n        df[f\"macro_f1_{aspect}\"] = result[\"macro_f1\"]\n        \n        df[f\"w_precision_{aspect}\"] = result[\"w_precision\"]\n        df[f\"w_recall_{aspect}\"] = result[\"w_recall\"]\n        df[f\"w_f1_{aspect}\"] = result[\"w_f1\"]\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:46:22.855475Z","iopub.execute_input":"2024-08-21T17:46:22.855747Z","iopub.status.idle":"2024-08-21T17:46:22.865404Z","shell.execute_reply.started":"2024-08-21T17:46:22.855724Z","shell.execute_reply":"2024-08-21T17:46:22.864418Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"import openpyxl\n\nattempts = 1\nnum_aspects = 3\n\nmodels = [\"llama31\"] #[\"gemma2\",\"BART\",]\nshots = [\"X\"] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Technology\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_labels_new.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_label(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\n#results.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:46:23.475784Z","iopub.execute_input":"2024-08-21T17:46:23.476147Z","iopub.status.idle":"2024-08-21T17:46:23.513537Z","shell.execute_reply.started":"2024-08-21T17:46:23.476120Z","shell.execute_reply":"2024-08-21T17:46:23.512593Z"},"trusted":true},"execution_count":188,"outputs":[{"name":"stdout","text":"     model shots  num_queries  retrieved type_acceptance     neutral  \\\n0  llama31     X            1         20       Skeptical  No-Neutral   \n1  llama31     X            1         20       Skeptical     Neutral   \n2  llama31     X            1         20       Credulous  No-Neutral   \n3  llama31     X            1         20       Credulous     Neutral   \n\n   precision_0  recall_0   f1_0  support_0  precision_1  recall_1   f1_1  \\\n0        0.611       1.0  0.722        1.0        0.333      0.25  0.286   \n1        0.611       1.0  0.722        1.0        0.333      0.25  0.286   \n2        0.611       1.0  0.722        1.0        0.333      0.25  0.286   \n3        0.611       1.0  0.722        1.0        0.333      0.25  0.286   \n\n   support_1  \n0        2.0  \n1        2.0  \n2        2.0  \n3        2.0  \n","output_type":"stream"}]},{"cell_type":"code","source":"attempts = 1\nnum_aspects = 3\n\nmodels = [\"llama31\"]#[\"BART\",\"llama31\"] #[\"gemma2\"]\nshots = [12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Technology\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_aspects_new.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_aspects(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\n#results.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:48:31.912473Z","iopub.execute_input":"2024-08-21T17:48:31.912897Z","iopub.status.idle":"2024-08-21T17:48:31.983640Z","shell.execute_reply.started":"2024-08-21T17:48:31.912868Z","shell.execute_reply":"2024-08-21T17:48:31.982772Z"},"trusted":true},"execution_count":190,"outputs":[{"name":"stdout","text":"     model  shots  num_queries  retrieved type_acceptance     neutral  \\\n0  llama31     12            1         20       Skeptical  No-Neutral   \n1  llama31     12            1         20       Skeptical     Neutral   \n2  llama31     12            1         20       Credulous  No-Neutral   \n3  llama31     12            1         20       Credulous     Neutral   \n\n   precision_Health  recall_Health  f1_Health  accuracy_Health  ...  \\\n0               0.5            0.5        0.5              1.0  ...   \n1               0.5            0.5        0.5              1.0  ...   \n2               0.5            0.5        0.5              1.0  ...   \n3               0.5            0.5        0.5              1.0  ...   \n\n   precision_Society  recall_Society  f1_Society  accuracy_Society  \\\n0               0.75           0.875       0.762               0.8   \n1               0.75           0.875       0.762               0.8   \n2               0.75           0.875       0.762               0.8   \n3               0.75           0.875       0.762               0.8   \n\n   macro_precision_Society  macro_recall_Society  macro_f1_Society  \\\n0                     0.75                 0.875             0.762   \n1                     0.75                 0.875             0.762   \n2                     0.75                 0.875             0.762   \n3                     0.75                 0.875             0.762   \n\n   w_precision_Society  w_recall_Society  w_f1_Society  \n0                  0.9               0.8         0.819  \n1                  0.9               0.8         0.819  \n2                  0.9               0.8         0.819  \n3                  0.9               0.8         0.819  \n\n[4 rows x 36 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Ragas (application evaluation)","metadata":{}},{"cell_type":"markdown","source":"https://docs.ragas.io/en/stable/","metadata":{}},{"cell_type":"code","source":"#todo","metadata":{},"execution_count":null,"outputs":[]}]}