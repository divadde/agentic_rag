{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9212052,"sourceType":"datasetVersion","datasetId":5570293},{"sourceId":9223440,"sourceType":"datasetVersion","datasetId":5560790},{"sourceId":98659,"sourceType":"modelInstanceVersion","modelInstanceId":82777,"modelId":107076}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 ragas langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bias detection model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_entailment/Textual%20Entailment%20Explanation%20Demo.html\n- https://huggingface.co/facebook/bart-large-mnli","metadata":{}},{"cell_type":"markdown","source":"Entailment model (BART):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbart_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\",device=device)\nbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BART_prediction(premise,hypothesis):\n    #print(f\"Premise: {premise}\")\n    #print(f\"Hypo: {hypothesis}\")\n    input_ids = bart_tokenizer.encode(premise, hypothesis, return_tensors=\"pt\")\n    logits = bart_model(input_ids)[0]\n    probs = logits.softmax(dim=1)\n\n    max_index = torch.argmax(probs).item()\n\n    bart_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n    return bart_label_map[max_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nBERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nBERT_model = BertForSequenceClassification.from_pretrained('/kaggle/input/bert_hate_speech/pytorch/default/1/fine_tuned_bert')\nBERT_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BERT_hate_speech(text):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BERT_model.to(device)\n\n    encoded_dict = BERT_tokenizer.encode_plus(\n                    text,\n                    add_special_tokens = True,\n                    max_length = None, #Uso della massima lunghezza del modello, nel caso di BERT 512 tokens\n                    padding = \"max_length\",\n                    truncation = True,\n                    return_attention_mask = True,\n                    return_tensors = 'pt',\n                )\n\n    text_ids = encoded_dict['input_ids'].to(device)\n    attention_mask = encoded_dict['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = BERT_model(input_ids=text_ids, attention_mask=attention_mask)\n\n    # Estrai i logits (output non normalizzati del modello)\n    logits = outputs.logits\n\n    # Converti i logits in probabilità (se necessario)\n    probs = torch.softmax(logits, dim=1)\n\n    # Identifica la classe con la probabilità più alta\n    predicted_class = torch.argmax(probs, dim=1)\n    \n    #Label 0, no hate speech, Label 1 hate speech\n    if predicted_class==0: response = \"no\"\n    else: response = \"yes\"\n\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Web Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nweb_search_tool = TavilySearchResults(k=2)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"markdown","source":"osservazione sul chuncking: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338","metadata":{}},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\ndef create_retriever(index_name,top_k):\n    vectorstore = PineconeVectorStore(\n        index_name=index_name,\n        embedding=AI21Embeddings(device=\"cuda\")\n    )\n    return vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\ndef create_KBT_retrievers(aspects,top_k):\n    retrievers = []\n    for aspect in aspects:\n        retriever = create_retriever(f\"{aspect.lower()}-kbt\",top_k)\n        retrievers.append(retriever)\n    return retrievers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt support","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import Ollama\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\ndef create_prompt(system_prompt, input_variables, model, examples=None):\n    input_var_str = [f\"{input_var}\" for input_var in input_variables]\n    if \"llama3.1\" in model:\n        prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \"+ system_prompt\n    else:\n        prompt = system_prompt\n    \n    if examples!=None:\n        prompt = prompt + \"\\nHere some examples: \\n\"\n        for example in examples:\n            prompt = prompt + \"\\n\"\n            for key in input_var_str:\n                prompt = prompt + key + \": \" + example[key] +\"\\n\"\n            prompt = prompt + \"ouput: \" + example[\"output\"] +\"\\n\"   \n        prompt = prompt + \"\\n\"\n    \n    if \"llama3.1\" in model:\n        prompt = prompt + \"<|eot_id|><|start_header_id|>user<|end_header_id|> \\n\"\n    for key in input_var_str:\n            prompt = prompt + key + \": \" +f\"{{{key}}}\" +\"\\n\"\n    if \"llama3.1\" in model:\n        prompt = prompt + \"output:\" + \" <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n    prompt_template = PromptTemplate(template = prompt, input_variables=input_var_str)\n    \n    #Debug:\n    \"\"\"\n    dict_input = {}\n    for input_var in input_variables:\n        dict_input[f'{input_var}'] = \"USER_SUBMISSION\"\n    print(prompt_template.format(**dict_input))\n    \"\"\"\n    \n    return prompt_template","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def call_model(llm: str, prompt: str, input_variables:list[str], examples=None):\n    model = Ollama(model=llm, temperature=0)\n    prompt_final = create_prompt(prompt, input_variables, llm, examples)\n    return prompt_final | model | StrOutputParser()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prompt(path):\n    with open(path, 'r') as file:\n        prompt = file.read()\n    return prompt\n\ndef get_examples(path):\n    with open(path, 'r') as file:\n        examples = file.read()\n    return eval(examples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://learnprompting.org/docs/reliability/debiasing","metadata":{}},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\nimport sklearn.metrics\nimport numpy as np\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    original_query: str\n    query: str\n    aspect: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n        \ndef rewrite_query(state,verbose,llm,observer):\n    if verbose: \n        print(\"---REWRITING QUERY---\")\n        print(f\"State: {state}\")\n    original_query = state[\"original_query\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/query_rewriting.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/query_rewriting_examples.txt\")\n    chain = call_model(llm, prompt, [\"original_query\",\"aspect\"], examples)\n    generation = chain.invoke({\"original_query\": original_query, \"aspect\": aspect})\n    #print(f\"Aspect {aspect} query: {generation}\")\n    #print(list(generation.values())) #Debug\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"] = {\"aspect_query\": generation}\n    return {\"query\": generation}\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fairness,observer):\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n\n    # RAG generation\n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/generating_answer.txt\")\n    chain = call_model(llm, prompt, [\"context\",\"question\"])\n    documents_content = [d.page_content for d in documents]\n    generation = chain.invoke({\"context\": documents_content, \"question\": query})\n    \n    aspect_id = state[\"aspect_id\"]\n    #print(f\"Aspect agent {aspect_id} generates: {generation}\") #Debug\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"documents_for_generation\"] = documents_content\n            observer.aspects[f\"{aspect}\"][\"answer\"] = generation\n            observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = \"//\"\n    if fairness:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation], \"ord_aspects\": [state[\"aspect\"]]}\n\n\ndef confirm_answer(state,verbose):\n    if verbose:\n        print(\"---CONFIRM ANSWER---\")\n        print(f\"State: {state}\")\n    my_answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n\n    #print(f\"Answer for aspect {aspect}: {my_answer}\")\n    return {\"answers_agent\": [my_answer], \"ord_aspects\": [aspect]}\n\n\ndef grade_documents(state,verbose,llm,observer):\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/retrieval_grader.txt\")\n    chain = call_model(llm, prompt, [\"question\",\"document\"])\n\n    # Score each doc\n    filtered_docs = []\n    not_relevant_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = chain.invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if \"yes\" in score.lower():\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            not_relevant_docs.append(d.page_content)\n            web_search = \"Yes\"\n            continue\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"web_search\"] = web_search\n            observer.aspects[f\"{aspect}\"][\"not_relevants_docs\"] = not_relevant_docs\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose,observer):\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    #print(f\"docs from web: {docs}\") #Debug\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents([web_results])\n    for doc in doc_splits:\n        if documents is None:\n            documents = [doc]\n        else:\n            documents.append(doc)\n    observer.aspects[f\"{aspect}\"][\"web_docs\"] = [d.page_content for d in doc_splits]\n    return {\"documents\": documents, \"query\": query}\n\n\ndef hate_speech_filter(state,verbose,llm,encoder,observer):\n    if verbose:\n        print(\"---HATE SPEECH FILTER---\")\n        print(f\"State: {state}\")\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/hate_speech.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/hate_speech_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    hate_speech_docs = []\n    for d in documents:\n        if encoder:\n            score = BERT_hate_speech(d.page_content)\n        else:\n            score = chain.invoke(\n                {\"input\": d.page_content}\n            )\n        #grade = score[\"score\"]\n        if \"no\" in score.lower():\n            if verbose: print(\"---DOCUMENT ACCEPTED---\")\n            filtered_docs.append(d)\n        else:\n            if verbose: print(\"---DOCUMENT HATEFUL---\")\n            hate_speech_docs.append(d.page_content)\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"hate_speech_docs\"] = hate_speech_docs\n    return {\"documents\": filtered_docs}\n\n\ndef entailment_filter(state,BART_model,strategy_entailment,neutral_acceptance,verbose,observer,llm,shots):    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n    aspect_id = state[\"aspect_id\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/entailment_checker.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/entailment_checker_examples.txt\")\n    if shots>0:\n        chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"], examples[:shots])\n    else:\n        chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"])\n\n    # Score each doc\n    filtered_docs = []\n    #counter_docs = 0 # Debug\n    #La procedura si può semplificare: TODO\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"contradiction\" in score.lower():\n                    # contradiction found\n                    break\n            if (not neutral or neutral_acceptance) and score.lower() != \"contradiction\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug            \n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                # Document entailed\n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"entailment\" in score.lower():\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                    filtered_docs.append(d)\n                    break\n            if (neutral and neutral_acceptance) and score.lower() != \"entailment\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    if observer!=None:\n        #todo misura metriche con sklearn classifier\n        #0 tweet veri, #1 tweet falsi\n        y_true = [int(document.metadata.get(\"label\")) for document in documents if \"label\" in document.metadata]\n        y_pred = [0 if document in filtered_docs else 1 for document in documents if \"label\" in document.metadata]\n        #print(f\"Real docs with aspect {aspect_id} ,y_true: {y_true}\") #Debug\n        #print(f\"Filtered docs with aspect {aspect_id} ,y_pred: {y_pred}\") #Debug\n        report = sklearn.metrics.classification_report(y_true,y_pred,labels=[0,1],\n                                                       output_dict=True,zero_division=0)\n        observer.aspects[f\"{aspect}\"][\"report_entailment\"] = report\n        observer.aspects[f\"{aspect}\"][\"fake_docs\"] = [d.page_content for d in documents if d not in filtered_docs]\n        \n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose,llm,observer):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/debiasing_answer.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/debiasing_answer_examples.txt\")\n    chain = call_model(llm, prompt, [\"text\"], examples)\n    \n    unbiased_answer = chain.invoke({\"text\": answer})\n    \n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = unbiased_answer\n    \n    #print(f\"Answer for aspect {aspect}: {unbiased_answer}\")\n    \n    return {\"answers_agent\": [unbiased_answer], \"ord_aspects\": [aspect]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose,llm,bias_encoder_model):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    response = \"biased\"\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/bias_detection.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/bias_detection_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n    \n    if bias_encoder_model:\n        bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n        response = bias_detection(answer)[0]['label'].lower()\n    else:\n        response = chain.invoke({\"input\": answer})\n        if \"biased\" in response: response = \"biased\"\n        if \"non-biased\" in response: response = \"non-biased\"\n        \n    if verbose: print(response) #biased, non-biased\n    \n    return response\n\n\ndef decide_to_generate(state,verbose):\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: RELEVANT---\")\n        return \"relevant\"","metadata":{"id":"6iV1oJJtLcFM","execution":{"iopub.status.busy":"2024-08-27T14:46:52.985384Z","iopub.execute_input":"2024-08-27T14:46:52.985811Z","iopub.status.idle":"2024-08-27T14:46:53.037366Z","shell.execute_reply.started":"2024-08-27T14:46:52.985781Z","shell.execute_reply":"2024-08-27T14:46:53.036505Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Workflow condizionale\ndef workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    workflow.add_node(\"rewrite_query\", functools.partial(rewrite_query, verbose=configs.verbose, \n                                                    llm=configs.local_llm, observer=configs.observer))  # query rewriting\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, \n                                                    retriever=configs.retriever, \n                                                    retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, \n                                                    llm=configs.local_llm, \n                                                    fairness=configs.fairness, observer=configs.observer))  # generatae\n    \n    if configs.web_search:\n        workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose, \n                                                        observer=configs.observer))  # web search\n        workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, \n                                                               llm=configs.local_llm,\n                                                               observer=configs.observer))  # grade documents\n    if configs.safeness:\n        workflow.add_node(\"hate_speech_filter\", functools.partial(hate_speech_filter, verbose=configs.verbose, \n                                                                  llm=configs.local_llm, \n                                                                  encoder=configs.hate_encoder_model,\n                                                                  observer=configs.observer))\n    if configs.trustworthiness:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, BART_model=configs.BART_model, strategy_entailment=configs.strategy_entailment, \n                                                                 neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.local_llm, \n                                                                 observer=configs.observer,\n                                                                 shots=configs.shots))  # entailment\n    if configs.fairness:  \n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose, \n                                                                llm=configs.local_llm,\n                                                                observer=configs.observer))\n        workflow.add_node(\"confirm_answer\", functools.partial(confirm_answer, verbose=configs.verbose))\n\n    workflow.add_edge(START, \"rewrite_query\")\n    workflow.add_edge(\"rewrite_query\", \"retrieve\")\n    \n    if configs.web_search:\n        workflow.add_edge(\"retrieve\", \"grade_documents\")  \n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            functools.partial(decide_to_generate, verbose=configs.verbose),\n            {\n                \"websearch\": \"websearch\",\n                \"relevant\": \"hate_speech_filter\" if configs.safeness else \"entailment_filter\" if configs.trustworthiness else \"generate\"\n            },\n        )\n    \n    if configs.safeness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"hate_speech_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"hate_speech_filter\")\n        if configs.trustworthiness:\n            workflow.add_edge(\"hate_speech_filter\", \"entailment_filter\")\n            workflow.add_edge(\"entailment_filter\", \"generate\")\n        else:\n            workflow.add_edge(\"hate_speech_filter\",\"generate\")\n    elif configs.trustworthiness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"entailment_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n    else:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"generate\")\n        else: \n            workflow.add_edge(\"retrieve\", \"generate\")\n    \n    if configs.fairness:\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose, llm=configs.local_llm, bias_encoder_model=configs.bias_encoder_model),\n            {\n                \"biased\": \"debiasing_filter\",\n                \"non-biased\": \"confirm_answer\",\n            },\n        )\n        workflow.add_edge(\"confirm_answer\", END)\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"generate\", END)\n\n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","execution":{"iopub.status.busy":"2024-08-27T14:46:56.224645Z","iopub.execute_input":"2024-08-27T14:46:56.225493Z","iopub.status.idle":"2024-08-27T14:46:56.243624Z","shell.execute_reply.started":"2024-08-27T14:46:56.225463Z","shell.execute_reply":"2024-08-27T14:46:56.242735Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    final_answer: str\n\ndef send_aspects(state,verbose):\n    if verbose: \n        print(\"---SEND ASPECT TO EACH ASPECT-AGENT---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"original_query\": state[\"question\"], \"aspect\": a, \"aspect_id\": state[\"aspects\"].index(a)}) for a in state[\"aspects\"]]\n\ndef organize_answers(state,verbose,llm,organize):\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n    ord_aspects = state[\"ord_aspects\"]\n    \n    #print(f\"I'm the master agent and I received: {answers_agent}\") #Debug\n    if organize: #with sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer_with_sections.txt\")\n        chain = call_model(llm, prompt, [\"answers\",\"aspects\"])\n        final_output=chain.invoke({\"answers\": answers_agent, \"aspects\": ord_aspects})\n    else: #without sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer.txt\")\n        chain = call_model(llm, prompt, [\"answers\"])\n        final_output=chain.invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","execution":{"iopub.status.busy":"2024-08-27T14:46:58.082671Z","iopub.execute_input":"2024-08-27T14:46:58.083369Z","iopub.status.idle":"2024-08-27T14:46:58.093395Z","shell.execute_reply.started":"2024-08-27T14:46:58.083337Z","shell.execute_reply":"2024-08-27T14:46:58.092538Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"organize_answers\", functools.partial(organize_answers, verbose=configs.verbose, llm=configs.local_llm, organize=configs.organize))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_conditional_edges(START, functools.partial(send_aspects, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_answers\")\n    master_flow.add_edge(\"organize_answers\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:47:00.073056Z","iopub.execute_input":"2024-08-27T14:47:00.073411Z","iopub.status.idle":"2024-08-27T14:47:00.079920Z","shell.execute_reply.started":"2024-08-27T14:47:00.073381Z","shell.execute_reply":"2024-08-27T14:47:00.078972Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"markdown","source":"Vectorstore configuration","metadata":{}},{"cell_type":"code","source":"index_name = \"entailment-test\"\naspects = [\"Health\",\"Governmental\",\"Society\"] #\"Technology\"\n\ntop_retriever = 10 #documents retrieved by retriever\ntop_KBT = 5 #documents retrievede by KBT retriever\n\nretriever = create_retriever(index_name, top_retriever)\nretrievers_KBT = create_KBT_retrievers(aspects, top_KBT)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:30:18.142040Z","iopub.execute_input":"2024-08-27T14:30:18.142509Z","iopub.status.idle":"2024-08-27T14:30:18.196304Z","shell.execute_reply.started":"2024-08-27T14:30:18.142478Z","shell.execute_reply":"2024-08-27T14:30:18.195567Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,retriever,retrievers_KBT,aspects):\n        self.local_llm = \"llama3.1\" #\"gemma2\"\n        \n        # retrievers\n        self.retriever = retriever\n        self.retrievers_KBT = retrievers_KBT\n        self.aspects = aspects\n        \n        #if we want print all the process: True\n        self.verbose = False \n        \n        #if we want to include websearch in the workflow\n        self.web_search = True\n\n        # Controlling properties\n        self.safeness = True # if we want to add hate speech detection module\n        self.trustworthiness = True # if we want to add entailment module with KBT\n        self.fairness = True  # if we want to add debiasing module.\n\n        #Controlling entailment\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = False\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True   \n        # True: uses BART model for the entailment, False: uses LLM\n        self.BART_model = False\n        self.shots = 6 #few-shot learning per chi fa entailment\n        \n        #controlling bias detection\n        # True: uses encoder model. False: uses LLM\n        self.bias_encoder_model = False\n        \n        #controlling hate speech detection\n        # True: uses encoder model. False: uses LLM\n        self.hate_encoder_model = True\n        \n        #controlling final generation \n        # True: organize final output in section. False: organize output without sections\n        self.organize = True\n        \n        # For testing\n        self.observer = None\n    \nconfigs = Config(retriever,retrievers_KBT,aspects)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:30:20.222272Z","iopub.execute_input":"2024-08-27T14:30:20.223020Z","iopub.status.idle":"2024-08-27T14:30:20.238896Z","shell.execute_reply.started":"2024-08-27T14:30:20.222987Z","shell.execute_reply":"2024-08-27T14:30:20.237918Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Starting language model","metadata":{}},{"cell_type":"code","source":"start_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = {\"question\": \"How people stayed during covid19 pandemy?\", \"aspects\": configs.aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\nanswer = value[\"final_answer\"]\nprint(answer)","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Funzioni di supporto:\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n\n\n\"\"\"\nqueries_gpt = [\"Does the COVID-19 vaccine contain a microchip for controlling people?\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Does inhaling hot steam kill the COVID-19 virus?\",\n\"Was COVID-19 created in a lab as a biological weapon?\",\n\"Does drinking alcohol help prevent COVID-19 infection?\",\n\"Is 5G responsible for spreading COVID-19?\",\n\"Do masks cause a reduction in blood oxygen levels?\",\n\"Do COVID-19 vaccines cause infertility in women?\",\n\"Can high doses of vitamin C cure COVID-19?\",\n\"Can pets transmit COVID-19 to humans?\"]\n\"\"\"\n\n# Query più generali\nqueries_gpt = [\"Tell me about the Covid-19 pandemic.\",\n\"What measures were taken to manage the Covid-19 emergency?\",\n\"What were the repercussions of the Covid-19 pandemic?\",\n\"How did people experience the Covid-19 pandemic?\",\n\"Tell me about the misinformation spread during the Covid-19 pandemic.\",\n\"How important was the role of doctors during the covid19 pandemic?\",\n\"What origin does covid19 have?\",\n\"Effectiveness of COVID-19 vaccines\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Was COVID-19 created in a lab as a biological weapon?\"]\n\n#write_file(\"/kaggle/working/gpt_queries.json\",queries_gpt)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:23:37.049614Z","iopub.execute_input":"2024-08-27T14:23:37.050280Z","iopub.status.idle":"2024-08-27T14:23:37.057006Z","shell.execute_reply.started":"2024-08-27T14:23:37.050242Z","shell.execute_reply":"2024-08-27T14:23:37.056092Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Predisposizione dell'observer per salvare i risultati","metadata":{}},{"cell_type":"code","source":"class Observer(object):\n    def __init__(self):\n        self.query=\"\"\n        self.type_of_acceptance=\"\"\n        self.neutral_acceptance=False\n        self.aspects={}\n        self.final_answer=\"\"\n    \n    def generate_dict(self):\n        return {\"query\": self.query,\n                \"type_of_acceptance\": self.type_of_acceptance,\n                \"neutral_acceptance\": self.neutral_acceptance,\n                \"aspects\": self.aspects,\n                \"final_answer\": self.final_answer}","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:30:24.740432Z","iopub.execute_input":"2024-08-27T14:30:24.741342Z","iopub.status.idle":"2024-08-27T14:30:24.746950Z","shell.execute_reply.started":"2024-08-27T14:30:24.741307Z","shell.execute_reply":"2024-08-27T14:30:24.746048Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Combinazioni di configurazione con skeptical/credulous e neutral","metadata":{}},{"cell_type":"markdown","source":"Multiple combination for entailment:","metadata":{}},{"cell_type":"code","source":"shots = [0,3,6,12]\ncombination = [(True,True), (True,False), (False,True), (False,False)] #Skeptical-Neutral #Skeptical-No-Neutral #Cred-Neu #Cred-No-Neu\n\nfor shot in shots:\n    for comb in combination:\n        print(f\"Start combination with shots: {shot}\")\n        configs.shots = shot\n        configs.strategy_entailment =  comb[0]\n        configs.neutral_acceptance = comb[1]\n    \n        queries_list = queries_gpt\n\n        attempt = 1\n        ret_dict = {}\n        for query in queries_list:\n            inputs = {\"question\": query, \"aspects\": configs.aspects}\n            print(f\"Attempt {attempt} start\")\n    \n            configs.observer = Observer()\n            configs.observer.query=query\n            if configs.strategy_entailment:\n                configs.observer.type_of_acceptance=\"Skeptical\"\n            else:\n                configs.observer.type_of_acceptance=\"Credulous\"\n            configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n            for output in master_flow(configs).stream(inputs):\n                for key, value in output.items():\n                    pass\n                    #pprint(f\"Finished running: {key}:\")\n            answer = value[\"final_answer\"]\n    \n            configs.observer.final_answer= answer\n            ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n            attempt = attempt + 1\n\n        stringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\n        write_file(f\"/kaggle/working/llama31_{shot}_shots_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Single test for a list of query (complete test):","metadata":{}},{"cell_type":"code","source":"#queries_list = load_queries(\"/kaggle/input/preference/gpt_queries.json\")\nqueries_list = [\"Talk to me about COVID-19\",\n                \"What are the consequences of COVID-19, specifically on the aspects considered?\"] #queries_gpt\n\nattempt = 1\nret_dict = {}\nfor query in queries_list:\n    inputs = {\"question\": query, \"aspects\": configs.aspects}\n    print(f\"Attempt {attempt} start\")\n    \n    configs.observer = Observer()\n    configs.observer.query=query\n    if configs.strategy_entailment:\n        configs.observer.type_of_acceptance=\"Skeptical\"\n    else:\n        configs.observer.type_of_acceptance=\"Credulous\"\n    configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n    for output in master_flow(configs).stream(inputs):\n        for key, value in output.items():\n            pass\n            #pprint(f\"Finished running: {key}:\")\n    answer = value[\"final_answer\"]\n    \n    configs.observer.final_answer= answer\n    ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n    attempt = attempt + 1\n\nstringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\nwrite_file(f\"/kaggle/working/complete_test_llama31.json\",ret_dict)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:47:08.216894Z","iopub.execute_input":"2024-08-27T14:47:08.217604Z","iopub.status.idle":"2024-08-27T14:50:12.266906Z","shell.execute_reply.started":"2024-08-27T14:47:08.217573Z","shell.execute_reply":"2024-08-27T14:50:12.265911Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Attempt 1 start\n[GIN] 2024/08/27 - 14:47:09 | 200 |   1.20613897s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:09 | 200 |  1.244264154s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:09 | 200 |  1.460408378s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:12 | 200 |  430.103302ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:12 | 200 |  461.224551ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:12 | 200 |  578.608186ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:12 | 200 |  483.684035ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:12 | 200 |  399.586448ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:12 | 200 |  299.763414ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  448.033918ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  369.197593ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  313.113343ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  285.198708ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  320.676382ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  332.490585ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  303.748072ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  410.493289ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:13 | 200 |  346.262366ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  367.612062ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  320.576392ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  345.364466ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  320.373529ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  438.623252ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  350.969372ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:14 | 200 |  384.033718ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |   357.55582ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  329.533816ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  509.841517ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  261.399684ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  339.913548ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  327.840388ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  331.610681ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:15 | 200 |  319.135205ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:18 | 200 |  940.521275ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:19 | 200 |  313.846344ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:21 | 200 |  1.980085115s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:21 | 200 |  1.992287579s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:21 | 200 |  1.946811975s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:22 | 200 |  802.524046ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:22 | 200 |  803.741095ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:22 | 200 |  855.137044ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:22 | 200 |  839.235449ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:22 | 200 |  873.694078ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:23 | 200 |  957.774425ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:23 | 200 |  832.814641ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:24 | 200 |  1.162449147s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:24 | 200 |  1.131822659s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:24 | 200 |  882.372162ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:25 | 200 |  915.924158ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:25 | 200 |   1.13900362s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:26 | 200 |  2.141236697s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:26 | 200 |  1.840982201s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:26 | 200 |   1.12540197s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:27 | 200 |  1.339408711s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:28 | 200 |  1.341627304s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:28 | 200 |  1.382385672s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:28 | 200 |  1.277952533s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:28 | 200 |  676.262385ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:29 | 200 |  1.173747546s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:30 | 200 |  1.794700165s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:30 | 200 |  1.246408656s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:30 | 200 |  1.801375252s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:31 | 200 |   637.09334ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:32 | 200 |  1.214253991s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:32 | 200 |  1.265660718s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:32 | 200 |  1.068124471s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:33 | 200 |  1.081175236s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:35 | 200 |   1.99965734s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:35 | 200 |  3.110809988s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:36 | 200 |  1.003432163s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:36 | 200 |  1.123187358s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:37 | 200 |  930.764798ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:38 | 200 |  1.677115729s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:38 | 200 |  1.005981264s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:38 | 200 |  629.120398ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:38 | 200 |  626.881082ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:39 | 200 |  773.618192ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:39 | 200 |  927.390908ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:40 | 200 |  957.082926ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:40 | 200 |  1.076153158s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:40 | 200 |  8.150357612s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:41 | 200 |    537.0182ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:41 | 200 |  1.166686914s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:42 | 200 |  1.334496028s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:42 | 200 |  1.968023212s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:42 | 200 |  838.539205ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:43 | 200 |  815.419057ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:44 | 200 |   1.64518931s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:44 | 200 |  1.288904337s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:45 | 200 |  600.280302ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:45 | 200 |  1.027462033s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:46 | 200 |   918.03475ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:46 | 200 |  994.975476ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:47 | 200 |  983.288723ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:47 | 200 |   1.00773308s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:48 | 200 |  1.014336943s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:48 | 200 |  956.999323ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:48 | 200 |  882.918074ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:49 | 200 |  1.061083589s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:50 | 200 |  1.188776827s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:50 | 200 |  921.781135ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:51 | 200 |   904.51885ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:51 | 200 |  915.993486ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:52 | 200 |  1.140774893s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:52 | 200 |  1.209204409s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:53 | 200 |  1.194508154s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:54 | 200 |  1.141095266s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:54 | 200 |  1.095484752s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:54 | 200 | 11.795969958s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:55 | 200 |  1.206817449s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:55 | 200 |  1.268498754s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:57 | 200 |  1.903602369s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:57 | 200 |   1.29615327s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:57 | 200 |  540.627304ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:57 | 200 |  618.739481ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:58 | 200 |  407.199665ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:58 | 200 |  604.094369ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:58 | 200 |  688.973289ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:59 | 200 |  682.952818ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:59 | 200 |  649.059529ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:47:59 | 200 |  753.074199ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:00 | 200 |  1.308629938s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:00 | 200 |  936.570797ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:04 | 200 |  3.223873545s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:04 | 200 |  801.534326ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:05 | 200 |  747.835305ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:06 | 200 |    923.3221ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:07 | 200 |  871.871068ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:08 | 200 |  584.726025ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:08 | 200 |  571.009008ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:08 | 200 |  8.027823705s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:09 | 200 |  396.466145ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:10 | 200 |  1.149628859s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:10 | 200 |  986.511255ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:10 | 200 |  674.305405ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:11 | 200 |  541.762816ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:11 | 200 |  569.660099ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:12 | 200 |  477.809832ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:13 | 200 |  684.900192ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:13 | 200 |  641.498375ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:15 | 200 |  5.084861877s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:17 | 200 |  3.700920005s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:18 | 200 |  646.019848ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:20 | 200 |  2.571524902s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:28 | 200 |  7.865857063s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 2 start\n[GIN] 2024/08/27 - 14:48:29 | 200 |  972.849957ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:29 | 200 |   1.04499542s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:30 | 200 |  1.149591334s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:32 | 200 |   357.52451ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:32 | 200 |  348.678122ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:32 | 200 |  647.644265ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:32 | 200 |  512.616429ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  493.699294ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  390.208522ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  349.562202ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  499.551567ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  284.290257ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  405.767967ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  415.256585ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:33 | 200 |  382.538285ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |  238.373366ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |    425.6226ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |  344.511766ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |  388.183969ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |  222.879962ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |  345.570267ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:34 | 200 |   299.31591ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  476.385049ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  322.862663ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  353.632774ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  186.370761ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  344.743196ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  306.402888ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  376.604847ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:35 | 200 |  334.770511ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:36 | 200 |  395.638705ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:36 | 200 |  423.760219ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:36 | 200 |  392.231705ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:41 | 200 |  1.535041943s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:41 | 200 |  1.398725348s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:42 | 200 |   2.05882101s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:42 | 200 |  1.092862262s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:42 | 200 |  1.460804657s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:43 | 200 |  1.028107628s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:43 | 200 |   1.00585739s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:43 | 200 |  557.597805ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:44 | 200 |  960.697871ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:44 | 200 |  965.022007ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:44 | 200 |  1.412846225s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:44 | 200 |  800.353328ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:45 | 200 |  1.062507741s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:46 | 200 |  1.362017554s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:46 | 200 |  1.405944705s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:46 | 200 |  1.108391185s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:47 | 200 |  932.198957ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:47 | 200 |  834.694757ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:47 | 200 |  888.633533ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:47 | 200 |  739.572234ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:48 | 200 |   1.19445338s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:48 | 200 |  904.196866ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:48 | 200 |  718.860596ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:49 | 200 |  823.625479ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:49 | 200 |   1.29256748s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:49 | 200 |  1.035199847s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:49 | 200 |  715.554139ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:50 | 200 |  746.402183ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:50 | 200 |    1.0470635s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:51 | 200 |  1.042632876s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:51 | 200 |   922.36874ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:51 | 200 |  988.884677ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:52 | 200 |  1.028939048s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:52 | 200 |  1.029650145s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:52 | 200 |  702.856436ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:53 | 200 |  961.878933ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:53 | 200 |  895.337121ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:53 | 200 |  1.243855201s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:54 | 200 |  1.099167784s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:54 | 200 |  770.602537ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:54 | 200 |  1.117815798s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:55 | 200 |  1.281149952s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:55 | 200 |  1.245296735s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:55 | 200 |  882.929217ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:56 | 200 |  1.088021876s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:57 | 200 |  2.113104864s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:57 | 200 |  1.773533512s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:57 | 200 |  1.043970416s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:58 | 200 |  1.212817822s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:58 | 200 |  1.236954456s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:58 | 200 |  1.272525837s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:59 | 200 |  806.149685ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:59 | 200 |  899.338381ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:48:59 | 200 |  931.519122ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:00 | 200 |   875.57025ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:00 | 200 |  1.124858392s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:00 | 200 |   1.13425244s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:01 | 200 |  934.584546ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:02 | 200 |  1.151199222s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:02 | 200 |  1.170453646s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:03 | 200 |  1.607978042s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:03 | 200 |  1.183830969s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:03 | 200 |  1.289237101s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:04 | 200 |  1.445845824s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:04 | 200 |   1.24662002s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:04 | 200 |  1.165797615s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:05 | 200 |  999.932615ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:05 | 200 |  1.015530884s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:05 | 200 |  1.134864698s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:06 | 200 |  391.217296ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:06 | 200 |  707.361003ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:07 | 200 |   1.01724971s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:07 | 200 |  1.462367204s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:07 | 200 |  877.668639ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:07 | 200 |  610.456463ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:08 | 200 |  1.402826597s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:08 | 200 |  964.253771ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:08 | 200 |  1.402863946s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:09 | 200 |  839.640886ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:09 | 200 |  843.473484ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:10 | 200 |  2.008116072s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:10 | 200 |  1.183836223s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:10 | 200 |  1.257767601s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:11 | 200 |  1.023379397s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:11 | 200 |  993.258551ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:11 | 200 |  990.805905ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:12 | 200 |  573.645181ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:12 | 200 |  1.006753008s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:13 | 200 |   786.27005ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:13 | 200 |  1.378922571s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:13 | 200 |  634.965081ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:14 | 200 |  1.141010179s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:14 | 200 |  923.582062ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:14 | 200 |  1.309499619s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:14 | 200 |  509.379064ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:15 | 200 |  1.030520545s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:16 | 200 |  1.326595365s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:16 | 200 |  1.687864169s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:16 | 200 |  908.555635ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:17 | 200 |  1.275714262s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:17 | 200 |  1.324451998s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:17 | 200 |  1.140318791s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:18 | 200 |  1.308538501s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:18 | 200 |  1.313736191s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:19 | 200 |  1.464628756s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:19 | 200 |  1.014047265s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:20 | 200 |  1.277912537s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:20 | 200 |  674.192606ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:20 | 200 |  1.632824882s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:21 | 200 |    959.5328ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:22 | 200 |  1.357448316s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:22 | 200 |  1.350878073s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:22 | 200 |   1.35120724s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:22 | 200 |  782.665448ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:24 | 200 |  1.504797173s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:24 | 200 |  1.325902125s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:24 | 200 |  2.089160584s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:24 | 200 |  320.303481ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:24 | 200 |  548.132882ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:25 | 200 |  1.040423455s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:25 | 200 |   1.32825231s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:26 | 200 |  1.380852443s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:26 | 200 |  815.511128ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:27 | 200 |   1.86201104s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:27 | 200 |  1.348899973s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:27 | 200 |  1.613666647s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:28 | 200 |  875.812027ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:28 | 200 |  871.570393ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:29 | 200 |  1.438707178s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:30 | 200 |  2.493946172s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:30 | 200 |  2.193966722s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:30 | 200 |  1.090572986s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:31 | 200 |  1.113508904s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:32 | 200 |  1.889169052s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:32 | 200 |  1.885038568s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:32 | 200 |  1.330533584s |       127.0.0.1 | POST     \"/api/generate\"\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1148 n_keep=4 n_left=2044 n_shift=1022 tid=\"134174047027200\" timestamp=1724770173\n[GIN] 2024/08/27 - 14:49:35 | 200 |  2.528508482s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:35 | 200 |  2.683840795s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:37 | 200 |  2.241449288s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:38 | 200 |  828.914005ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:39 | 200 |  701.182769ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:40 | 200 |  955.979112ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:41 | 200 |  769.133509ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:41 | 200 |  748.771726ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:45 | 200 | 10.071116173s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:46 | 200 |  1.019308328s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:49 | 200 |  7.181221824s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:49 | 200 |  819.365113ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:52 | 200 |  6.127468474s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:53 | 200 |  3.697498114s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:53 | 200 | 21.010023326s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:49:54 | 200 |  869.155915ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:50:00 | 200 |  6.049746765s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/27 - 14:50:12 | 200 | 11.392550828s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Entailment evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione su notizie vere (0) e notizie false (1)","metadata":{}},{"cell_type":"code","source":"# Label 0, notizie vere\n# Label 1, notizie false\n\ndef results_on_label(label, num_attempts, dict_input, aspects):\n    recall = 0\n    precision = 0\n    support = 0\n    valid_attempts = 0\n    recalls = []\n    precisions = []\n\n    for i in range(1,num_attempts+1):\n        for aspect in aspects:\n            recall = recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"]\n            precision = precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"]\n            support_dict = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"support\"]\n            # Il tentativo è valido solo se c'è almeno un documento recuperato!\n            if support_dict != 0:\n                recalls.append(dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"])\n                precisions.append(dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"])\n                valid_attempts = valid_attempts + 1\n                support = support + support_dict\n                \n    e_recall = recall/valid_attempts\n    e_precision = precision/valid_attempts\n    f1 = 2*e_recall*e_precision/(e_recall+e_precision)\n    diff_rec = [(rec - e_recall)**2 for rec in recalls]\n    diff_prec = [(prec - e_precision)**2 for prec in precisions]\n    var_recall = sum(diff_rec)/valid_attempts\n    var_precision =  sum(diff_prec)/valid_attempts\n    return {\"recall\":e_recall, \n            \"var_recall\": var_recall,\n            \"precision\":e_precision,\n            \"var_precision\": var_precision,\n            \"f1\":f1, \n            \"support\":support/(num_attempts*len(aspects))}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione per aspetto","metadata":{}},{"cell_type":"code","source":"def results_on_aspects(aspect, num_attempts, dict_input):\n    recall_0 = 0\n    precision_0 = 0\n    recall_1 = 0\n    precision_1 = 0\n    \n    accuracy = 0\n    \n    weight_recall = 0\n    weight_precision = 0\n    \n    valid_attempts_0 = 0\n    valid_attempts_1 = 0\n\n    for i in range(1,num_attempts+1):\n        recall_0 = recall_0 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"recall\"]\n        precision_0 = precision_0 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"precision\"]\n        support_dict_0 = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"support\"]\n        if support_dict_0 != 0:\n            valid_attempts_0 = valid_attempts_0 + 1\n        \n        recall_1 = recall_1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"recall\"]\n        precision_1 = precision_1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"precision\"]\n        support_dict_1 = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"support\"]\n        if support_dict_1 != 0:\n            valid_attempts_1 = valid_attempts_1 + 1\n            \n        weight_recall = weight_recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"recall\"]\n        weight_precision = weight_precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"precision\"]\n                \n        try:\n            accuracy = accuracy + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"accuracy\"]\n        except:\n            accuracy = accuracy + 1 #se l'accuracy non è presente nel dizionario, è perché vale 1 (vedi esempio 4 di llama_second_skeptical_neutral)\n    \n    e_recall_0 = recall_0/valid_attempts_0\n    e_precision_0 = precision_0/valid_attempts_0\n    \n    e_recall_1 = recall_1/valid_attempts_1\n    e_precision_1 = precision_1/valid_attempts_1\n    \n    macro_recall = (e_recall_0+e_recall_1)/2\n    macro_precision = (e_precision_0+e_precision_1)/2\n    f1 = 2*macro_recall*macro_precision/(macro_recall+macro_precision)\n    \n    w_recall = weight_recall/(num_attempts)\n    w_precision = weight_precision/(num_attempts)\n    weight_f1 = 2*w_recall*w_precision/(w_recall+w_precision)\n    return {\"macro_recall\": macro_recall,\n            \"macro_precision\": macro_precision, \n            \"macro_f1\": f1,\n            \"macro_accuracy\":accuracy/(num_attempts),\n            \"w_recall\": w_recall,\n            \"w_precision\": w_precision, \n            \"w_f1\":weight_f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label 0: notizie vere (negative class)\n# label 1: notizie false (positive class)\nimport pandas as pd\n\ndef compute_result_on_label(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        dict_input = load_json(f\"/kaggle/working/{model}_{shot}_shots_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_gov_10kbt_{acceptance}_{neutral}.json\")\n\n    results_label_0 = results_on_label(\"0\", num_queries, dict_input, aspects)\n    results_label_1 = results_on_label(\"1\", num_queries, dict_input, aspects)\n    \n\n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n\n    df[\"precision_0\"] = results_label_0[\"precision\"]\n    df[\"var_prec_0\"] = results_label_0[\"var_precision\"]\n    df[\"recall_0\"] = results_label_0[\"recall\"]\n    df[\"var_recall_0\"] = results_label_0[\"var_recall\"]\n    df[\"f1_0\"] = results_label_0[\"f1\"]\n    df[\"support_0\"] = results_label_0[\"support\"]\n    \n\n    df[\"precision_1\"] = results_label_1[\"precision\"]\n    df[\"var_prec_1\"] = results_label_1[\"var_precision\"]\n    df[\"recall_1\"] = results_label_1[\"recall\"]\n    df[\"var_recall_1\"] = results_label_1[\"var_recall\"]\n    df[\"f1_1\"] = results_label_1[\"f1\"]\n    df[\"support_1\"] = results_label_1[\"support\"]\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_result_on_aspects(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        dict_input = load_json(f\"/kaggle/working/{model}_{shot}_shots_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_gov_{acceptance}_{neutral}.json\")\n        \n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n    \n    for aspect in aspects:\n        result = results_on_aspects(aspect, num_queries, dict_input)\n        df[f\"precision_{aspect}\"] = result[\"macro_precision\"]\n        df[f\"recall_{aspect}\"] = result[\"macro_recall\"]\n        df[f\"f1_{aspect}\"] = result[\"macro_f1\"]\n        df[f\"accuracy_{aspect}\"] = result[\"macro_accuracy\"]\n        \n        df[f\"w_recall_{aspect}\"]= result[\"w_recall\"]\n        df[f\"w_precision_{aspect}\"]= result[\"w_precision\"]\n        df[f\"w_f1_{aspect}\"]= result[\"w_f1\"]\n        \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import openpyxl\n\nattempts = 10\nnum_aspects = 3\n\nmodels = [\"llama31\"] #[\"gemma2\",\"BART\",]\nshots = [0,3,6,12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Governmental\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_labels_X.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_label(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\nresults.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attempts = 10\nnum_aspects = 3\n\nmodels = [\"llama31\"]#[\"BART\",\"llama31\"] #[\"gemma2\"]\nshots = [0,3,6,12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Governmental\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_aspects_X.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_aspects(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\nresults.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ragas (application evaluation)","metadata":{}},{"cell_type":"markdown","source":"https://docs.ragas.io/en/stable/","metadata":{}},{"cell_type":"code","source":"#todo","metadata":{},"execution_count":null,"outputs":[]}]}