{"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z17cxUk666E_","gJ5m7640AD8x","kNT1n4npE6YP","MFgY6-seHsdQ","0Ot0mK4hISuW","Vky-iraRI7V4","b4jwj6nZJBsA"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9212052,"sourceType":"datasetVersion","datasetId":5570293},{"sourceId":9223440,"sourceType":"datasetVersion","datasetId":5560790},{"sourceId":98659,"sourceType":"modelInstanceVersion","modelInstanceId":82777,"modelId":107076}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and preparation","metadata":{"id":"Z17cxUk666E_"}},{"cell_type":"markdown","source":"refs:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb?ref=blog.langchain.dev\n- https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb?ref=blog.langchain.dev","metadata":{"id":"FDViz0cXZuzh"}},{"cell_type":"markdown","source":"MAP:REDUCE: https://langchain-ai.github.io/langgraph/how-tos/map-reduce/","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"id":"oXVSxKB973-R","outputId":"d7318967-09ea-4dc4-86f7-7070c2864c4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"id":"7KINjEkC8vZl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"id":"ow0d0MMn6--U","outputId":"901abc3b-e73f-49fd-f111-dbd30102442c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 ragas langchain-pinecone langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] langchain-text-splitters","metadata":{"id":"X0zaM9fk9U1O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tracing and api-keys\nimport os\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"PINECONE_API_KEY\"] = \"94ef7896-1fae-44d3-b8d2-0bd6f5f664f5\"\nos.environ[\"AI21_API_KEY\"] = \"KlINkh5QKw3hG1b5Hr75YDO7TwGoQvzn\"","metadata":{"id":"Dy6H1-68Bv8a","outputId":"892b2ceb-1d2f-4a93-95cc-539ab8507e00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bias detection model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbias_model_tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\nbias_model = AutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\",from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_entailment/Textual%20Entailment%20Explanation%20Demo.html\n- https://huggingface.co/facebook/bart-large-mnli","metadata":{}},{"cell_type":"markdown","source":"Entailment model (BART):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nbart_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\",device=device)\nbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BART_prediction(premise,hypothesis):\n    #print(f\"Premise: {premise}\")\n    #print(f\"Hypo: {hypothesis}\")\n    input_ids = bart_tokenizer.encode(premise, hypothesis, return_tensors=\"pt\")\n    logits = bart_model(input_ids)[0]\n    probs = logits.softmax(dim=1)\n\n    max_index = torch.argmax(probs).item()\n\n    bart_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n    return bart_label_map[max_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nBERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nBERT_model = BertForSequenceClassification.from_pretrained('/kaggle/input/bert_hate_speech/pytorch/default/1/fine_tuned_bert')\nBERT_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BERT_hate_speech(text):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BERT_model.to(device)\n\n    encoded_dict = BERT_tokenizer.encode_plus(\n                    text,\n                    add_special_tokens = True,\n                    max_length = None, #Uso della massima lunghezza del modello, nel caso di BERT 512 tokens\n                    padding = \"max_length\",\n                    truncation = True,\n                    return_attention_mask = True,\n                    return_tensors = 'pt',\n                )\n\n    text_ids = encoded_dict['input_ids'].to(device)\n    attention_mask = encoded_dict['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = BERT_model(input_ids=text_ids, attention_mask=attention_mask)\n\n    # Estrai i logits (output non normalizzati del modello)\n    logits = outputs.logits\n\n    # Converti i logits in probabilità (se necessario)\n    probs = torch.softmax(logits, dim=1)\n\n    # Identifica la classe con la probabilità più alta\n    predicted_class = torch.argmax(probs, dim=1)\n    \n    #Label 0, no hate speech, Label 1 hate speech\n    if predicted_class==0: response = \"no\"\n    else: response = \"yes\"\n\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tools","metadata":{"id":"rEoV2aHBSVc5"}},{"cell_type":"markdown","source":"refs:\n- https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/","metadata":{}},{"cell_type":"code","source":"### Web Search\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nweb_search_tool = TavilySearchResults(k=2)","metadata":{"id":"37TQnYfuSYjJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Indexing","metadata":{"id":"gJ5m7640AD8x"}},{"cell_type":"markdown","source":"Organizing external sources for the llm. Phase of indexing and chunking of docs refs:\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n- Nomic embeddings: https://docs.nomic.ai/atlas/capabilities/embeddings#selecting-a-device","metadata":{"id":"taJSlGE5AJcH"}},{"cell_type":"markdown","source":"osservazione: si possono controllare gli indici direttamente da https://app.pinecone.io/organizations/-O2Tiw_0VD7HTOASPJE5/projects/2a95c518-e514-4d39-bed8-4b12fd90ad44/indexes","metadata":{}},{"cell_type":"markdown","source":"osservazione sul chuncking: https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338","metadata":{}},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore\nfrom langchain_ai21 import AI21Embeddings\n\ndef create_retriever(index_name,top_k):\n    vectorstore = PineconeVectorStore(\n        index_name=index_name,\n        embedding=AI21Embeddings(device=\"cuda\")\n    )\n    return vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\ndef create_KBT_retrievers(aspects,top_k):\n    retrievers = []\n    for aspect in aspects:\n        retriever = create_retriever(f\"{aspect.lower()}-kbt\",top_k)\n        retrievers.append(retriever)\n    return retrievers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt support","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import Ollama\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\ndef create_prompt(system_prompt, input_variables, model, examples=None):\n    input_var_str = [f\"{input_var}\" for input_var in input_variables]\n    if \"llama3.1\" in model:\n        prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \"+ system_prompt\n    else:\n        prompt = system_prompt\n    \n    if examples!=None:\n        prompt = prompt + \"\\nHere some examples: \\n\"\n        for example in examples:\n            prompt = prompt + \"\\n\"\n            for key in input_var_str:\n                prompt = prompt + key + \": \" + example[key] +\"\\n\"\n            prompt = prompt + \"ouput: \" + example[\"output\"] +\"\\n\"   \n        prompt = prompt + \"\\n\"\n    \n    if \"llama3.1\" in model:\n        prompt = prompt + \"<|eot_id|><|start_header_id|>user<|end_header_id|> \\n\"\n    for key in input_var_str:\n            prompt = prompt + key + \": \" +f\"{{{key}}}\" +\"\\n\"\n    if \"llama3.1\" in model:\n        prompt = prompt + \"output:\" + \" <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n    prompt_template = PromptTemplate(template = prompt, input_variables=input_var_str)\n    \n    #Debug:\n    \"\"\"\n    dict_input = {}\n    for input_var in input_variables:\n        dict_input[f'{input_var}'] = \"USER_SUBMISSION\"\n    print(prompt_template.format(**dict_input))\n    \"\"\"\n    \n    return prompt_template","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def call_model(llm: str, prompt: str, input_variables:list[str], examples=None):\n    model = Ollama(model=llm, temperature=0)\n    prompt_final = create_prompt(prompt, input_variables, llm, examples)\n    return prompt_final | model | StrOutputParser()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prompt(path):\n    with open(path, 'r') as file:\n        prompt = file.read()\n    return prompt\n\ndef get_examples(path):\n    with open(path, 'r') as file:\n        examples = file.read()\n    return eval(examples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://learnprompting.org/docs/reliability/debiasing","metadata":{}},{"cell_type":"markdown","source":"# Aspect agents","metadata":{"id":"50do0z1CLXci"}},{"cell_type":"markdown","source":"refs\n- https://www.langchain.com/langgraph","metadata":{"id":"iSZInKfcMOUn"}},{"cell_type":"code","source":"from pprint import pprint\nfrom typing import List, Annotated\nimport operator\nimport functools\nimport sklearn.metrics\nimport numpy as np\n\nfrom langchain_core.documents import Document\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import END, StateGraph, START\n\n### State\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of graph of aspect agents.\n    \"\"\"\n    \n    original_query: str\n    query: str\n    aspect: str\n    aspect_id: int\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    my_answer: str\n    web_search: str\n    documents: List[str]\n    documents_kbt: List[str]\n        \ndef rewrite_query(state,verbose,llm,observer):\n    if verbose: \n        print(\"---REWRITING QUERY---\")\n        print(f\"State: {state}\")\n    original_query = state[\"original_query\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/query_rewriting.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/query_rewriting_examples.txt\")\n    chain = call_model(llm, prompt, [\"original_query\",\"aspect\"], examples)\n    generation = chain.invoke({\"original_query\": original_query, \"aspect\": aspect})\n    #print(f\"Aspect {aspect} query: {generation}\")\n    #print(list(generation.values())) #Debug\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"] = {\"aspect_query\": generation}\n    return {\"query\": generation}\n\n\ndef retrieve(state,verbose,retriever,retrievers_KBT):\n    if verbose: \n        print(\"---RETRIEVE---\")\n        print(f\"State: {state}\")\n        \n    query = state[\"query\"]\n    aspect_id = state[\"aspect_id\"]\n\n    # Retrieval\n    documents = retriever.invoke(query)\n    documents_kbt = retrievers_KBT[aspect_id].invoke(query)\n    \n    #pprint(f\"Documents retrieved: {documents}\")\n    #pprint(f\"Documents KBT retrieved: {documents_kbt}\")\n    \n    return {\"documents\": documents, \"documents_kbt\": documents_kbt, \"query\": query}\n\n\ndef generate(state,verbose,llm,fairness,observer):\n    if verbose:\n        print(\"---GENERATE---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n\n    # RAG generation\n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/generating_answer.txt\")\n    chain = call_model(llm, prompt, [\"context\",\"question\"])\n    documents_content = [d.page_content for d in documents]\n    generation = chain.invoke({\"context\": documents_content, \"question\": query})\n    \n    aspect_id = state[\"aspect_id\"]\n    #print(f\"Aspect agent {aspect_id} generates: {generation}\") #Debug\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"documents_for_generation\"] = documents_content\n            observer.aspects[f\"{aspect}\"][\"answer\"] = generation\n            observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = \"//\"\n    if fairness:\n        return {\"documents\": documents, \"query\": query, \"my_answer\": generation}\n    return {\"documents\": documents, \"query\": query, \"my_answer\": generation, \"answers_agent\": [generation], \"ord_aspects\": [state[\"aspect\"]]}\n\n\ndef confirm_answer(state,verbose):\n    if verbose:\n        print(\"---CONFIRM ANSWER---\")\n        print(f\"State: {state}\")\n    my_answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n\n    #print(f\"Answer for aspect {aspect}: {my_answer}\")\n    return {\"answers_agent\": [my_answer], \"ord_aspects\": [aspect]}\n\n\ndef grade_documents(state,verbose,llm,observer):\n    if verbose:\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/retrieval_grader.txt\")\n    chain = call_model(llm, prompt, [\"question\",\"document\"])\n\n    # Score each doc\n    filtered_docs = []\n    not_relevant_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = chain.invoke(\n            {\"question\": query, \"document\": d.page_content}\n        )\n        #grade = score[\"score\"]\n        # Document relevant\n        if \"yes\" in score.lower():\n            if verbose: print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            if verbose: print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            not_relevant_docs.append(d.page_content)\n            web_search = \"Yes\"\n            continue\n    if observer!=None:\n            observer.aspects[f\"{aspect}\"][\"web_search\"] = web_search\n            observer.aspects[f\"{aspect}\"][\"not_relevants_docs\"] = not_relevant_docs\n    return {\"documents\": filtered_docs, \"query\": query, \"web_search\": web_search}\n\n\ndef web_search(state,verbose):\n    if verbose:\n        print(\"---WEB SEARCH---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": query})\n    #print(f\"docs from web: {docs}\") #Debug\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents([web_results])\n    for doc in doc_splits:\n        if documents is None:\n            documents = [doc]\n        else:\n            documents.append(doc)\n    return {\"documents\": documents, \"query\": query}\n\n\ndef hate_speech_filter(state,verbose,llm,encoder,observer):\n    if verbose:\n        print(\"---HATE SPEECH FILTER---\")\n        print(f\"State: {state}\")\n    documents = state[\"documents\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/hate_speech.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/hate_speech_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n\n    # Score each doc\n    filtered_docs = []\n    hate_speech_docs = []\n    for d in documents:\n        if encoder:\n            score = BERT_hate_speech(d.page_content)\n        else:\n            score = chain.invoke(\n                {\"input\": d.page_content}\n            )\n        #grade = score[\"score\"]\n        if \"no\" in score.lower():\n            if verbose: print(\"---DOCUMENT ACCEPTED---\")\n            filtered_docs.append(d)\n        else:\n            if verbose: print(\"---DOCUMENT HATEFUL---\")\n            hate_speech_docs.append(d.page_content)\n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"hate_speech_docs\"] = hate_speech_docs\n    return {\"documents\": filtered_docs}\n\n\ndef entailment_filter(state,BART_model,strategy_entailment,neutral_acceptance,verbose,observer,llm,shots):    \n    if verbose:\n        print(\"---ENTAILMENT FILTER---\")\n        print(f\"State: {state}\")\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    documents_KBT = state[\"documents_kbt\"]\n    aspect_id = state[\"aspect_id\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/entailment_checker.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/entailment_checker_examples.txt\")\n    if shots>0:\n        chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"], examples[:shots])\n    else:\n        chain = call_model(llm, prompt, [\"premise\",\"hypothesis\"])\n\n    # Score each doc\n    filtered_docs = []\n    #counter_docs = 0 # Debug\n    #La procedura si può semplificare: TODO\n    if strategy_entailment: #Skeptical\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"contradiction\" in score.lower():\n                    # contradiction found\n                    break\n            if (not neutral or neutral_acceptance) and score.lower() != \"contradiction\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug            \n                if verbose: print(\"---DOCUMENT ENTAILED---\")   \n    else: #Credolous\n        for d in documents:\n            #counter_docs = counter_docs + 1 #Debug\n            neutral = True\n            for d_kbt in documents_KBT:\n                if BART_model:\n                    score = BART_prediction(d_kbt.page_content,d.page_content)\n                else:\n                    score = chain.invoke(\n                        {\"premise\": d_kbt.page_content, \"hypothesis\": d.page_content}\n                    )\n                #grade = score[\"score\"]\n                \n                #if score.lower() == \"neutral\": print(f\"---Neutral {counter_docs}---\") #Debug\n                #if score.lower() == \"entailment\": print(f\"---Entailment {counter_docs}---\") #Debug\n                #if score.lower() == \"contradiction\": print(f\"---Contradiction {counter_docs}---\") #Debug\n                \n                # Document entailed\n                if not \"neutral\" in score.lower():\n                    neutral = False\n                if \"entailment\" in score.lower():\n                    if verbose: print(\"---DOCUMENT ENTAILED---\")\n                    #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                    filtered_docs.append(d)\n                    break\n            if (neutral and neutral_acceptance) and score.lower() != \"entailment\":\n                filtered_docs.append(d)\n                #print(f\"---Document accepted {counter_docs}---\")  #Debug\n                if verbose: print(\"---DOCUMENT ENTAILED---\")\n    \n    if observer!=None:\n        #todo misura metriche con sklearn classifier\n        #0 tweet veri, #1 tweet falsi\n        y_true = [int(document.metadata.get(\"label\")) for document in documents if \"label\" in document.metadata]\n        y_pred = [0 if document in filtered_docs else 1 for document in documents if \"label\" in document.metadata]\n        #print(f\"Real docs with aspect {aspect_id} ,y_true: {y_true}\") #Debug\n        #print(f\"Filtered docs with aspect {aspect_id} ,y_pred: {y_pred}\") #Debug\n        report = sklearn.metrics.classification_report(y_true,y_pred,labels=[0,1],\n                                                       output_dict=True,zero_division=0)\n        observer.aspects[f\"{aspect}\"][\"report_entailment\"] = report\n        \n    return {\"documents\": filtered_docs}\n\n\ndef debiasing(state,verbose,llm,observer):\n    if verbose:\n        print(\"---DEBIASING FILTER---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    aspect = state[\"aspect\"]\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/debiasing_answer.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/debiasing_answer_examples.txt\")\n    chain = call_model(llm, prompt, [\"text\"], examples)\n    \n    unbiased_answer = chain.invoke({\"text\": answer})\n    \n    if observer!=None:\n        observer.aspects[f\"{aspect}\"][\"debiased_answer\"] = unbiased_answer\n    \n    #print(f\"Answer for aspect {aspect}: {unbiased_answer}\")\n    \n    return {\"answers_agent\": [unbiased_answer], \"ord_aspects\": [aspect]}\n\n\n### Conditional edge\n\ndef bias_detection(state,verbose,llm,bias_encoder_model):\n    if verbose:\n        print(\"---BIAS DETECTION---\")\n        print(f\"State: {state}\")\n        \n    answer = state[\"my_answer\"]\n    response = \"biased\"\n    \n    prompt = get_prompt(\"/kaggle/input/prompts/prompts/bias_detection.txt\")\n    examples = get_examples(\"/kaggle/input/prompts/prompts/bias_detection_examples.txt\")\n    chain = call_model(llm, prompt, [\"input\"], examples)\n    \n    if bias_encoder_model:\n        bias_detection = pipeline('text-classification', model=bias_model, tokenizer=bias_model_tokenizer, device=device) # cuda = 0,1 based on gpu availability\n        response = bias_detection(answer)[0]['label'].lower()\n    else:\n        response = chain.invoke({\"input\": answer})\n        if \"biased\" in response: response = \"biased\"\n        if \"non-biased\" in response: response = \"non-biased\"\n        \n    if verbose: print(response) #biased, non-biased\n    \n    return response\n\n\ndef decide_to_generate(state,verbose):\n    if verbose:\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        print(f\"State: {state}\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        if verbose: print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n            )\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        if verbose: print(\"---DECISION: RELEVANT---\")\n        return \"relevant\"","metadata":{"id":"6iV1oJJtLcFM","execution":{"iopub.status.busy":"2024-08-22T18:41:30.448723Z","iopub.execute_input":"2024-08-22T18:41:30.449087Z","iopub.status.idle":"2024-08-22T18:41:30.501743Z","shell.execute_reply.started":"2024-08-22T18:41:30.449056Z","shell.execute_reply":"2024-08-22T18:41:30.500726Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"markdown","source":"**Building graph with edges**","metadata":{"id":"jdiZVS8-3pAC"}},{"cell_type":"code","source":"# Workflow condizionale\ndef workflow_aspect_agent(configs):\n    # Build graph\n    workflow = StateGraph(GraphState)\n\n    workflow.add_node(\"rewrite_query\", functools.partial(rewrite_query, verbose=configs.verbose, \n                                                    llm=configs.local_llm, observer=configs.observer))  # query rewriting\n    workflow.add_node(\"retrieve\", functools.partial(retrieve, verbose=configs.verbose, \n                                                    retriever=configs.retriever, \n                                                    retrievers_KBT=configs.retrievers_KBT))  # retrieve\n    workflow.add_node(\"generate\", functools.partial(generate, verbose=configs.verbose, \n                                                    llm=configs.local_llm, \n                                                    fairness=configs.fairness, observer=configs.observer))  # generatae\n    \n    if configs.web_search:\n        workflow.add_node(\"websearch\", functools.partial(web_search, verbose=configs.verbose))  # web search\n        workflow.add_node(\"grade_documents\", functools.partial(grade_documents, verbose=configs.verbose, \n                                                               llm=configs.local_llm,\n                                                               observer=configs.observer))  # grade documents\n    if configs.safeness:\n        workflow.add_node(\"hate_speech_filter\", functools.partial(hate_speech_filter, verbose=configs.verbose, \n                                                                  llm=configs.local_llm, \n                                                                  encoder=configs.hate_encoder_model,\n                                                                  observer=configs.observer))\n    if configs.trustworthiness:\n        workflow.add_node(\"entailment_filter\", functools.partial(entailment_filter, BART_model=configs.BART_model, strategy_entailment=configs.strategy_entailment, \n                                                                 neutral_acceptance=configs.neutral_acceptance, verbose=configs.verbose, llm=configs.local_llm, \n                                                                 observer=configs.observer,\n                                                                 shots=configs.shots))  # entailment\n    if configs.fairness:  \n        workflow.add_node(\"debiasing_filter\", functools.partial(debiasing, verbose=configs.verbose, \n                                                                llm=configs.local_llm,\n                                                                observer=configs.observer))\n        workflow.add_node(\"confirm_answer\", functools.partial(confirm_answer, verbose=configs.verbose))\n\n    workflow.add_edge(START, \"rewrite_query\")\n    workflow.add_edge(\"rewrite_query\", \"retrieve\")\n    \n    if configs.web_search:\n        workflow.add_edge(\"retrieve\", \"grade_documents\")  \n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            functools.partial(decide_to_generate, verbose=configs.verbose),\n            {\n                \"websearch\": \"websearch\",\n                \"relevant\": \"hate_speech_filter\" if configs.safeness else \"entailment_filter\" if configs.trustworthiness else \"generate\"\n            },\n        )\n    \n    if configs.safeness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"hate_speech_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"hate_speech_filter\")\n        if configs.trustworthiness:\n            workflow.add_edge(\"hate_speech_filter\", \"entailment_filter\")\n            workflow.add_edge(\"entailment_filter\", \"generate\")\n        else:\n            workflow.add_edge(\"hate_speech_filter\",\"generate\")\n    elif configs.trustworthiness:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"entailment_filter\")\n        else: \n            workflow.add_edge(\"retrieve\", \"entailment_filter\")\n        workflow.add_edge(\"entailment_filter\", \"generate\")\n    else:\n        if configs.web_search:\n            workflow.add_edge(\"websearch\", \"generate\")\n        else: \n            workflow.add_edge(\"retrieve\", \"generate\")\n    \n    if configs.fairness:\n        workflow.add_conditional_edges(\n            \"generate\",\n            functools.partial(bias_detection, verbose=configs.verbose, llm=configs.local_llm, bias_encoder_model=configs.bias_encoder_model),\n            {\n                \"biased\": \"debiasing_filter\",\n                \"non-biased\": \"confirm_answer\",\n            },\n        )\n        workflow.add_edge(\"confirm_answer\", END)\n        workflow.add_edge(\"debiasing_filter\", END)\n    else:\n        workflow.add_edge(\"generate\", END)\n\n    workflow_compiled = workflow.compile()\n    return workflow_compiled","metadata":{"id":"PL0NHOMy3oQs","execution":{"iopub.status.busy":"2024-08-22T18:41:37.413542Z","iopub.execute_input":"2024-08-22T18:41:37.413913Z","iopub.status.idle":"2024-08-22T18:41:37.432123Z","shell.execute_reply.started":"2024-08-22T18:41:37.413884Z","shell.execute_reply":"2024-08-22T18:41:37.431202Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow_aspect_agent(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"zoVRqqCwPzCK","outputId":"b7f34450-3301-45ac-d3b2-e76d9cd14e02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Master agent","metadata":{"id":"tsNAEoSTCps4"}},{"cell_type":"code","source":"from typing import Annotated\nimport operator\nfrom langgraph.constants import Send\n\n\n### Super Graph State\nclass SuperGraphState(TypedDict):\n    \"\"\"\n    Represents the state of our super-graph.\n    \"\"\"\n    \n    question: str\n    aspects: List[str]\n    answers_agent: Annotated[List[str], operator.add]\n    ord_aspects: Annotated[List[str], operator.add]\n    final_answer: str\n\ndef send_aspects(state,verbose):\n    if verbose: \n        print(\"---SEND ASPECT TO EACH ASPECT-AGENT---\")\n        print(f\"State: {state}\")\n    return [Send(\"aspect_agent_node\", {\"original_query\": state[\"question\"], \"aspect\": a, \"aspect_id\": state[\"aspects\"].index(a)}) for a in state[\"aspects\"]]\n\ndef organize_answers(state,verbose,llm,organize):\n    if verbose:\n        print(\"---ORGANIZE OUTPUTS---\")\n        print(f\"State: {state}\")\n    answers_agent = state[\"answers_agent\"]\n    ord_aspects = state[\"ord_aspects\"]\n    \n    #print(f\"I'm the master agent and I received: {answers_agent}\") #Debug\n    if organize: #with sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer_with_sections.txt\")\n        chain = call_model(llm, prompt, [\"answers\",\"aspects\"])\n        final_output=chain.invoke({\"answers\": answers_agent, \"aspects\": ord_aspects})\n    else: #without sections\n        prompt = get_prompt(\"/kaggle/input/prompts/prompts/final_answer.txt\")\n        chain = call_model(llm, prompt, [\"answers\"])\n        final_output=chain.invoke({\"answers\": answers_agent})\n    return {\"final_answer\": final_output}","metadata":{"id":"TxQR9nC5FEUw","execution":{"iopub.status.busy":"2024-08-22T18:41:39.542282Z","iopub.execute_input":"2024-08-22T18:41:39.543342Z","iopub.status.idle":"2024-08-22T18:41:39.553710Z","shell.execute_reply.started":"2024-08-22T18:41:39.543299Z","shell.execute_reply":"2024-08-22T18:41:39.552772Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"def master_flow(configs):\n    master_flow = StateGraph(SuperGraphState)\n\n    # Define the nodes\n    master_flow.add_node(\"organize_answers\", functools.partial(organize_answers, verbose=configs.verbose, llm=configs.local_llm, organize=configs.organize))\n    master_flow.add_node(\"aspect_agent_node\",workflow_aspect_agent(configs))\n\n    # Build graph\n    master_flow.add_conditional_edges(START, functools.partial(send_aspects, verbose=configs.verbose), [\"aspect_agent_node\"])\n    master_flow.add_edge(\"aspect_agent_node\", \"organize_answers\")\n    master_flow.add_edge(\"organize_answers\", END)\n\n    master_compiled = master_flow.compile()\n    return master_compiled","metadata":{"execution":{"iopub.status.busy":"2024-08-22T18:41:41.101679Z","iopub.execute_input":"2024-08-22T18:41:41.102040Z","iopub.status.idle":"2024-08-22T18:41:41.108511Z","shell.execute_reply.started":"2024-08-22T18:41:41.102012Z","shell.execute_reply":"2024-08-22T18:41:41.107594Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(master_flow(configs).get_graph().draw_mermaid_png()))","metadata":{"id":"l49vdfQsMU1R","outputId":"b3ded57f-fdc3-4c42-ef30-dc37cbd8d43c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration and app-launching","metadata":{}},{"cell_type":"markdown","source":"Vectorstore configuration","metadata":{}},{"cell_type":"code","source":"index_name = \"entailment-test\"\naspects = [\"Health\",\"Governmental\",\"Society\"] #\"Technology\"\n\ntop_retriever = 10 #documents retrieved by retriever\ntop_KBT = 5 #documents retrievede by KBT retriever\n\nretriever = create_retriever(index_name, top_retriever)\nretrievers_KBT = create_KBT_retrievers(aspects, top_KBT)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T18:41:45.496464Z","iopub.execute_input":"2024-08-22T18:41:45.497171Z","iopub.status.idle":"2024-08-22T18:41:45.553261Z","shell.execute_reply.started":"2024-08-22T18:41:45.497141Z","shell.execute_reply":"2024-08-22T18:41:45.552549Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,retriever,retrievers_KBT,aspects):\n        self.local_llm = \"llama3.1\" #\"gemma2\"\n        \n        # retrievers\n        self.retriever = retriever\n        self.retrievers_KBT = retrievers_KBT\n        self.aspects = aspects\n        \n        #if we want print all the process: True\n        self.verbose = False \n        \n        #if we want to include websearch in the workflow\n        self.web_search = False\n\n        # Controlling properties\n        self.safeness = False # if we want to add hate speech detection module\n        self.trustworthiness = True # if we want to add entailment module with KBT\n        self.fairness = False  # if we want to add debiasing module.\n\n        #Controlling entailment\n        # strategy for the entailment, False = \"Credolous\", True = \"Skeptical\" \n        self.strategy_entailment = True\n        #manage the total neutral entailed documents (what if a document is neutral with all documents of KBT)\n        # True = accept the neutral documents, False = don't accept\n        self.neutral_acceptance = True   \n        # True: uses BART model for the entailment, False: uses LLM\n        self.BART_model = False\n        self.shots = 0 #few-shot learning per chi fa entailment\n        \n        #controlling bias detection\n        # True: uses encoder model. False: uses LLM\n        self.bias_encoder_model = False\n        \n        #controlling hate speech detection\n        # True: uses encoder model. False: uses LLM\n        self.hate_encoder_model = True\n        \n        #controlling final generation \n        # True: organize final output in section. False: organize output without sections\n        self.organize = False\n        \n        # For testing\n        self.observer = None\n    \nconfigs = Config(retriever,retrievers_KBT,aspects)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T18:42:08.724271Z","iopub.execute_input":"2024-08-22T18:42:08.724680Z","iopub.status.idle":"2024-08-22T18:42:08.733243Z","shell.execute_reply.started":"2024-08-22T18:42:08.724651Z","shell.execute_reply":"2024-08-22T18:42:08.732104Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"Starting language model","metadata":{}},{"cell_type":"code","source":"start_ollama()\npull_model(configs.local_llm)\nstart_model(configs.local_llm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = {\"question\": \"How people stayed during covid19 pandemy?\", \"aspects\": configs.aspects}\n\nfor output in master_flow(configs).stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\nanswer = value[\"final_answer\"]\nprint(answer)","metadata":{"id":"dJ4IKnHrFfEk","outputId":"6ff28226-55e7-483b-a2b4-b34d39615a67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Funzioni di supporto:\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n\n\n\"\"\"\nqueries_gpt = [\"Does the COVID-19 vaccine contain a microchip for controlling people?\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Does inhaling hot steam kill the COVID-19 virus?\",\n\"Was COVID-19 created in a lab as a biological weapon?\",\n\"Does drinking alcohol help prevent COVID-19 infection?\",\n\"Is 5G responsible for spreading COVID-19?\",\n\"Do masks cause a reduction in blood oxygen levels?\",\n\"Do COVID-19 vaccines cause infertility in women?\",\n\"Can high doses of vitamin C cure COVID-19?\",\n\"Can pets transmit COVID-19 to humans?\"]\n\"\"\"\n\n# Query più generali\nqueries_gpt = [\"Tell me about the Covid-19 pandemic.\",\n\"What measures were taken to manage the Covid-19 emergency?\",\n\"What were the repercussions of the Covid-19 pandemic?\",\n\"How did people experience the Covid-19 pandemic?\",\n\"Tell me about the misinformation spread during the Covid-19 pandemic.\",\n\"How important was the role of doctors during the covid19 pandemic?\",\n\"What origin does covid19 have?\",\n\"Effectiveness of COVID-19 vaccines\",\n\"Was the COVID-19 pandemic planned by a global elite?\",\n\"Was COVID-19 created in a lab as a biological weapon?\"]\n\n#write_file(\"/kaggle/working/gpt_queries.json\",queries_gpt)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T18:42:15.137930Z","iopub.execute_input":"2024-08-22T18:42:15.138628Z","iopub.status.idle":"2024-08-22T18:42:15.145522Z","shell.execute_reply.started":"2024-08-22T18:42:15.138596Z","shell.execute_reply":"2024-08-22T18:42:15.144575Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"Predisposizione dell'observer per salvare i risultati","metadata":{}},{"cell_type":"code","source":"class Observer(object):\n    def __init__(self):\n        self.query=\"\"\n        self.type_of_acceptance=\"\"\n        self.neutral_acceptance=False\n        self.aspects={}\n        self.final_answer=\"\"\n    \n    def generate_dict(self):\n        return {\"query\": self.query,\n                \"type_of_acceptance\": self.type_of_acceptance,\n                \"neutral_acceptance\": self.neutral_acceptance,\n                \"aspects\": self.aspects,\n                \"final_answer\": self.final_answer}","metadata":{"execution":{"iopub.status.busy":"2024-08-22T18:42:17.028743Z","iopub.execute_input":"2024-08-22T18:42:17.029572Z","iopub.status.idle":"2024-08-22T18:42:17.035112Z","shell.execute_reply.started":"2024-08-22T18:42:17.029541Z","shell.execute_reply":"2024-08-22T18:42:17.034172Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"markdown","source":"Combinazioni di configurazione con skeptical/credulous e neutral","metadata":{}},{"cell_type":"markdown","source":"Multiple combination for entailment:","metadata":{}},{"cell_type":"code","source":"shots = [0,3,6,12]\ncombination = [(True,True), (True,False), (False,True), (False,False)] #Skeptical-Neutral #Skeptical-No-Neutral #Cred-Neu #Cred-No-Neu\n\nfor shot in shots:\n    for comb in combination:\n        print(f\"Start combination with shots: {shot}\")\n        configs.shots = shot\n        configs.strategy_entailment =  comb[0]\n        configs.neutral_acceptance = comb[1]\n    \n        queries_list = queries_gpt\n\n        attempt = 1\n        ret_dict = {}\n        for query in queries_list:\n            inputs = {\"question\": query, \"aspects\": configs.aspects}\n            print(f\"Attempt {attempt} start\")\n    \n            configs.observer = Observer()\n            configs.observer.query=query\n            if configs.strategy_entailment:\n                configs.observer.type_of_acceptance=\"Skeptical\"\n            else:\n                configs.observer.type_of_acceptance=\"Credulous\"\n            configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n            for output in master_flow(configs).stream(inputs):\n                for key, value in output.items():\n                    pass\n                    #pprint(f\"Finished running: {key}:\")\n            answer = value[\"final_answer\"]\n    \n            configs.observer.final_answer= answer\n            ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n            attempt = attempt + 1\n\n        stringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\n        write_file(f\"/kaggle/working/llama31_{shot}_shots_{configs.observer.type_of_acceptance}_{stringa}.json\",ret_dict)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T18:43:06.128697Z","iopub.execute_input":"2024-08-22T18:43:06.129072Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start combination with shots: 0\nAttempt 1 start\n","output_type":"stream"},{"name":"stderr","text":"time=2024-08-22T18:43:06.523Z level=INFO source=sched.go:710 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe gpu=GPU-530c08b6-12d8-cce5-7b49-bd8a8fe870c2 parallel=4 available=15615524864 required=\"6.2 GiB\"\ntime=2024-08-22T18:43:06.524Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.5 GiB]\" memory.required.full=\"6.2 GiB\" memory.required.partial=\"6.2 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.2 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-08-22T18:43:06.525Z level=INFO source=server.go:393 msg=\"starting llama server\" cmd=\"/tmp/ollama2381662000/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 37571\"\ntime=2024-08-22T18:43:06.526Z level=INFO source=sched.go:445 msg=\"loaded runners\" count=1\ntime=2024-08-22T18:43:06.526Z level=INFO source=server.go:593 msg=\"waiting for llama runner to start responding\"\ntime=2024-08-22T18:43:06.526Z level=INFO source=server.go:627 msg=\"waiting for server to become available\" status=\"llm server error\"\nllama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 2\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"137257124888576\" timestamp=1724352186\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"137257124888576\" timestamp=1724352186 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"37571\" tid=\"137257124888576\" timestamp=1724352186\n","output_type":"stream"},{"name":"stderr","text":"time=2024-08-22T18:43:06.778Z level=INFO source=server.go:627 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4156.00 MiB\nllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"137257124888576\" timestamp=1724352190\n","output_type":"stream"},{"name":"stderr","text":"time=2024-08-22T18:43:10.183Z level=INFO source=server.go:632 msg=\"llama runner started in 3.66 seconds\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/22 - 18:43:11 | 200 |  4.885420409s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:11 | 200 |  4.919519566s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:11 | 200 |    4.9411836s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:14 | 200 |  931.920918ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:14 | 200 |  1.137370246s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:14 | 200 |  1.245426761s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:15 | 200 |  916.359594ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:15 | 200 |  865.091196ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:15 | 200 |  664.676513ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:16 | 200 |   573.24136ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:16 | 200 |  853.410602ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:16 | 200 |   752.53056ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:16 | 200 |  827.043542ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:17 | 200 |  1.084705985s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:17 | 200 |  846.517457ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:17 | 200 |  539.888233ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:17 | 200 |  460.181602ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:18 | 200 |  701.868544ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:18 | 200 |  1.235993419s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:18 | 200 |  792.801953ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:18 | 200 |  558.192988ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:19 | 200 |  610.386923ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:19 | 200 |  745.141458ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:19 | 200 |  1.413643051s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:19 | 200 |  848.342926ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:20 | 200 |  827.590038ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:20 | 200 |  998.148399ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:21 | 200 |  796.454545ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:21 | 200 |  1.227628957s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:21 | 200 |   691.75362ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:22 | 200 |  935.185319ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:22 | 200 |  973.515259ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:22 | 200 |  1.099244304s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:22 | 200 |  814.446192ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:23 | 200 |  1.190885126s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:23 | 200 |  942.006242ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:23 | 200 |  899.576468ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:24 | 200 |   837.28239ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:24 | 200 |  825.320984ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:24 | 200 |  860.828632ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:24 | 200 |  541.483743ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:25 | 200 |  799.698957ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:25 | 200 |  1.010394568s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:25 | 200 |  1.013795372s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:26 | 200 |   923.90857ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:26 | 200 |  766.031756ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:26 | 200 |  770.496279ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:26 | 200 |  551.676457ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:27 | 200 |  579.639223ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:27 | 200 |  543.369982ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:27 | 200 |   435.35322ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:27 | 200 |  415.319186ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:28 | 200 |  650.080361ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:28 | 200 |  790.986994ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:28 | 200 |  541.595752ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:28 | 200 |  851.607389ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:28 | 200 |  854.752909ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:29 | 200 |  867.026276ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:29 | 200 |  697.830781ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:29 | 200 |  736.146201ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:29 | 200 |  649.066581ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:30 | 200 |  769.565968ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:30 | 200 |  818.870846ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:30 | 200 |   528.09944ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:31 | 200 |  768.058819ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:31 | 200 |  479.440638ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:31 | 200 |  618.651646ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:32 | 200 |   393.69741ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:33 | 200 |  791.067719ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:33 | 200 |  517.651639ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:34 | 200 |   4.45317962s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:34 | 200 |  550.690992ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:35 | 200 |  3.581643057s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:35 | 200 |  1.578378594s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:40 | 200 |  5.023246382s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 2 start\n[GIN] 2024/08/22 - 18:43:42 | 200 |   1.09225458s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:42 | 200 |  1.139306081s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:42 | 200 |  1.229165843s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:44 | 200 |  975.424846ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:45 | 200 |  1.213150382s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:45 | 200 |  1.414808614s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:45 | 200 |   900.66237ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:46 | 200 |  875.479233ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:46 | 200 |  605.546606ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:46 | 200 |  887.315232ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:47 | 200 |  847.338011ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:47 | 200 |  1.062386794s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:47 | 200 |  1.101074928s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:47 | 200 |  669.953349ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:48 | 200 |  519.439525ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:48 | 200 |  558.461253ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:48 | 200 |  520.683932ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:48 | 200 |  566.934479ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:48 | 200 |  643.960135ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:48 | 200 |  628.557484ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:49 | 200 |  359.299527ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:49 | 200 |  429.625427ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:49 | 200 |  596.331705ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:49 | 200 |  426.501432ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:50 | 200 |  754.873955ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:50 | 200 |  802.036158ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:50 | 200 |  822.556136ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:50 | 200 |  873.883578ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:51 | 200 |  806.797582ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:51 | 200 |  801.228951ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:51 | 200 |  810.496227ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:52 | 200 |  900.816621ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:52 | 200 |  933.604765ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:52 | 200 |  841.408699ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:52 | 200 |  863.301033ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:53 | 200 |  894.614198ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:53 | 200 |  744.557507ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:53 | 200 |  614.782265ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:53 | 200 |  616.516571ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:54 | 200 |  611.935491ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:54 | 200 |  548.172899ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:54 | 200 |  484.778476ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:54 | 200 |  558.508814ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:55 | 200 |  521.303752ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:55 | 200 |  647.889125ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:55 | 200 |  658.619896ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:56 | 200 |  627.301657ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:56 | 200 |  613.266936ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:56 | 200 |  610.740557ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:56 | 200 |  3.009621906s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:56 | 200 |  533.184665ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:57 | 200 |  776.677218ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:57 | 200 |   608.60045ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:57 | 200 |  405.595276ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:58 | 200 |  449.015526ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:58 | 200 |  516.745941ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:59 | 200 |   561.27453ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:43:59 | 200 |  478.875095ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:01 | 200 |  2.984163456s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:01 | 200 |  2.188447466s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:05 | 200 |   3.74021343s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 3 start\n[GIN] 2024/08/22 - 18:44:06 | 200 |  924.489827ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:06 | 200 |  959.354259ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:06 | 200 |  1.101725007s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:09 | 200 |   484.87082ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:09 | 200 |  1.180189663s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:10 | 200 |  1.648441267s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:10 | 200 |  1.255882157s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:10 | 200 |  636.833015ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:10 | 200 |  552.517968ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:10 | 200 |  679.354597ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:10 | 200 |  725.085711ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:11 | 200 |  780.219067ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:11 | 200 |  766.734544ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:11 | 200 |  852.360694ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:12 | 200 |  497.091426ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:12 | 200 |  506.081197ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:12 | 200 |   542.31835ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:12 | 200 |  381.518032ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:12 | 200 |  493.503006ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:12 | 200 |  399.908456ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:13 | 200 |  700.981221ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:13 | 200 |  611.287323ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:13 | 200 |  704.417177ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:13 | 200 |  344.179525ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:14 | 200 |  662.743725ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:14 | 200 |  584.309929ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:14 | 200 |  589.096608ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:15 | 200 |  898.146921ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:15 | 200 |  915.392199ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:15 | 200 |   719.69011ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:15 | 200 |  507.562281ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:15 | 200 |  658.120873ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:16 | 200 |  853.007462ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:16 | 200 |  427.029756ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:16 | 200 |  493.401513ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:16 | 200 |  920.771839ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:17 | 200 |  996.458156ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:17 | 200 |  796.919219ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:17 | 200 |  383.863285ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:17 | 200 |  780.686083ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:17 | 200 |   703.12996ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:17 | 200 |  485.806774ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:18 | 200 |  552.296439ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:18 | 200 |  608.450603ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:18 | 200 |  596.554755ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:18 | 200 |  251.368612ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:19 | 200 |  735.317407ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:19 | 200 |  528.634721ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:19 | 200 |  734.207842ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:19 | 200 |  424.037957ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:19 | 200 |  619.042198ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:20 | 200 |  847.939242ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:20 | 200 |  684.219659ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:20 | 200 |  542.832354ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:21 | 200 |  1.039035352s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:21 | 200 |  812.083341ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:22 | 200 |  517.775128ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:22 | 200 |  618.082903ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:22 | 200 |  593.658738ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:23 | 200 |  3.168779444s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:23 | 200 |   460.21045ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:23 | 200 |  397.330577ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:23 | 200 |  373.162145ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:24 | 200 |  1.954952183s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:24 | 200 |  251.168931ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:24 | 200 |  301.680552ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:24 | 200 |  301.652328ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:25 | 200 |  372.557366ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:27 | 200 |  1.858233022s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:30 | 200 |  3.508274969s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 4 start\n[GIN] 2024/08/22 - 18:44:31 | 200 |  1.091988753s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:31 | 200 |  1.074126508s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:31 | 200 |  1.077498264s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:34 | 200 |  634.098366ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:34 | 200 |  934.328046ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:35 | 200 |  1.208607382s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:35 | 200 |  1.659666386s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:36 | 200 |  1.771301516s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:36 | 200 |  992.111239ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:36 | 200 |  1.031459442s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:37 | 200 |  649.129194ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:37 | 200 |  781.162142ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:37 | 200 |  771.084962ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:37 | 200 |  712.581317ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:38 | 200 |  777.714272ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:38 | 200 |  941.480726ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:38 | 200 |  651.141515ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:38 | 200 |  417.321877ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:39 | 200 |  578.357833ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:39 | 200 |   700.77511ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:39 | 200 |  745.263304ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:39 | 200 |  734.969214ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  575.955847ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  592.198432ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  497.943155ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  423.098335ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  466.057242ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  286.935453ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:40 | 200 |  438.937425ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:41 | 200 |  611.528304ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:41 | 200 |  939.401051ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:41 | 200 |  553.145997ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:41 | 200 |  456.436765ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:42 | 200 |  604.563867ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:42 | 200 |  679.273956ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:42 | 200 |  629.384417ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:42 | 200 |   517.92013ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:42 | 200 |  603.692033ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:42 | 200 |  792.065189ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:43 | 200 |  863.980088ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:43 | 200 |  701.822422ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:44 | 200 |  578.783938ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:44 | 200 |  617.344473ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:44 | 200 |  465.549188ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:44 | 200 |  1.850907147s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:44 | 200 |  542.958349ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:45 | 200 |   523.84539ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:45 | 200 |  584.993516ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:45 | 200 |  404.981806ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:45 | 200 |  400.237322ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:45 | 200 |  359.404581ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:46 | 200 |   487.97761ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:46 | 200 |  487.550388ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:47 | 200 |  584.803956ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:47 | 200 |  657.220287ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:47 | 200 |  451.675631ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:47 | 200 |  383.331792ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:48 | 200 |   632.87925ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:48 | 200 |  601.413387ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:48 | 200 |  601.862015ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:48 | 200 |  604.249147ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:49 | 200 |  489.200574ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:49 | 200 |  489.406763ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:50 | 200 |   518.13088ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:50 | 200 |  402.414102ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:51 | 200 |  330.638149ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:51 | 200 |  2.772974778s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:51 | 200 |  481.742232ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:51 | 200 |  276.043349ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:52 | 200 |  288.778251ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:52 | 200 |  354.326354ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:53 | 200 |  458.249243ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:55 | 200 |  2.248837867s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:44:58 | 200 |  3.596290132s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 5 start\n[GIN] 2024/08/22 - 18:45:00 | 200 |  980.385582ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:00 | 200 |  1.057308349s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:00 | 200 |  1.059481833s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:03 | 200 |  1.008379378s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:03 | 200 |  1.281756671s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:03 | 200 |  1.583364754s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:04 | 200 |  897.809247ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:04 | 200 |  1.009903028s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:04 | 200 |  979.510579ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:04 | 200 |  831.357865ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:05 | 200 |  1.325580856s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:05 | 200 |  1.127852688s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:06 | 200 |  1.076911114s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:06 | 200 |  567.092266ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:06 | 200 |  694.362767ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:06 | 200 |  958.710406ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:07 | 200 |  781.917185ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:07 | 200 |   791.06847ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:07 | 200 |  581.878019ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:08 | 200 |  946.347122ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:08 | 200 |  805.475839ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:08 | 200 |  748.787325ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:08 | 200 |   645.07532ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:09 | 200 |  791.081193ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:09 | 200 |  946.275725ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:09 | 200 |  555.194969ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:09 | 200 |  466.743672ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:09 | 200 |  597.182345ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:10 | 200 |  595.508286ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:10 | 200 |  591.209939ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:10 | 200 |  826.996304ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:10 | 200 |  696.863143ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:10 | 200 |  667.517869ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:11 | 200 |  689.835237ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:11 | 200 |  691.848825ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:11 | 200 |  616.905962ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:12 | 200 |  875.080799ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:12 | 200 |  848.497966ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:12 | 200 |  877.669826ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  621.976924ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  696.077747ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  657.045006ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  524.251737ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  578.814651ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  597.547039ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:13 | 200 |  380.268365ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:14 | 200 |  560.239766ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:14 | 200 |  790.206322ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:14 | 200 |  540.472506ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:14 | 200 |   512.67454ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:15 | 200 |  1.039621139s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:15 | 200 |  1.036341256s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:15 | 200 |  773.627478ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:16 | 200 |  452.748012ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:16 | 200 |  641.889311ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:16 | 200 |  664.593446ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:16 | 200 |  617.275087ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:16 | 200 |   655.02478ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:16 | 200 |  689.408289ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:17 | 200 |  704.717606ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:17 | 200 |   542.49573ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:17 | 200 |  341.505089ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:17 | 200 |   759.81534ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:18 | 200 |  524.867638ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:18 | 200 |   520.58219ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:18 | 200 |   592.49881ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:18 | 200 |   379.27506ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:18 | 200 |  576.977103ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:18 | 200 |   495.92612ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:19 | 200 |  757.308589ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:19 | 200 |  913.157431ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:20 | 200 |  506.515266ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:20 | 200 |  904.363692ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:20 | 200 |  611.608817ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:21 | 200 |  630.217819ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:21 | 200 |  700.030831ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:22 | 200 |  800.209315ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:24 | 200 |  5.714939923s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:25 | 200 |  3.115889804s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:25 | 200 |  3.921970057s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:32 | 200 |  6.859040435s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 6 start\n[GIN] 2024/08/22 - 18:45:33 | 200 |  1.011121527s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:33 | 200 |   1.12825142s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:33 | 200 |  1.255651087s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:36 | 200 |  1.001969017s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:36 | 200 |  952.098804ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:36 | 200 |   1.22772895s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:37 | 200 |   1.17726773s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:37 | 200 |  938.692831ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:38 | 200 |  1.485026469s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:38 | 200 |  1.036820424s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:38 | 200 |  1.033556931s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:38 | 200 |  673.795702ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:39 | 200 |  1.263019557s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:39 | 200 |  1.223847148s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:39 | 200 |  1.024065317s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:40 | 200 |  953.197227ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:40 | 200 |  1.055639209s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:40 | 200 |  1.097764876s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:41 | 200 |  350.018394ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:41 | 200 |  975.419899ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:42 | 200 |  757.631096ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:42 | 200 |  1.050735544s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:42 | 200 |  211.128806ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:42 | 200 |  592.295552ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:43 | 200 |  1.099433331s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:43 | 200 |  980.424375ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:43 | 200 |  1.054712489s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:43 | 200 |  803.036525ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:44 | 200 |  843.468991ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:44 | 200 |  795.529355ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:44 | 200 |  587.175167ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:44 | 200 |  579.463005ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:44 | 200 |  370.253252ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:45 | 200 |  641.758071ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:45 | 200 |  996.089874ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:45 | 200 |  976.266649ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:46 | 200 |  503.787716ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:46 | 200 |  900.959545ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:46 | 200 |   781.67685ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:46 | 200 |  514.370027ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:46 | 200 |  575.141884ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:47 | 200 |  492.806619ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:47 | 200 |  596.950035ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:47 | 200 |  579.183721ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:48 | 200 |  1.185931046s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:48 | 200 |  1.039037938s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:48 | 200 |  1.159674532s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:48 | 200 |  393.881563ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:49 | 200 |  830.276615ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:49 | 200 |  1.020943115s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:49 | 200 |  1.011560774s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:49 | 200 |  573.627224ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:50 | 200 |  739.896837ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:50 | 200 |  483.934267ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:50 | 200 |  813.296038ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:51 | 200 |  959.026395ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:51 | 200 |  998.055682ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:51 | 200 |  608.677983ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:51 | 200 |  451.592785ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:51 | 200 |  547.105746ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:51 | 200 |  548.464399ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:52 | 200 |  634.186051ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:53 | 200 |  1.266318349s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:53 | 200 |  797.639214ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:53 | 200 |  597.444384ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:53 | 200 |  556.878143ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:54 | 200 |  705.844462ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:54 | 200 |  861.401871ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:55 | 200 |  474.034364ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:55 | 200 |  3.066134762s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:55 | 200 |  798.793085ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:55 | 200 |  508.490846ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:55 | 200 |  454.024037ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:56 | 200 |  530.716585ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:56 | 200 |  615.751125ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:56 | 200 |  779.003385ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:57 | 200 |  689.926039ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:57 | 200 |  689.347211ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:57 | 200 |  513.473829ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:58 | 200 |  540.612476ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:58 | 200 |  728.161315ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:59 | 200 |  1.033931841s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:45:59 | 200 |  557.280052ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:00 | 200 |  475.510102ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:00 | 200 |  463.835031ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:01 | 200 |  493.323216ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:01 | 200 |  470.738922ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:02 | 200 |  3.576066267s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:02 | 200 |  470.086631ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:04 | 200 |  2.627237317s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:10 | 200 |  5.143284778s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 7 start\n[GIN] 2024/08/22 - 18:46:11 | 200 |  956.493024ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:11 | 200 |  1.071140356s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:11 | 200 |  1.104824523s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:14 | 200 |  1.158002195s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:14 | 200 |  1.373512342s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:14 | 200 |  1.539468509s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:15 | 200 |  718.669513ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |  1.359582254s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |  983.109371ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |  1.149523574s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |  493.686722ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |   452.66305ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |  482.473021ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:16 | 200 |  266.098274ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:17 | 200 |  543.668493ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:17 | 200 |  567.515361ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:17 | 200 |  376.588004ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:17 | 200 |   612.32356ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:17 | 200 |  638.085819ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:17 | 200 |  631.809402ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:18 | 200 |   371.56597ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:18 | 200 |  1.131511499s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:18 | 200 |  1.081761243s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |  838.133393ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |  315.654386ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |  444.615634ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |  408.142542ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |   202.41597ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |  317.282757ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |  388.284001ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:19 | 200 |   382.54174ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:20 | 200 |  379.202052ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:20 | 200 |  376.048947ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:20 | 200 |  349.317675ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:20 | 200 |  391.698746ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:21 | 200 |  708.970233ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:21 | 200 |  1.134020309s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:21 | 200 |  271.728617ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:21 | 200 |  1.441234128s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:21 | 200 |  308.346223ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:22 | 200 |   390.97083ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:24 | 200 |  2.119067945s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:26 | 200 |  2.274914466s |       127.0.0.1 | POST     \"/api/generate\"\nAttempt 8 start\n[GIN] 2024/08/22 - 18:46:27 | 200 |  917.102139ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:27 | 200 |  1.027427088s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:27 | 200 |  1.068512123s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:30 | 200 |  668.506149ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:30 | 200 |   1.09473996s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:31 | 200 |  1.475815807s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:31 | 200 |  2.050758304s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:32 | 200 |  1.736206965s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:32 | 200 |  997.756318ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:33 | 200 |  1.070644812s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:33 | 200 |  582.444167ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:33 | 200 |  890.578346ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:34 | 200 |  1.058378328s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:34 | 200 |  1.303912661s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:34 | 200 |  830.329416ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:34 | 200 |  596.432246ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:34 | 200 |  588.078734ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:35 | 200 |  681.129924ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:35 | 200 |  772.626492ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:35 | 200 |  881.177438ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:36 | 200 |   740.67327ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:36 | 200 |   998.40331ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:36 | 200 |  995.493027ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:37 | 200 |  864.814678ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:37 | 200 |  833.376266ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:37 | 200 |  991.685088ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:37 | 200 |  827.249541ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:38 | 200 |   456.47285ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:38 | 200 |  766.114626ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:39 | 200 |  1.304016559s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:39 | 200 |  1.167114699s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:39 | 200 |  995.062651ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:39 | 200 |  573.075572ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  616.967323ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  615.827131ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  599.066225ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  646.262956ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  607.481091ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  532.854145ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/22 - 18:46:40 | 200 |  532.854145ms |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Single test for a list of query (complete test):","metadata":{}},{"cell_type":"code","source":"#queries_list = load_queries(\"/kaggle/input/preference/gpt_queries.json\")\nqueries_list = [\"Talk to me about COVID-19\"] #queries_gpt\n\nattempt = 1\nret_dict = {}\nfor query in queries_list:\n    inputs = {\"question\": query, \"aspects\": configs.aspects}\n    print(f\"Attempt {attempt} start\")\n    \n    configs.observer = Observer()\n    configs.observer.query=query\n    if configs.strategy_entailment:\n        configs.observer.type_of_acceptance=\"Skeptical\"\n    else:\n        configs.observer.type_of_acceptance=\"Credulous\"\n    configs.observer.neutral_acceptance=configs.neutral_acceptance\n\n    for output in master_flow(configs).stream(inputs):\n        for key, value in output.items():\n            pass\n            #pprint(f\"Finished running: {key}:\")\n    answer = value[\"final_answer\"]\n    \n    configs.observer.final_answer= answer\n    ret_dict[f\"attempt {attempt}\"] = configs.observer.generate_dict()\n    \n    attempt = attempt + 1\n\nstringa = \"Neutral\" if configs.neutral_acceptance else \"No-Neutral\"\nwrite_file(f\"/kaggle/working/complete_test_llama31.json\",ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ret_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Entailment evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione su notizie vere (0) e notizie false (1)","metadata":{}},{"cell_type":"code","source":"# Label 0, notizie vere\n# Label 1, notizie false\n\ndef results_on_label(label, num_attempts, dict_input, aspects):\n    recall = 0\n    precision = 0\n    support = 0\n    valid_attempts = 0\n    recalls = []\n    precisions = []\n\n    for i in range(1,num_attempts+1):\n        for aspect in aspects:\n            recall = recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"]\n            precision = precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"]\n            support_dict = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"support\"]\n            # Il tentativo è valido solo se c'è almeno un documento recuperato!\n            if support_dict != 0:\n                recalls.append(dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"recall\"])\n                precisions.append(dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][label][\"precision\"])\n                valid_attempts = valid_attempts + 1\n                support = support + support_dict\n                \n    e_recall = recall/valid_attempts\n    e_precision = precision/valid_attempts\n    f1 = 2*e_recall*e_precision/(e_recall+e_precision)\n    diff_rec = [(rec - e_recall)**2 for rec in recalls]\n    diff_prec = [(prec - e_precision)**2 for prec in precisions]\n    var_recall = sum(diff_rec)/valid_attempts\n    var_precision =  sum(diff_prec)/valid_attempts\n    return {\"recall\":e_recall, \n            \"var_recall\": var_recall,\n            \"precision\":e_precision,\n            \"var_precision\": var_precision,\n            \"f1\":f1, \n            \"support\":support/(num_attempts*len(aspects))}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Valutazione per aspetto","metadata":{}},{"cell_type":"code","source":"def results_on_aspects(aspect, num_attempts, dict_input):\n    recall_0 = 0\n    precision_0 = 0\n    recall_1 = 0\n    precision_1 = 0\n    \n    accuracy = 0\n    \n    weight_recall = 0\n    weight_precision = 0\n    \n    valid_attempts_0 = 0\n    valid_attempts_1 = 0\n\n    for i in range(1,num_attempts+1):\n        recall_0 = recall_0 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"recall\"]\n        precision_0 = precision_0 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"precision\"]\n        support_dict_0 = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"0\"][\"support\"]\n        if support_dict_0 != 0:\n            valid_attempts_0 = valid_attempts_0 + 1\n        \n        recall_1 = recall_1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"recall\"]\n        precision_1 = precision_1 + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"precision\"]\n        support_dict_1 = dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"1\"][\"support\"]\n        if support_dict_1 != 0:\n            valid_attempts_1 = valid_attempts_1 + 1\n            \n        weight_recall = weight_recall + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"recall\"]\n        weight_precision = weight_precision + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"weighted avg\"][\"precision\"]\n                \n        try:\n            accuracy = accuracy + dict_input[f\"attempt {i}\"][\"aspects\"][f\"{aspect}\"][\"report_entailment\"][\"accuracy\"]\n        except:\n            accuracy = accuracy + 1 #se l'accuracy non è presente nel dizionario, è perché vale 1 (vedi esempio 4 di llama_second_skeptical_neutral)\n    \n    e_recall_0 = recall_0/valid_attempts_0\n    e_precision_0 = precision_0/valid_attempts_0\n    \n    e_recall_1 = recall_1/valid_attempts_1\n    e_precision_1 = precision_1/valid_attempts_1\n    \n    macro_recall = (e_recall_0+e_recall_1)/2\n    macro_precision = (e_precision_0+e_precision_1)/2\n    f1 = 2*macro_recall*macro_precision/(macro_recall+macro_precision)\n    \n    w_recall = weight_recall/(num_attempts)\n    w_precision = weight_precision/(num_attempts)\n    weight_f1 = 2*w_recall*w_precision/(w_recall+w_precision)\n    return {\"macro_recall\": macro_recall,\n            \"macro_precision\": macro_precision, \n            \"macro_f1\": f1,\n            \"macro_accuracy\":accuracy/(num_attempts),\n            \"w_recall\": w_recall,\n            \"w_precision\": w_precision, \n            \"w_f1\":weight_f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label 0: notizie vere (negative class)\n# label 1: notizie false (positive class)\nimport pandas as pd\n\ndef compute_result_on_label(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        dict_input = load_json(f\"/kaggle/working/{model}_{shot}_shots_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_gov_10kbt_{acceptance}_{neutral}.json\")\n\n    results_label_0 = results_on_label(\"0\", num_queries, dict_input, aspects)\n    results_label_1 = results_on_label(\"1\", num_queries, dict_input, aspects)\n    \n\n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n\n    df[\"precision_0\"] = results_label_0[\"precision\"]\n    df[\"var_prec_0\"] = results_label_0[\"var_precision\"]\n    df[\"recall_0\"] = results_label_0[\"recall\"]\n    df[\"var_recall_0\"] = results_label_0[\"var_recall\"]\n    df[\"f1_0\"] = results_label_0[\"f1\"]\n    df[\"support_0\"] = results_label_0[\"support\"]\n    \n\n    df[\"precision_1\"] = results_label_1[\"precision\"]\n    df[\"var_prec_1\"] = results_label_1[\"var_precision\"]\n    df[\"recall_1\"] = results_label_1[\"recall\"]\n    df[\"var_recall_1\"] = results_label_1[\"var_recall\"]\n    df[\"f1_1\"] = results_label_1[\"f1\"]\n    df[\"support_1\"] = results_label_1[\"support\"]\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_result_on_aspects(model, shot, acceptance, neutral, num_queries, aspects) -> pd.DataFrame:\n    if shot==\"first\" or model==\"BART\" or model==\"gemma2\":\n        dict_input = load_json(f\"/kaggle/working/test_{model}_{acceptance}_{neutral}.json\")\n    else:\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_{shot}_{acceptance}_{neutral}.json\")\n        dict_input = load_json(f\"/kaggle/working/{model}_{shot}_shots_{acceptance}_{neutral}.json\")\n        #dict_input = load_json(f\"/kaggle/working/test_{model}_gov_{acceptance}_{neutral}.json\")\n        \n    df = pd.DataFrame()\n    df[\"model\"] = [model]\n    df[\"shots\"] = [shot]\n    df[\"num_queries\"] = [num_queries]\n    df[\"retrieved\"] = [10 if num_queries==10 else 20]\n    df[\"type_acceptance\"] = [acceptance]\n    df[\"neutral\"] = [neutral]\n    \n    for aspect in aspects:\n        result = results_on_aspects(aspect, num_queries, dict_input)\n        df[f\"precision_{aspect}\"] = result[\"macro_precision\"]\n        df[f\"recall_{aspect}\"] = result[\"macro_recall\"]\n        df[f\"f1_{aspect}\"] = result[\"macro_f1\"]\n        df[f\"accuracy_{aspect}\"] = result[\"macro_accuracy\"]\n        \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import openpyxl\n\nattempts = 10\nnum_aspects = 3\n\nmodels = [\"llama31\"] #[\"gemma2\",\"BART\",]\nshots = [0,3,6,12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Governmental\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_labels_X.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_label(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\n#results.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attempts = 10\nnum_aspects = 3\n\nmodels = [\"llama31\"]#[\"BART\",\"llama31\"] #[\"gemma2\"]\nshots = [0,3,6,12] #[\"first\",\"second\",\"third\"]\ntype_acceptance = [\"Skeptical\",\"Credulous\"]\nneutral_acceptance = [\"No-Neutral\",\"Neutral\"]\naspects = [\"Health\",\"Governmental\",\"Society\"]\n\nexcel_file = \"/kaggle/working/test_entailment_aspects_X.xlsx\"\nresults = pd.DataFrame()\nfor model in models:\n    for shot in shots:\n        for acceptance in type_acceptance:\n            for neutral in neutral_acceptance:\n                new_row = compute_result_on_aspects(model, shot, acceptance, neutral, attempts, aspects)\n                results = pd.concat([results, new_row], ignore_index=True)\n        if model==\"BART\": break\n\nprint(results.round(3))\n#results.round(3).to_excel(excel_file, index=False, engine='openpyxl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ragas (application evaluation)","metadata":{}},{"cell_type":"markdown","source":"https://docs.ragas.io/en/stable/","metadata":{}},{"cell_type":"code","source":"#todo","metadata":{},"execution_count":null,"outputs":[]}]}