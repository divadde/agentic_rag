{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7257995,
          "sourceType": "datasetVersion",
          "datasetId": 4205998
        }
      ],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "NaulCKkvQpCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "dK2YNbyNwDCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6501fdac-cc83-4828-a618-21b628b37536"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "\n",
        "#istallazione di ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:09.097338Z",
          "iopub.execute_input": "2024-08-13T11:13:09.098272Z",
          "iopub.status.idle": "2024-08-13T11:13:12.852185Z",
          "shell.execute_reply.started": "2024-08-13T11:13:09.098229Z",
          "shell.execute_reply": "2024-08-13T11:13:12.851000Z"
        },
        "trusted": true,
        "id": "H_iRuw_Lv00C",
        "outputId": "79b602e2-6edf-45da-9e1c-8895d45e9fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def start_ollama():\n",
        "    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n",
        "    t.start()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:12.854455Z",
          "iopub.execute_input": "2024-08-13T11:13:12.855253Z",
          "iopub.status.idle": "2024-08-13T11:13:12.860443Z",
          "shell.execute_reply.started": "2024-08-13T11:13:12.855213Z",
          "shell.execute_reply": "2024-08-13T11:13:12.859428Z"
        },
        "trusted": true,
        "id": "MOsJonv0v00E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pull_model(local_llm):\n",
        "    !ollama pull local_llm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:12.861731Z",
          "iopub.execute_input": "2024-08-13T11:13:12.862069Z",
          "iopub.status.idle": "2024-08-13T11:13:12.914838Z",
          "shell.execute_reply.started": "2024-08-13T11:13:12.862031Z",
          "shell.execute_reply": "2024-08-13T11:13:12.914004Z"
        },
        "trusted": true,
        "id": "IM4kCKPfv00F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_model(local_llm):\n",
        "    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n",
        "    t2.start()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:12.916848Z",
          "iopub.execute_input": "2024-08-13T11:13:12.917588Z",
          "iopub.status.idle": "2024-08-13T11:13:12.925330Z",
          "shell.execute_reply.started": "2024-08-13T11:13:12.917556Z",
          "shell.execute_reply": "2024-08-13T11:13:12.924515Z"
        },
        "trusted": true,
        "id": "JHFNOrWgv00F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U scikit-learn==1.3 langchain-ai21 langchain_community tiktoken langchainhub langchain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:12.926370Z",
          "iopub.execute_input": "2024-08-13T11:13:12.926652Z",
          "iopub.status.idle": "2024-08-13T11:13:26.299377Z",
          "shell.execute_reply.started": "2024-08-13T11:13:12.926630Z",
          "shell.execute_reply": "2024-08-13T11:13:26.298011Z"
        },
        "trusted": true,
        "id": "x0_KFzE1v00F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "#os.environ[\"LANGCHAIN_PROJECT\"]=\"hate-speech-project\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:26.302096Z",
          "iopub.execute_input": "2024-08-13T11:13:26.302418Z",
          "iopub.status.idle": "2024-08-13T11:13:26.309351Z",
          "shell.execute_reply.started": "2024-08-13T11:13:26.302390Z",
          "shell.execute_reply": "2024-08-13T11:13:26.308568Z"
        },
        "trusted": true,
        "id": "96Q_pDBPv00G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:26.310355Z",
          "iopub.execute_input": "2024-08-13T11:13:26.310635Z",
          "iopub.status.idle": "2024-08-13T11:13:26.321511Z",
          "shell.execute_reply.started": "2024-08-13T11:13:26.310612Z",
          "shell.execute_reply": "2024-08-13T11:13:26.320650Z"
        },
        "trusted": true,
        "id": "vVEKuoilv00G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo #all'incirca 7.000 testi di test\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/predictions_hate/HateSpeechDatasetBalanced.csv\")\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "#label_0 = df[df[\"Label\"]==0] #no hate speech\n",
        "#label_1 = df[df[\"Label\"]==1] #hate speech\n",
        "#print(f\"Label con valore 0: {len(label_0)}\")\n",
        "#print(f\"Label con valore 1: {len(label_1)}\")\n",
        "\n",
        "# Campiona l'1% del DataFrame\n",
        "sampled_df = df.sample(frac=0.01, random_state=42)\n",
        "\n",
        "x_test = sampled_df[\"Content\"]\n",
        "y_test = sampled_df[\"Label\"]\n",
        "\n",
        "print(len(x_test))\n",
        "print(len(y_test))\n",
        "\n",
        "label_0 = sampled_df[sampled_df[\"Label\"]==0] #no hate speech\n",
        "label_1 = sampled_df[sampled_df[\"Label\"]==1] #hate speech\n",
        "print(f\"Label con valore 0: {len(label_0)}\")\n",
        "print(f\"Label con valore 1: {len(label_1)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:26.322815Z",
          "iopub.execute_input": "2024-08-13T11:13:26.323419Z",
          "iopub.status.idle": "2024-08-13T11:13:28.322790Z",
          "shell.execute_reply.started": "2024-08-13T11:13:26.323387Z",
          "shell.execute_reply": "2024-08-13T11:13:28.321851Z"
        },
        "trusted": true,
        "id": "cYXvQDJpv00G",
        "outputId": "9960543f-688f-4bf2-a98e-7e1f62a6b068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "726119\n",
            "7261\n",
            "7261\n",
            "Label con valore 0: 3584\n",
            "Label con valore 1: 3677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\"input\": \"We need to keep our culture pure.\", \"output\": \"yes\"},\n",
        "    {\"input\": \"Fuck you.\", \"output\": \"yes\"},\n",
        "    {\"input\": \"It’s important to respect other people's choices.\", \"output\": \"no\"},\n",
        "    {\"input\": \"Some groups of people are bastards\", \"output\": \"yes\"},\n",
        "    {\"input\": \"I think the weather is nice today.\", \"output\": \"no\"},\n",
        "    {\"input\": \"Go back to where you came from, asshole.\", \"output\": \"yes\"},\n",
        "    {\"input\": \"Have a great day!.\", \"output\": \"no\"},\n",
        "    {\"input\": \"They're just different, and that's not a bad thing, but they shouldn't try to change us.\", \"output\": \"yes\"},\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:28.324036Z",
          "iopub.execute_input": "2024-08-13T11:13:28.324325Z",
          "iopub.status.idle": "2024-08-13T11:13:28.330107Z",
          "shell.execute_reply.started": "2024-08-13T11:13:28.324299Z",
          "shell.execute_reply": "2024-08-13T11:13:28.329192Z"
        },
        "trusted": true,
        "id": "Z5s_mU9sv00H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "def create_prompt(shot):\n",
        "    example_prompt  = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"assistant\", \"{output}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "        example_prompt=example_prompt ,\n",
        "        examples=examples[:shot], #:2 two-shot, #4: four-shot #8 eight-shot\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"\"\"You are an helpful assistant who has to detect the presence of hate speech.\n",
        "                Hate speech is speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.\n",
        "                You have to answer \"yes\" if it contains hate speech, or \"no\" if it doesn't contain hate speech.\n",
        "                Give only the answer \"yes\" or \"no\", NO PREAMBLE, NO EXPLANATIONS.\"\"\"),\n",
        "             few_shot_prompt,\n",
        "            (\"user\", \"{input}\"),\n",
        "        ] if shot>0 else\n",
        "        [\n",
        "            (\"system\", \"\"\"You are an helpful assistant who has to detect the presence of hate speech.\n",
        "                Hate speech is speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.\n",
        "                You have to answer \"yes\" if it contains hate speech, or \"no\" if it doesn't contain hate speech.\n",
        "                Give only the answer \"yes\" or \"no\", NO PREAMBLE, NO EXPLANATIONS.\"\"\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    return prompt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:28.332988Z",
          "iopub.execute_input": "2024-08-13T11:13:28.333315Z",
          "iopub.status.idle": "2024-08-13T11:13:28.342308Z",
          "shell.execute_reply.started": "2024-08-13T11:13:28.333291Z",
          "shell.execute_reply": "2024-08-13T11:13:28.341333Z"
        },
        "trusted": true,
        "id": "sPTQVSnov00H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import functools\n",
        "import sys\n",
        "import io\n",
        "\n",
        "def hate_speech_detection(llm, shot):\n",
        "    prompt_final = create_prompt(shot)\n",
        "    hate_speech_detection = prompt_final | llm\n",
        "    return hate_speech_detection"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:28.343388Z",
          "iopub.execute_input": "2024-08-13T11:13:28.343676Z",
          "iopub.status.idle": "2024-08-13T11:13:28.357996Z",
          "shell.execute_reply.started": "2024-08-13T11:13:28.343632Z",
          "shell.execute_reply": "2024-08-13T11:13:28.357137Z"
        },
        "trusted": true,
        "id": "DQHF3Ghiv00I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(llm,x_test,shot):\n",
        "    y_pred = []\n",
        "    chain = hate_speech_detection(llm,shot)\n",
        "    for x in tqdm(x_test):\n",
        "        answer = chain.invoke({\"input\": x})\n",
        "        #print(answer)\n",
        "        if \"no\" in answer.lower(): y_pred.append(0) #no hate speech\n",
        "        else: y_pred.append(1) #hate speech\n",
        "    return y_pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:28.359152Z",
          "iopub.execute_input": "2024-08-13T11:13:28.359686Z",
          "iopub.status.idle": "2024-08-13T11:13:28.368866Z",
          "shell.execute_reply.started": "2024-08-13T11:13:28.359655Z",
          "shell.execute_reply": "2024-08-13T11:13:28.368009Z"
        },
        "trusted": true,
        "id": "r1aBrss8v00I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "00l1pJykQ5Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "models = [\"mistral\"]\n",
        "shots = [2,4,8]\n",
        "\n",
        "def write_file(filename,content):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(content, file, indent=4)\n",
        "\n",
        "for model in models:\n",
        "    start_ollama()\n",
        "    pull_model(model)\n",
        "    start_model(model)\n",
        "    time.sleep(500)\n",
        "    llm = Ollama(model=model, temperature=0)\n",
        "    for shot in shots:\n",
        "        y_pred = predict(llm,x_test,shot)\n",
        "        if model==\"llama3.1\":\n",
        "            write_file(f\"/content/drive/MyDrive/predictions_hate/prediction_hate_speech_llama31_{shot}_shot.json\", y_pred)\n",
        "        else:\n",
        "            write_file(f\"/content/drive/MyDrive/predictions_hate/prediction_hate_speech_{model}_{shot}_shot.json\", y_pred)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T11:13:28.369906Z",
          "iopub.execute_input": "2024-08-13T11:13:28.370164Z"
        },
        "trusted": true,
        "id": "j0FLeKKMv00J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "S6F8Q8JuQ7ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_json(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Label 0: no hate speech\n",
        "# Label 1: hate speech\n",
        "\n",
        "model = \"llama31\" #gemma2 #mistral #llama31\n",
        "shot = 0 #2 #4 #8\n",
        "y_pred = load_json(f\"/content/drive/MyDrive/predictions_hate/prediction_hate_speech_{model}_{shot}_shot.json\")\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred, labels=[0,1], output_dict=True)\n",
        "\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{class_report}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ijc6_VIRv00L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cf1287-e664-420d-c984-3683ce5a3739"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6607905247211128\n",
            "Confusion Matrix:\n",
            "[[3027  557]\n",
            " [1906 1771]]\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.613622542063653, 'recall': 0.8445870535714286, 'f1-score': 0.7108136667840789, 'support': 3584.0}, '1': {'precision': 0.7607388316151202, 'recall': 0.4816426434593419, 'f1-score': 0.589841798501249, 'support': 3677.0}, 'accuracy': 0.6607905247211128, 'macro avg': {'precision': 0.6871806868393866, 'recall': 0.6631148485153853, 'f1-score': 0.650327732642664, 'support': 7261.0}, 'weighted avg': {'precision': 0.6881228308228796, 'recall': 0.6607905247211128, 'f1-score': 0.6495530195349445, 'support': 7261.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_json(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def write_file(filename,content):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(content, file, indent=4)"
      ],
      "metadata": {
        "id": "0p70KBPIv00L"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_result(model, shot, y_test) -> pd.DataFrame:\n",
        "  y_pred = load_json(f\"/content/drive/MyDrive/predictions_hate/prediction_hate_speech_{model}_{shot}_shot.json\")\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  class_report = classification_report(y_test, y_pred, labels=[0,1], output_dict=True)\n",
        "\n",
        "  TN, FP, FN, TP = conf_matrix.ravel()\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  df[\"model\"] = [model]\n",
        "  df[\"shots\"] = [shot]\n",
        "\n",
        "  df[\"precision\"] = class_report[\"1\"][\"precision\"]\n",
        "  df[\"recall\"] = class_report[\"1\"][\"recall\"]\n",
        "  df[\"f1\"] = class_report[\"1\"][\"f1-score\"]\n",
        "  df[\"accuracy\"] = class_report[\"accuracy\"]\n",
        "  df[\"TP\"] = TP\n",
        "  df[\"FP\"] = FP\n",
        "  df[\"TN\"] = TN\n",
        "  df[\"FN\"] = FN\n",
        "\n",
        "  df[\"w_precision\"] = class_report[\"weighted avg\"][\"precision\"]\n",
        "  df[\"w_recall\"] = class_report[\"weighted avg\"][\"recall\"]\n",
        "  df[\"w_f1\"] = class_report[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "cQb7lXVheTnF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "models = [\"llama31\",\"gemma2\",\"mistral\"]\n",
        "shots = [0,2,4,8]\n",
        "\n",
        "excel_file = \"/content/drive/MyDrive/predictions_hate/Hate_speech_test.xlsx\"\n",
        "results = pd.DataFrame()\n",
        "for model in models:\n",
        "  for shot in shots:\n",
        "    new_row = compute_result(model, shot, y_test)\n",
        "    results = pd.concat([results, new_row], ignore_index=True)\n",
        "print(results.round(3))\n",
        "\n",
        "results.round(3).to_excel(excel_file, index=False, engine='openpyxl')"
      ],
      "metadata": {
        "id": "0yQ0BPy0v00M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7b37c5-5400-4fc8-d768-d7d9d5080dbd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      model  shots  precision  recall     f1  accuracy    TP    FP    TN  \\\n",
            "0   llama31      0      0.761   0.482  0.590     0.661  1771   557  3027   \n",
            "1   llama31      2      0.723   0.722  0.723     0.719  2656  1018  2566   \n",
            "2   llama31      4      0.677   0.821  0.742     0.711  3018  1437  2147   \n",
            "3   llama31      8      0.644   0.883  0.745     0.694  3248  1795  1789   \n",
            "4    gemma2      0      0.728   0.682  0.704     0.710  2508   937  2647   \n",
            "5    gemma2      2      0.725   0.710  0.717     0.717  2611   992  2592   \n",
            "6    gemma2      4      0.718   0.767  0.742     0.729  2820  1108  2476   \n",
            "7    gemma2      8      0.703   0.787  0.743     0.724  2895  1222  2362   \n",
            "8   mistral      0      0.657   0.742  0.697     0.673  2728  1425  2159   \n",
            "9   mistral      2      0.556   0.948  0.701     0.590  3487  2787   797   \n",
            "10  mistral      4      0.545   0.887  0.675     0.568  3262  2723   861   \n",
            "11  mistral      8      0.576   0.793  0.667     0.599  2917  2150  1434   \n",
            "\n",
            "      FN  w_precision  w_recall   w_f1  \n",
            "0   1906        0.688     0.661  0.650  \n",
            "1   1021        0.719     0.719  0.719  \n",
            "2    659        0.721     0.711  0.708  \n",
            "3    429        0.724     0.694  0.682  \n",
            "4   1169        0.711     0.710  0.710  \n",
            "5   1066        0.717     0.717  0.717  \n",
            "6    857        0.730     0.729  0.729  \n",
            "7    782        0.727     0.724  0.723  \n",
            "8    949        0.676     0.673  0.671  \n",
            "9    190        0.680     0.590  0.527  \n",
            "10   415        0.609     0.568  0.517  \n",
            "11   760        0.614     0.599  0.583  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwTfV71dhSde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}