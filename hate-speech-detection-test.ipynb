{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7257995,"sourceType":"datasetVersion","datasetId":4205998}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-10T17:30:01.465956Z","iopub.execute_input":"2024-08-10T17:30:01.466301Z","iopub.status.idle":"2024-08-10T17:30:05.940218Z","shell.execute_reply.started":"2024-08-10T17:30:01.466272Z","shell.execute_reply":"2024-08-10T17:30:05.939077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":">>> Downloading ollama...\n######################################################################## 100.0%#=#=#                                                                          \n>>> Installing ollama to /usr/local/bin...\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}]},{"cell_type":"code","source":"def start_ollama():\n    t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n    t.start()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:30:36.444601Z","iopub.execute_input":"2024-08-10T17:30:36.445111Z","iopub.status.idle":"2024-08-10T17:30:36.450512Z","shell.execute_reply.started":"2024-08-10T17:30:36.445074Z","shell.execute_reply":"2024-08-10T17:30:36.449536Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def pull_model(local_llm):\n    !ollama pull local_llm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:30:36.626493Z","iopub.execute_input":"2024-08-10T17:30:36.627049Z","iopub.status.idle":"2024-08-10T17:30:36.631336Z","shell.execute_reply.started":"2024-08-10T17:30:36.627019Z","shell.execute_reply":"2024-08-10T17:30:36.630392Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def start_model(local_llm):        \n    t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", local_llm]),daemon=True)\n    t2.start()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:30:37.926703Z","iopub.execute_input":"2024-08-10T17:30:37.927093Z","iopub.status.idle":"2024-08-10T17:30:37.932027Z","shell.execute_reply.started":"2024-08-10T17:30:37.927062Z","shell.execute_reply":"2024-08-10T17:30:37.931054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stderr\n%pip install -U scikit-learn==1.3 langchain-ai21 langchain_community tiktoken langchainhub langchain langgraph","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:30:39.375101Z","iopub.execute_input":"2024-08-10T17:30:39.375721Z","iopub.status.idle":"2024-08-10T17:31:04.846703Z","shell.execute_reply.started":"2024-08-10T17:30:39.375690Z","shell.execute_reply":"2024-08-10T17:31:04.845433Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:04.849080Z","iopub.execute_input":"2024-08-10T17:31:04.849818Z","iopub.status.idle":"2024-08-10T17:31:04.854665Z","shell.execute_reply.started":"2024-08-10T17:31:04.849779Z","shell.execute_reply":"2024-08-10T17:31:04.853805Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom tqdm import tqdm\nfrom langchain_community.llms import Ollama\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:04.855973Z","iopub.execute_input":"2024-08-10T17:31:04.856289Z","iopub.status.idle":"2024-08-10T17:31:06.518099Z","shell.execute_reply.started":"2024-08-10T17:31:04.856259Z","shell.execute_reply":"2024-08-10T17:31:06.517327Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Dati","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/datasets/waalbannyantudre/hate-speech-detection-curated-dataset","metadata":{}},{"cell_type":"markdown","source":"https://www.sciencedirect.com/science/article/pii/S2352340922010356","metadata":{}},{"cell_type":"code","source":"# todo #all'incirca 7.000 testi di test\n\ndf = pd.read_csv(\"/kaggle/input/hate-speech-detection-curated-dataset/HateSpeechDatasetBalanced.csv\")\n\nprint(len(df))\n\n#label_0 = df[df[\"Label\"]==0] #no hate speech\n#label_1 = df[df[\"Label\"]==1] #hate speech\n#print(f\"Label con valore 0: {len(label_0)}\")\n#print(f\"Label con valore 1: {len(label_1)}\")\n\n# Campiona l'1% del DataFrame\nsampled_df = df.sample(frac=0.01, random_state=42)\n\nx_test = sampled_df[\"Content\"]\ny_test = sampled_df[\"Label\"]\n\nprint(len(x_test))\nprint(len(y_test))\n\nlabel_0 = sampled_df[sampled_df[\"Label\"]==0] #no hate speech\nlabel_1 = sampled_df[sampled_df[\"Label\"]==1] #hate speech\nprint(f\"Label con valore 0: {len(label_0)}\")\nprint(f\"Label con valore 1: {len(label_1)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:06.520624Z","iopub.execute_input":"2024-08-10T17:31:06.521164Z","iopub.status.idle":"2024-08-10T17:31:09.661300Z","shell.execute_reply.started":"2024-08-10T17:31:06.521131Z","shell.execute_reply":"2024-08-10T17:31:09.660243Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"726119\n7261\n7261\nLabel con valore 0: 3584\nLabel con valore 1: 3677\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hate speech","metadata":{}},{"cell_type":"code","source":"model = \"mixtral:8x7b\" #llama3.1 #gemma2 #mistral #mixtral:8x7b \n\nstart_ollama()\npull_model(model)\nstart_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prompting","metadata":{}},{"cell_type":"markdown","source":"https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples_chat/","metadata":{}},{"cell_type":"code","source":"examples = [\n    {\"input\": \"We need to keep our culture pure.\", \"output\": \"yes\"},\n    {\"input\": \"Fuck you.\", \"output\": \"yes\"},\n    {\"input\": \"Itâ€™s important to respect other people's choices.\", \"output\": \"no\"},\n    {\"input\": \"They're just different, and that's not a bad thing, but they shouldn't try to change us.\", \"output\": \"yes\"},\n    {\"input\": \"I think the weather is nice today.\", \"output\": \"no\"},\n    {\"input\": \"Some groups of people are bastards\", \"output\": \"yes\"},\n    {\"input\": \"Go back to where you came from, asshole.\", \"output\": \"yes\"},\n    {\"input\": \"Have a great day!.\", \"output\": \"no\"},\n]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:09.662596Z","iopub.execute_input":"2024-08-10T17:31:09.662979Z","iopub.status.idle":"2024-08-10T17:31:09.669586Z","shell.execute_reply.started":"2024-08-10T17:31:09.662946Z","shell.execute_reply":"2024-08-10T17:31:09.668257Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n\nexample_prompt  = ChatPromptTemplate.from_messages(\n    [\n        (\"user\", \"{input}\"),\n        (\"assistant\", \"{output}\"),\n    ]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt ,\n    examples=examples[:2], #:2 two-shot, #4: four-shot #8 eight-shot\n)\n\nprint(few_shot_prompt.format())\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"\"\"You are an helpful assistant who has to detect the presence of hate speech.\n            Hate speech is speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. \n            You have to answer \"yes\" if it contains hate speech, or \"no\" if it doesn't contain hate speech. \n            Give only the answer \"yes\" or \"no\", NO PREAMBLE, NO EXPLANATIONS.\"\"\"),\n         #few_shot_prompt, #commenta per lo zero shot\n        (\"user\", \"{input}\"),\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n\ndef create_prompt(shot):\n    example_prompt  = ChatPromptTemplate.from_messages(\n        [\n            (\"user\", \"{input}\"),\n            (\"assistant\", \"{output}\"),\n        ]\n    )\n\n    few_shot_prompt = FewShotChatMessagePromptTemplate(\n        example_prompt=example_prompt ,\n        examples=examples[:shot], #:2 two-shot, #4: four-shot #8 eight-shot\n    )\n    \n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", \"\"\"You are an helpful assistant who has to detect the presence of hate speech.\n                Hate speech is speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. \n                You have to answer \"yes\" if it contains hate speech, or \"no\" if it doesn't contain hate speech. \n                Give only the answer \"yes\" or \"no\", NO PREAMBLE, NO EXPLANATIONS.\"\"\"),\n             few_shot_prompt, \n            (\"user\", \"{input}\"),\n        ] if shot>0 else\n        [\n            (\"system\", \"\"\"You are an helpful assistant who has to detect the presence of hate speech.\n                Hate speech is speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. \n                You have to answer \"yes\" if it contains hate speech, or \"no\" if it doesn't contain hate speech. \n                Give only the answer \"yes\" or \"no\", NO PREAMBLE, NO EXPLANATIONS.\"\"\"),\n            (\"user\", \"{input}\"),\n        ]\n    )\n    return prompt\n\n#llm = Ollama(model=model, temperature=0)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:09.670680Z","iopub.execute_input":"2024-08-10T17:31:09.670963Z","iopub.status.idle":"2024-08-10T17:31:09.685001Z","shell.execute_reply.started":"2024-08-10T17:31:09.670940Z","shell.execute_reply":"2024-08-10T17:31:09.684083Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#import functools\nimport sys\nimport io\n\ndef hate_speech_detection(llm, shot):\n    prompt_final = create_prompt(shot)\n    hate_speech_detection = prompt_final | llm\n    return hate_speech_detection","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:09.686211Z","iopub.execute_input":"2024-08-10T17:31:09.686475Z","iopub.status.idle":"2024-08-10T17:31:09.703369Z","shell.execute_reply.started":"2024-08-10T17:31:09.686453Z","shell.execute_reply":"2024-08-10T17:31:09.702486Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Label 0: no-hate speech (no answer)\n# Label 1: hate speech (yes answer)\n\ndef predict(llm,x_test,shot):\n    y_pred = []\n    chain = hate_speech_detection(llm,shot)\n    for x in tqdm(x_test):\n        answer = chain.invoke({\"input\": x})\n        #print(answer)\n        if \"yes\" in answer.lower(): y_pred.append(1)\n        else: y_pred.append(0)\n    return y_pred\n\n#y_pred = predict(llm,x_test,2)\n#write_file(\"/kaggle/working/prediction_hate_speech_mistral_zero_shot.json\", y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T17:31:13.253250Z","iopub.execute_input":"2024-08-10T17:31:13.253920Z","iopub.status.idle":"2024-08-10T17:31:13.259411Z","shell.execute_reply.started":"2024-08-10T17:31:13.253884Z","shell.execute_reply":"2024-08-10T17:31:13.258322Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Test completo:","metadata":{}},{"cell_type":"code","source":"import time\nmodels = [\"llama3.1\",\"gemma2\",\"mistral\"]\nshots = [0,2,4,8]\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n\nfor model in models:\n    start_ollama()\n    pull_model(model)\n    start_model(model)\n    time.sleep(400)\n    llm = Ollama(model=model, temperature=0)\n    for shot in shots:\n        y_pred = predict(llm,x_test,shot)\n        write_file(f\"/kaggle/working/prediction_hate_speech_mistral_{shot}_shot.json\", y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Valutazione del modello\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred, labels=[0,1], output_dict=True)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Classification Report:\\n{class_report}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Salva risultati\n\nimport json\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef write_file(filename,content):\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n        \n\ndict_to_write = {\"class_report\":class_report, \"0_0\":int(conf_matrix[0][0]), \"0_1\":int(conf_matrix[0][1]), \"1_0\":int(conf_matrix[1][0]), \"1_1\":int(conf_matrix[1][1])}        \nwrite_file(\"/kaggle/working/test_hate_speech_mistral_zero_shot.json\", dict_to_write)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Da ripetere i test con llama3.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}